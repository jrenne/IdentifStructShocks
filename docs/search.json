[{"path":"index.html","id":"intro","chapter":"The Identification of Dynamic Structural Shocks","heading":"The Identification of Dynamic Structural Shocks","text":"identification estimation dynamic responses structural shocks one principal goals macroeconometrics. responses correspond effect, time, exogenous intervention propagates economy, modeled system simultaneous equations.last decades, several methodologies proposed estimate responses. objective course, developed Kenza Benhima Jean-Paul Renne, provide exhaustive view methodologies provide students tools enabling implement various contexts.Codes associated course part IdSS package (Identification Structural Shocks), available GitHub. load package GitHub, need use function install_github devtools package:Useful (R) links:Download R:\nR software: https://cran.r-project.org (basic R software)\nRStudio: https://www.rstudio.com (convenient R editor)\nDownload R:R software: https://cran.r-project.org (basic R software)RStudio: https://www.rstudio.com (convenient R editor)Tutorials:\nRstudio: https://dss.princeton.edu/training/RStudio101.pdf (Oscar Torres-Reyna)\nR: https://cran.r-project.org/doc/contrib/Paradis-rdebuts_en.pdf (Emmanuel Paradis)\ntutorial: https://jrenne.shinyapps.io/Rtuto_publiShiny/\nTutorials:Rstudio: https://dss.princeton.edu/training/RStudio101.pdf (Oscar Torres-Reyna)R: https://cran.r-project.org/doc/contrib/Paradis-rdebuts_en.pdf (Emmanuel Paradis)tutorial: https://jrenne.shinyapps.io/Rtuto_publiShiny/","code":"\ninstall.packages(\"devtools\") # devtools allows to use \"install_github\"\nlibrary(devtools)\ninstall_github(\"jrenne/IdSS\")\nlibrary(IdSS)"},{"path":"basics.html","id":"basics","chapter":"1 VARs and IRFs: the basics","heading":"1 VARs and IRFs: the basics","text":"Often, impulse response functions (IRFs) generated context vectorial autoregressive (VAR) models. section presents models show can used compute IRFs.","code":""},{"path":"basics.html","id":"definition-of-vars-and-svarma-models","chapter":"1 VARs and IRFs: the basics","heading":"1.1 Definition of VARs (and SVARMA) models","text":"Definition 1.1  ((S)VAR model) Let \\(y_{t}\\) denote \\(n \\times1\\) vector (endogenous) random variables. Process \\(y_{t}\\) follows \\(p^{th}\\)-order (S)VAR , \\(t\\), \n\\[\\begin{eqnarray}\n\\begin{array}{rllll}\nVAR:& y_t &=& c + \\Phi_1 y_{t-1} + \\dots + \\Phi_p y_{t-p} + \\varepsilon_t,\\\\\nSVAR:& y_t &=& c + \\Phi_1 y_{t-1} + \\dots + \\Phi_p y_{t-p} + B \\eta_t,\n\\end{array}\n\\tag{1.1}\n\\end{eqnarray}\\]\n\\(\\varepsilon_t = B\\eta_t\\), \\(\\{\\eta_{t}\\}\\) white noise sequence whose components mutually serially independent, satisfies \\(\\mathbb{V}ar(\\eta_t)=Id\\).first line Eq. (1.1) corresponds reduced-form VAR model (structural form second line). structural shocks (components \\(\\eta_t\\)) mutually uncorrelated, case innovations, components \\(\\varepsilon_t\\). However, cases, vectors \\(\\eta_t\\) \\(\\varepsilon_t\\) serially uncorrelated (time).Notice \\(\\mathbb{V}ar(\\eta_t)=Id\\) \\(\\varepsilon_t = B\\eta_t\\) jointly imply \\[\n\\Omega :=\\mathbb{V}ar(\\varepsilon_t)=\\mathbb{V}ar(B\\eta_t)= B  \\mathbb{V}ar(\\eta_t) B'=BB'.\n\\]Eq. (1.1) can also rewritten using lag operator \\(L\\), gives:1\n\\[\ny_t = c + \\Phi(L) y_{t} + \\varepsilon_t,\n\\]\n\\(\\Phi(L)=\\Phi_1 L + \\dots + \\Phi_p L^p\\).case univariate models, VARs can extended MA terms \\(\\eta_t\\), giving rise VARMA models:Definition 1.2  ((S)VARMA model) Let \\(y_{t}\\) denote \\(n \\times1\\) vector random variables. Process \\(y_{t}\\) follows VARMA model order (p,q) , \\(t\\), \n\\[\\begin{eqnarray}\n\\begin{array}{rllll}\nVARMA:& y_t &=& c + \\Phi_1 y_{t-1} + \\dots + \\Phi_p y_{t-p} + \\\\\n&&&\\varepsilon_t + \\Theta_1\\varepsilon_{t-1} + \\dots + \\Theta_q \\varepsilon_{t-q},\\\\\nSVARMA:& y_t &=& c + \\Phi_1 y_{t-1} + \\dots + \\Phi_p y_{t-p} + \\\\\n&&& B_0 \\eta_t + B_1 \\eta_{t-1} + \\dots +  B_q \\eta_{t-q},\n\\end{array}\n\\tag{1.2}\n\\end{eqnarray}\\]\n\\(\\varepsilon_t = B_0\\eta_t\\), \\(B_j = \\Theta_j B_0\\), \\(j \\ge 0\\) (\\(\\Theta_0=Id\\)), \\(\\{\\eta_{t}\\}\\) white noise sequence whose components mutually serially independent.","code":""},{"path":"basics.html","id":"IRFSVARMA","chapter":"1 VARs and IRFs: the basics","heading":"1.2 IRFs in SVARMA","text":"One main objectives macro-econometrics derive IRFs, represent dynamic effects structural shocks (components \\(\\eta_t\\)) though system variables \\(y_t\\).Formally, IRF difference conditional expectations:\n\\[\\begin{equation}\n\\boxed{\\Psi_{,j,h} = \\mathbb{E}(y_{,t+h}|\\eta_{j,t}=1) - \\mathbb{E}(y_{,t+h})}\n\\tag{1.3}\n\\end{equation}\\]\n(effect \\(y_{,t+h}\\) one-unit shock \\(\\eta_{j,t}\\)).IRFs closely relate Wold decomposition \\(y_t\\). Indeed, dynamics process \\(y_t\\) can described VARMA model (includes case VAR models), \\(y_t\\) covariance stationary (see Def. 11.1), \\(y_t\\) admits following infinite MA representation (Wold decomposition):\n\\[\\begin{equation}\n\\boxed{y_t = \\mu + \\sum_{=0}^\\infty \\Psi_{} \\eta_{t-}.}\n\\tag{1.4}\n\\end{equation}\\]\n, shifted \\(h\\) periods:\n\\[\\begin{eqnarray*}\ny_{t+h} &=& \\mu + \\sum_{=0}^\\infty \\Psi_{} \\eta_{t+h-}\\\\\n&=& \\mu + \\underbrace{\\sum_{=0}^{h-1} \\Psi_{} \\eta_{t+h-}}_{\\mbox{Effect }\\eta_{t+1},\\dots,\\eta_{t+h}} + \\underbrace{\\Psi_{h} \\eta_{t}}_{\\mbox{Effect }\\eta_{t}} + \\underbrace{\\sum_{=h+1}^\\infty \\Psi_{} \\eta_{t+h-}}_{\\mbox{Effect }\\eta_{t-1},\\eta_{t-2,\\dots}}.\n\\end{eqnarray*}\\]notations, get \\(\\mathbb{E}(y_{,t+h}|\\eta_{j,t}=1) = \\mu_i + \\Psi_{,j,h}\\), \\(\\Psi_{,j,h}\\) component \\((,j)\\) matrix \\(\\Psi_h\\) \\(\\mu_i\\) \\(^{th}\\) entry vector \\(\\mu\\). Since also \\(\\mathbb{E}(y_{,t+h})=\\mu_i\\), obtain Eq. (1.3).Example 1.1  (IRF univariate AR model) fix ideas, consider simple univariate AR(1) process:\n\\[\ny_t = \\phi y_{t-1} + b \\eta_t, \\quad \\eta_t \\sim \\text{..d. }\\mathcal{N}(0,1).\n\\]assume \\(|\\phi| < 1\\) process covariance stationary.Iterating forward:\n\\[\ny_{t+h} = b \\eta_{t+h} + \\phi b \\eta_{t+h-1} + \\dots + \\phi^h b \\eta_t + \\phi^{h+1} y_{t-1}.\n\\]\n\\(\\eta_t\\)’s independent, \\(\\mathbb{E}(\\eta_{t+h}|\\eta_t = 1)=0\\) \\(h \\ne 0\\) (1 otherwise). Therefore:\n\\[\n\\mathbb{E}(y_{t+h}|\\eta_t = 1) - \\mathbb{E}(y_{t+h}) = \\phi^h b.\n\\]\nHence, impulse response function (IRF) horizon \\(h\\) simply $_h = ^h b $.\nFigure 1.1: Impulse response functions AR(1) process: \\(y_t = \\phi y_{t-1} + b \\eta_t\\), \\(\\eta_t \\sim ..d. \\mathcal{N}(0,1)\\).\nHence, estimating IRFs amounts estimating \\(\\Psi_h\\) matrices. practice, three main approaches commonly used:Calibrate solve (purely structural) Dynamic Stochastic General Equilibrium (DSGE) model first order (linear approximation). resulting solution can written form Eq. (1.4).Calibrate solve (purely structural) Dynamic Stochastic General Equilibrium (DSGE) model first order (linear approximation). resulting solution can written form Eq. (1.4).Directly estimate \\(\\Psi_h\\) matrices using projection methods (see Section 8).Directly estimate \\(\\Psi_h\\) matrices using projection methods (see Section 8).Approximate infinite moving-average representation estimating parsimonious model, VAR(MA) model (see Section 1.4). (structural) VARMA representation obtained, Eq. (1.4) follows immediately result stated next proposition.Approximate infinite moving-average representation estimating parsimonious model, VAR(MA) model (see Section 1.4). (structural) VARMA representation obtained, Eq. (1.4) follows immediately result stated next proposition.Proposition 1.1  (IRF ARMA(p,q) process) \\(y_t\\) follows VARMA model described Def. 1.2, matrices \\(\\Psi_h\\) appearing Eq. (1.4) can computed recursively follows:Set \\(\\Psi_{-1}=\\dots=\\Psi_{-p}=0\\).\\(h \\ge 0\\), (recursively) apply:\n\\[\n\\Psi_h = \\Phi_1 \\Psi_{h-1} + \\dots + \\Phi_p \\Psi_{h-p} + \\Theta_h B_0,\n\\]\n\\(\\Theta_0 = Id\\) \\(\\Theta_h = 0\\) \\(h>q\\).Proof. obtained applying operator \\(\\frac{\\partial}{\\partial \\eta_{t}}\\) sides Eq. (1.2).Typically, consider VAR(2) case. first steps algorithm mentioned last bullet point follows:\n\\[\\begin{eqnarray*}\ny_t &=& \\Phi_1 {\\color{blue}y_{t-1}} + \\Phi_2 y_{t-2} + B \\eta_t  \\\\\n&=& \\Phi_1 \\color{blue}{(\\Phi_1 y_{t-2} + \\Phi_2 y_{t-3} + B \\eta_{t-1})} + \\Phi_2 y_{t-2} + B \\eta_t  \\\\\n&=& B \\eta_t + \\Phi_1 B \\eta_{t-1} + (\\Phi_2 + \\Phi_1^2) \\color{red}{y_{t-2}} + \\Phi_1\\Phi_2 y_{t-3}  \\\\\n&=& B \\eta_t + \\Phi_1 B \\eta_{t-1} + (\\Phi_2 + \\Phi_1^2) \\color{red}{(\\Phi_1 y_{t-3} + \\Phi_2 y_{t-4} + B \\eta_{t-2})} + \\Phi_1\\Phi_2 y_{t-3} \\\\\n&=& \\underbrace{B}_{=\\Psi_0} \\eta_t + \\underbrace{\\Phi_1 B}_{=\\Psi_1} \\eta_{t-1} + \\underbrace{(\\Phi_2 + \\Phi_1^2)B}_{=\\Psi_2} \\eta_{t-2} + f(y_{t-3},y_{t-4}).\n\\end{eqnarray*}\\]particular, \\(B = \\Psi_0\\). Matrix \\(B\\) indeed captures contemporaneous impact \\(\\eta_t\\) \\(y_t\\). matrix \\(B\\) sometimes called impulse matrix.Example 1.2  (IRFs SVAR model) Consider following VAR(2) model:\n\\[\\begin{eqnarray}\n\\quad y_t &=&\n\\underbrace{\\left[\\begin{array}{cc}\n0.6 & 0.2 \\\\\n0 & 0.5\n\\end{array}\\right]}_{=\\Phi_1}\ny_{t-1} +  \n\\underbrace{\\left[\\begin{array}{cc}\n-0.1 & 0.1 \\\\\n0.2 & 0.3\n\\end{array}\\right]}_{=\\Phi_2}y_{t-2} + \\underbrace{\\left[\\begin{array}{cc}\n0.5 & 1.5 \\\\\n-1 & 0.8\n\\end{array}\\right]}_{=B} \\eta_{t}.\n\\tag{1.5}\n\\end{eqnarray}\\]can use function simul.VARMA package IdSS produce IRFs (using indic.IRF=1 list arguments):\nFigure 1.2: Impulse response functions (VAR(2) specified ).\ntype output obtained using function simul.VAR package IdSS:2Example 1.3  (IRFs SVARMA model) Consider following VARMA(1,1) model:\n\\[\\begin{eqnarray}\n\\quad y_t &=&\n\\underbrace{\\left[\\begin{array}{cc}\n0.5 & 0.3 \\\\\n-0.4 & 0.7\n\\end{array}\\right]}_{\\Phi_1}\ny_{t-1} +  \\\\\n&& \\underbrace{\\left[\\begin{array}{cc}\n1 & 2 \\\\\n-1 & 1\n\\end{array}\\right]}_{B}\\eta_t + \\underbrace{\\left[\\begin{array}{cc}\n0.4 & 0 \\\\\n-1 & -0.5\n\\end{array}\\right]}_{\\Theta_1} \\underbrace{\\left[\\begin{array}{cc}\n1 & 2 \\\\\n-1 & 1\n\\end{array}\\right]}_{B}\\eta_{t-1}.\n\\tag{1.6}\n\\end{eqnarray}\\]can use function simul.VARMA package IdSS produce IRFs (using indic.IRF=1 list arguments):\nFigure 1.3: Impulse response functions (SVARMA(1,1) specified ).\n","code":"\nphi <- .8\nb <- .5\nplot(phi^(0:30)*b,type=\"b\",lwd=2,xlab=\"Periods after shock\",ylab=\"\",las=1)\ngrid()\nlibrary(IdSS)\n# ---- Specify model: ----\nPhi <- array(NaN,c(2,2,2)) # (2,2,2) for (n,n,p)\nPhi[,,1] <- matrix(c(.6,0,.2,.5),2,2)\nPhi[,,2] <- matrix(c(-.1,.2,.1,.3),2,2)\nc <- c(1,2)\nB <- matrix(c(.5,-1,1.5,.8),2,2)\nModel <- list(c = c,Phi = Phi,B = B)\n# ---- Define first shock: ----\neta0 <- c(1,0)\nres.sim.1 <- simul.VARMA(Model,nb.sim=30,eta0=eta0,indic.IRF=1)\n# ---- Define second shock: ----\neta0 <- c(0,1)\nres.sim.2 <- simul.VARMA(Model,nb.sim=30,eta0=eta0,indic.IRF=1)\n# ---- Prepare plots: ----\npar(plt=c(.15,.95,.25,.8)) # define margins\npar(mfrow=c(2,2)) # 2 rows and 2 columns of plots\nfor(i in 1:2){\n  if(i == 1){res.sim <- res.sim.1}else{res.sim <- res.sim.2}\n  for(j in 1:2){\n    plot(res.sim$Y[j,],las=1,\n         type=\"l\",lwd=3,xlab=\"\",ylab=\"\",\n         main=paste(\"Resp. of y\",j,\n                    \" to a 1-unit increase in η\",i,sep=\"\"))\n    abline(h=0,col=\"grey\",lty=3)\n  }}\nres.sim <- simul.VAR(c=c,Phi=Phi,B=B,nb.sim=30,indic.IRF=1,u.shock=eta0)\nlibrary(IdSS)\n# ---- Specify model: ----\nPhi <- array(NaN,c(2,2,1)) # (2,2,1) for (n,n,p)\nPhi[,,1] <- matrix(c(.5,-.4,.3,.7),2,2)\np <- dim(Phi)[3]\nTheta <- array(NaN,c(2,2,1))\nTheta[,,1] <- matrix(c(.4,-1,0,-.5),2,2)\nq <- dim(Theta)[3]\nc <- rep(0,2)\nB <- matrix(c(1,-1,2,1),2,2)\nModel <- list(c = c,Phi = Phi,Theta = Theta,B = B)\n# ---- Define first shock: ----\neta0 <- c(1,0)\nres.sim.1 <- simul.VARMA(Model,nb.sim=30,eta0=eta0,indic.IRF=1)\n# ---- Define second shock: ----\neta0 <- c(0,1)\nres.sim.2 <- simul.VARMA(Model,nb.sim=30,eta0=eta0,indic.IRF=1)\npar(plt=c(.15,.95,.25,.8))\npar(mfrow=c(2,2))\nfor(i in 1:2){\n  if(i == 1){res.sim <- res.sim.1}else{res.sim <- res.sim.2}\n  for(j in 1:2){\n    plot(res.sim$Y[j,],las=1,\n         type=\"l\",lwd=3,xlab=\"\",ylab=\"\",\n         main=paste(\"Resp. of y\",j,\n                    \" to a 1-unit increase in η\",i,sep=\"\"))\n    abline(h=0,col=\"grey\",lty=3)\n  }}"},{"path":"basics.html","id":"covariance-stationary-varma-models","chapter":"1 VARs and IRFs: the basics","heading":"1.3 Covariance-stationary VARMA models","text":"Let’s come back infinite MA case (Eq. (1.4)):\n\\[\ny_t = \\mu + \\sum_{h=0}^\\infty \\Psi_{h} \\eta_{t-h}.\n\\]\n\\(y_t\\) covariance-stationary (ergodic mean), case \n\\[\\begin{equation}\n\\sum_{=0}^\\infty \\|\\Psi_i\\| < \\infty,\n\\tag{1.7}\n\\end{equation}\\]\n\\(\\|\\|\\) denotes norm matrix \\(\\) (e.g. \\(\\|\\|=\\sqrt{tr(AA')}\\)). notably implies \\(y_t\\) stationary (ergodic mean), \\(\\|\\Psi_h\\|\\rightarrow 0\\) \\(h\\) gets large.satisfied \\(\\Phi_k\\)’s \\(\\Theta_k\\)’s VARMA-based process (Eq. (1.2)) stationary? conditions similar univariate case. Let us introduce following notations:\n\\[\\begin{eqnarray}\ny_t &=& c + \\underbrace{\\Phi_1 y_{t-1} + \\dots +\\Phi_p y_{t-p}}_{\\color{blue}{\\mbox{AR component}}} +  \\\\\n&&\\underbrace{B \\eta_t - \\Theta_1 B \\eta_{t-1} - \\dots - \\Theta_q B \\eta_{t-q}}_{\\color{red}{\\mbox{MA component}}} \\nonumber\\\\\n&\\Leftrightarrow& \\underbrace{ \\color{blue}{(- \\Phi_1 L - \\dots - \\Phi_p L^p)}}_{= \\color{blue}{\\Phi(L)}}y_t = c +  \\underbrace{ \\color{red}{(+ \\Theta_1 L + \\ldots + \\Theta_q L^q)}}_{=\\color{red}{\\Theta(L)}} B \\eta_{t}. \\nonumber\n\\tag{1.8}\n\\end{eqnarray}\\]Process \\(y_t\\) stationary iff roots \\(\\det(\\Phi(z))=0\\) strictly outside unit circle , equivalently, iff eigenvalues \n\\[\\begin{equation}\n\\underbrace{\\Phi}_{np \\times np} = \\left[\\begin{array}{cccc}\n\\Phi_{1} & \\Phi_{2} & \\cdots & \\Phi_{p}\\\\\n& 0 & \\cdots & 0\\\\\n0 & \\ddots & 0 & 0\\\\\n0 & 0 & & 0\\end{array}\\right]\n\\tag{1.9}\n\\end{equation}\\]\nlie strictly within unit circle. Hence, case univariate processes, covariance-stationarity VARMA model depends specification AR part.Let’s derive first two unconditional moments (covariance-stationary) VARMA process.Eq. (1.8) gives \\(\\mathbb{E}(\\Phi(L)y_t)=c\\), therefore \\(\\Phi(1)\\mathbb{E}(y_t)=c\\), \n\\[\n\\mathbb{E}(y_t) = (- \\Phi_1 - \\dots - \\Phi_p)^{-1}c.\n\\]\nautocovariances \\(y_t\\) can deduced infinite MA representation (Eq. (1.4)). :\n\\[\n\\gamma_j \\equiv \\mathbb{C}ov(y_t,y_{t-j}) = \\sum_{=j}^\\infty \\Psi_i \\Psi_{-j}'.\n\\]\nIndeed:\n\\[\\begin{eqnarray*}\n\\mathbb{C}ov(y_t,y_{t-j}) &=& \\mathbb{E}\\left(\\left[\\sum_{h=0}^\\infty \\Psi_{h} \\eta_{t-h}\\right]\\left[\\sum_{h=0}^\\infty \\Psi_{h} \\eta_{t-j-h}\\right]'\\right)\\\\\n&=& \\mathbb{E}\\left(\\left[\\sum_{=0}^{j-1} \\Psi_{} \\eta_{t-}+\\sum_{=j}^\\infty \\Psi_{} \\eta_{t-}\\right]\\left[\\sum_{=j}^\\infty \\Psi_{-j} \\eta_{t-}\\right]'\\right)\\\\\n&=&\\mathbb{E}\\left(\\sum_{=j}^\\infty \\Psi_{} \\eta_{t-}\\eta_{t-}'\\Psi_{-j}'\\right)= \\sum_{=j}^\\infty \\Psi_i \\Psi_{-j}'.\n\\end{eqnarray*}\\]\n(Remark: infinite sum exists soon Eq. (1.7) satisfied.)Conditional means autocovariances can also deduced Eq. (1.4). \\(0 \\le h\\) \\(0 \\le h_1 \\le h_2\\):\n\\[\\begin{eqnarray*}\n\\mathbb{E}_t(y_{t+h}) &=& \\mu + \\sum_{k=0}^\\infty \\Psi_{k+h} \\eta_{t-k} \\\\\n\\mathbb{C}ov_t(y_{t+1+h_1},y_{t+1+h_2}) &=& \\sum_{k=0}^{h_1} \\Psi_{k}\\Psi_{k+h_2-h_1}',\n\\end{eqnarray*}\\]\n\\(\\mathbb{E}_t\\) \\(\\mathbb{C}ov_t\\) denote expectation covariance conditional \\(\\{\\eta_t,\\eta_{t-1},\\dots\\}\\).3The previous formula implies particular forecasting error \\(y_{t+h} - \\mathbb{E}_t(y_{t+h})\\) variance equal :\n\\[\n\\boxed{\\mathbb{V}ar_t(y_{t+h}) = \\sum_{k=0}^{h-1} \\Psi_{k}\\Psi_{k}'.}\n\\]\ncomponents \\(\\eta_t\\) mutually serially independent (therefore uncorrelated), :\n\\[\n\\mathbb{V}ar(\\Psi_k \\eta_{t-k}) = \\mathbb{V}ar\\left(\\sum_{=1}^n \\psi_{k,} \\eta_{,t-k}\\right)  = \\sum_{=1}^n \\psi_{k,}\\psi_{k,}',\n\\]\n\\(\\psi_{k,}\\) denotes \\(^{th}\\) column \\(\\Psi_k\\). suggests following decomposition variance forecast error (called variance decomposition):\n\\[\n\\mathbb{V}ar_t(y_{t+h}) = \\sum_{=1}^n \\underbrace{\\sum_{k=0}^{h-1}\\psi_{k,}\\psi_{k,}'.}_{\\mbox{Contribution $\\eta_{,t}$}}\n\\]illustrate, let us use VAR(2) model proposed , Example 1.2. explained , variance decomposition directly results knowledge IRFs (shown Figure 1.2):Example 1.4  (Variance decomposition context VAR(2) defined.) chunk computes, different horizons, shares variances two variables accounted two components \\(\\eta_t\\) context model proposed Example 1.2.\nFigure 1.4: Variance decomposition VAR(2) model.\n","code":"\nPhi <- array(NaN,c(2,2,2)) # (2,2,2) for (n,n,p)\nPhi[,,1] <- matrix(c(.6,0,.2,.5),2,2)\nPhi[,,2] <- matrix(c(-.1,.2,.1,.3),2,2)\nB <- matrix(c(.5,-1,1.5,.8),2,2)\nmake_variance_decompo(Phi,B,maxHorizon=30)"},{"path":"basics.html","id":"estimVAR","chapter":"1 VARs and IRFs: the basics","heading":"1.4 VAR estimation","text":"section discusses estimation VAR models.4 Eq. (1.1) can written:\n\\[\ny_{t}=c+\\Phi(L)y_{t-1}+\\varepsilon_{t},\n\\]\n\\(\\Phi(L) = \\Phi_1 + \\Phi_2 L + \\dots + \\Phi_p L^{p-1}\\), reduced-form shocks (\\(\\varepsilon_{t}\\)) white noise shocks.5Using Hamilton (1994)’s notations, denote \\(\\Pi\\) matrix \\(\\left[\\begin{array}{ccccc}\nc & \\Phi_{1} & \\Phi_{2} & \\ldots & \\Phi_{p}\\end{array}\\right]'\\) \\(x_{t}\\) vector \\(\\left[\\begin{array}{ccccc}\n1 & y'_{t-1} & y'_{t-2} & \\ldots & y'_{t-p}\\end{array}\\right]'\\), :\n\\[\\begin{equation}\ny_{t}= \\Pi'x_{t} + \\varepsilon_{t}.\n\\tag{1.10}\n\\end{equation}\\]previous representation convenient discuss estimation VAR model, parameters gathered two matrices : \\(\\Pi\\) \\(\\Omega\\) (\\(\\Omega = \\mathbb{V}ar (\\varepsilon_t)\\)).","code":""},{"path":"basics.html","id":"maximum-likelihood-estimation-mle-when-the-shocks-are-gaussian","chapter":"1 VARs and IRFs: the basics","heading":"1.4.1 Maximum Likelihood Estimation (MLE) when the shocks are Gaussian","text":"Let us start case shocks ..d. Gaussian. :\n\\[\ny_{t}\\mid y_{t-1},y_{t-2},\\ldots\\sim \\mathcal{N}(c+\\Phi_{1}y_{t-1}+\\ldots\\Phi_{p}y_{t-p},\\Omega).\n\\]Proposition 1.2  (MLE Gaussian VAR) \\(y_t\\) follows VAR(\\(p\\)) (see Definition 1.1), \\(\\varepsilon_t \\sim \\,..d.\\,\\mathcal{N}(0,\\Omega)\\), ML estimate \\(\\Pi\\), denoted \\(\\hat{\\Pi}\\) (see Eq. (1.10)), given \n\\[\\begin{equation}\n\\hat{\\Pi}=\\left[\\sum_{t=1}^{T}x_{t}x'_{t}\\right]^{-1}\\left[\\sum_{t=1}^{T}y_{t}'x_{t}\\right]= (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{y},\n\\tag{1.11}\n\\end{equation}\\]\n\\(\\mathbf{X}\\) \\(T \\times (1+np)\\) matrix whose \\(t^{th}\\) row \\(x_t\\) \\(\\mathbf{y}\\) \\(T \\times n\\) matrix whose \\(t^{th}\\) row \\(y_{t}'\\)., \\(^{th}\\) column \\(\\hat{\\Pi}\\) (\\(b_i\\), say) OLS estimate \\(\\beta_i\\), :\n\\[\\begin{equation}\ny_{,t} = \\beta_i'x_t + \\varepsilon_{,t},\n\\tag{1.12}\n\\end{equation}\\]\n(.e., \\(\\beta_i' = [c_i,\\phi_{,1}',\\dots,\\phi_{,p}']'\\)).ML estimate \\(\\Omega\\), denoted \\(\\hat{\\Omega}\\), coincides sample covariance matrix \\(n\\) series OLS residuals Eq. (1.12), .e.:\n\\[\\begin{equation}\n\\hat{\\Omega} = \\frac{1}{T} \\sum_{=1}^T \\hat{\\varepsilon}_t\\hat{\\varepsilon}_t',\\quad\\mbox{} \\hat{\\varepsilon}_t= y_t - \\hat{\\Pi}'x_t.\n\\end{equation}\\]asymptotic distributions estimators ones resulting standard OLS formula.Proof. See Appendix 11.3.simplicity VAR framework tractability MLE open way convenient econometric testing. Let’s illustrate likelihood ratio test (see Def. 11.2). maximum value achieved MLE \n\\[\n\\log\\mathcal{L}(Y_{T};\\hat{\\Pi},\\hat{\\Omega}) = -\\frac{Tn}{2}\\log(2\\pi)+\\frac{T}{2}\\log\\left|\\hat{\\Omega}^{-1}\\right| -\\frac{1}{2}\\sum_{t=1}^{T}\\left[\\hat{\\varepsilon}_{t}'\\hat{\\Omega}^{-1}\\hat{\\varepsilon}_{t}\\right].\n\\]\nlast term :\n\\[\\begin{eqnarray*}\n\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}'\\hat{\\Omega}^{-1}\\hat{\\varepsilon}_{t} &=& \\mbox{Tr}\\left[\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}'\\hat{\\Omega}^{-1}\\hat{\\varepsilon}_{t}\\right] = \\mbox{Tr}\\left[\\sum_{t=1}^{T}\\hat{\\Omega}^{-1}\\hat{\\varepsilon}_{t}\\hat{\\varepsilon}_{t}'\\right]\\\\\n&=&\\mbox{Tr}\\left[\\hat{\\Omega}^{-1}\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}\\hat{\\varepsilon}_{t}'\\right] = \\mbox{Tr}\\left[\\hat{\\Omega}^{-1}\\left(T\\hat{\\Omega}\\right)\\right]=Tn.\n\\end{eqnarray*}\\]\nTherefore, optimized log-likelihood simply obtained :\n\\[\\begin{equation}\n\\log\\mathcal{L}(Y_{T};\\hat{\\Pi},\\hat{\\Omega})=-(Tn/2)\\log(2\\pi)+(T/2)\\log\\left|\\hat{\\Omega}^{-1}\\right|-Tn/2.\n\\tag{1.13}\n\\end{equation}\\]optimized likelihood analytical form allows flexible applications likelihood ratio (LR) test (Def. 11.2).Example 1.5  (Ad-hoc lag selection based LR test) Suppose want test whether system variables follows VAR(\\(p_0\\)) process (null hypothesis) alternative specification \\(p_1\\) lags, \\(p_1 > p_0\\).\nLet \\(\\hat{L}_0\\) \\(\\hat{L}_1\\) denote maximum log-likelihoods obtained two specifications, respectively.\nnull hypothesis (\\(H_0\\): \\(p = p_0\\)), LR statistic given \\[\\begin{equation}\n\\xi^{LR} := 2(\\hat{L}_1 - \\hat{L}_0)\n= T \\left( \\log\\left|\\hat{\\Omega}_1^{-1}\\right| - \\log\\left|\\hat{\\Omega}_0^{-1}\\right| \\right)\n\\sim \\chi^2\\left(n^2(p_1 - p_0)\\right).\n\\end{equation}\\]statistic can used determine whether adding lags significantly improves model fit.\nHowever, including many lags quickly exhausts degrees freedom: \\(p\\) lags, \\(n\\) equations VAR includes \\(n \\times p\\) coefficients plus intercept term.\nadding lags always improves -sample fit, can lead -parameterization deteriorate model’s --sample predictive performance.approach provides ad-hoc way select lag length using LR tests (Def. 11.2).\nsystematic procedures rely information criteria, balance goodness fit model parsimony.\nGaussian case (see Eq. (1.13)), common criteria include:\n\\[\\begin{eqnarray*}\nAIC & = & cst + \\log\\left|\\hat{\\Omega}\\right| + \\frac{2}{T}N \\\\\nBIC & = & cst + \\log\\left|\\hat{\\Omega}\\right| + \\frac{\\log T}{T}N,\n\\end{eqnarray*}\\]\n\\(N = p \\times n^2\\). expressions, \\(\\log|\\hat{\\Omega}|\\) captures model’s goodness fit (smaller values indicating better fit), last term penalty increases number estimated parameters.Example 1.6  (Three-variable VAR model) following example illustrates estimate analyze three-variable VAR model using U.S. quarterly data inflation, output gap, short-term nominal interest rate.begin using VARselect function vars package determine optimal lag length VAR specification.Next, estimate VAR model including exogenous variable — commodity price index.can now verify whether estimated VAR stationary:eigenvalues moduli smaller one, confirming system stationary.compute display impulse response functions (IRFs) using Cholesky identification approach (see Section 2.3), impact matrix \\(B\\) comes Cholesky factorization \\(\\Omega = \\mathbb{V}ar(\\varepsilon_t)\\).\nFigure 1.5: Impulse response functions 3-variable VAR model estimated U.S. quarterly data.\nFinally, compute variance decomposition associated model.\nFigure 1.6: Variance decomposition 3-variable VAR model estimated U.S. quarterly data.\n","code":"\nlibrary(vars)   # provides 'VARselect' function\nlibrary(IdSS)   # dataset is included here\n\nFirst.date <- \"1959-04-01\"\nLast.date  <- \"2015-01-01\"\n\ndata <- US3var\ndata <- data[(data$Date >= First.date) & (data$Date <= Last.date), ]\nY <- as.matrix(data[c(\"infl\", \"y.gdp.gap\", \"r\")])\n\nVARselect(Y, lag.max = 12)## $selection\n## AIC(n)  HQ(n)  SC(n) FPE(n) \n##     11      3      2     11 \n## \n## $criteria\n##                  1            2           3          4          5          6\n## AIC(n) -0.14340897 -0.323002241 -0.39796730 -0.3888482 -0.4110019 -0.5229686\n## HQ(n)  -0.06661725 -0.188616726 -0.20598799 -0.1392751 -0.1038350 -0.1582079\n## SC(n)   0.04658647  0.009489796  0.07702133  0.2286370  0.3489799  0.3795098\n## FPE(n)  0.86641130  0.724024392  0.67182519  0.6781505  0.6635579  0.5936165\n##                  7           8           9         10         11         12\n## AIC(n) -0.48840029 -0.50157851 -0.44613801 -0.4950938 -0.5372138 -0.4980589\n## HQ(n)  -0.06604582 -0.02163024  0.09140405  0.1000420  0.1155158  0.2122645\n## SC(n)   0.55657468  0.68589305  0.88383014  0.9773709  1.0777475  1.2593990\n## FPE(n)  0.61498914  0.60758020  0.64308309  0.6133871  0.5892917  0.6143270\np <- 6\nexogen <- matrix(data$commo, ncol = 1); colnames(exogen) <- \"commo\"\nestVAR <- VAR(Y, p = p, exogen = exogen) # estimate the VAR model\nPhi <- Acoef(estVAR)\neps <- residuals(estVAR)\nOmega <- var(eps) # covariance matrix of OLS residuals\nB <- t(chol(Omega)) # Cholesky decomposition of Omega (lower triangular)\nPHI <- make.PHI(Phi) # autoregressive matrix in companion form\nprint(abs(eigen(PHI)$values)) # eigenvalues must be < 1 for stationarity##  [1] 0.9485855 0.9485855 0.9180083 0.8539724 0.8539724 0.7683116 0.7683116\n##  [8] 0.7509592 0.7509592 0.7465920 0.7465920 0.7392400 0.7392400 0.6720905\n## [15] 0.6720905 0.4474043 0.4474043 0.3904400\nn <- dim(Y)[2] # number of endogenous variables\npar(mfrow = c(n, n))\npar(plt = c(.3, .95, .2, .8))\nModel <- list(c = c, Phi = Phi, B = B)\nnames.var <- c(\"Inflation\", \"Output gap\", \"Interest rate\")\n\nfor (variable in 1:n) {\n  for (shock in 1:n) {\n    eta0 <- rep(0, n)\n    eta0[shock] <- 1\n    res.sim <- simul.VAR(c = NaN, Phi, B, nb.sim = 30, indic.IRF = 1,\n                         u.shock = eta0)\n    plot(res.sim[, variable], type = \"l\", lwd = 2, las = 1,\n         xlab = \"\", ylab = ifelse(shock==1,\n                                  paste(\"Resp. of \",\n                                              names.var[variable],\"...\",sep=\"\"),\"\"),\n         main = ifelse(variable==1,paste(\"... to shock η\", shock, sep = \"\"),\"\")\n         )}\n}\nmake_variance_decompo(Phi, B, maxHorizon = 50,\n                      names.var = c(\"Inflation\", \"Real activity\",\n                                    \"Short-term rate\"))"},{"path":"basics.html","id":"asymptotic-distribution-of-the-ols-estimates","chapter":"1 VARs and IRFs: the basics","heading":"1.4.2 Asymptotic distribution of the OLS estimates","text":"stated following proposition, shocks \\(\\varepsilon_t\\) Gaussian, OLS regressions still provide consistent estimates model parameters.Proposition 1.3  (Asymptotic distribution OLS estimate VAR coefficients (one variable)) \\(y_t\\) follows VAR model, defined Definition 1.1, :\n\\[\n\\sqrt{T}(\\mathbf{b}_i-\\beta_i) =  \\underbrace{\\left[\\frac{1}{T}\\sum_{t=p}^T x_t x_t' \\right]^{-1}}_{\\overset{p}{\\rightarrow} \\mathbf{Q}^{-1}}\n\\underbrace{\\sqrt{T} \\left[\\frac{1}{T}\\sum_{t=1}^T x_t\\varepsilon_{,t} \\right]}_{\\overset{d}{\\rightarrow} \\mathcal{N}(0,\\sigma_i^2\\mathbf{Q})},\n\\]\n\\(\\sigma_i = \\mathbb{V}ar(\\varepsilon_{,t})\\) \\(\\mathbf{Q} = \\mbox{plim }\\frac{1}{T}\\sum_{t=p}^T x_t x_t'\\) given :\n\\[\\begin{equation}\n\\mathbf{Q} = \\left[\n\\begin{array}{ccccc}\n1 & \\mu' &\\mu' & \\dots & \\mu' \\\\\n\\mu & \\gamma_0 + \\mu\\mu' & \\gamma_1 + \\mu\\mu' & \\dots & \\gamma_{p-1} + \\mu\\mu'\\\\\n\\mu & \\gamma_1 + \\mu\\mu' & \\gamma_0 + \\mu\\mu' & \\dots & \\gamma_{p-2} + \\mu\\mu'\\\\\n\\vdots &\\vdots &\\vdots &\\dots &\\vdots \\\\\n\\mu & \\gamma_{p-1} + \\mu\\mu' & \\gamma_{p-2} + \\mu\\mu' & \\dots & \\gamma_{0} + \\mu\\mu'\n\\end{array}\n\\right],\n\\tag{1.14}\n\\end{equation}\\]\n\\(\\gamma_j\\) unconditional autocovariance matrix order \\(j\\) \\(y_t\\), .e., \\(\\gamma_i = \\mathbb{C}ov(y_{t},y_{t-j})\\).Proof. See Appendix 11.3.However, since \\(x_t\\) correlates \\(\\varepsilon_s\\) \\(s<t\\), OLS estimator \\(\\mathbf{b}_i\\) \\(\\boldsymbol\\beta_i\\) biased small sample. (also case ML estimator.) Indeed, denoting \\(\\boldsymbol\\varepsilon_i\\) \\(T \\times 1\\) vector \\(\\varepsilon_{,t}\\)’s, using notations \\(b_i\\) \\(\\beta_i\\) introduced Proposition 1.2, :\n\\[\\begin{equation}\n\\mathbf{b}_i = \\beta_i + (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\boldsymbol\\varepsilon_i.\n\\tag{1.15}\n\\end{equation}\\]non-zero correlation \\(x_t\\) \\(\\varepsilon_{,s}\\) \\(s<t\\) , therefore, \\(\\mathbb{E}[(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\boldsymbol\\varepsilon_i] \\ne 0\\).However, stated Proposition 1.3 \\(y_t\\) covariance stationary, \\(\\frac{1}{n}\\mathbf{X}'\\mathbf{X}\\) converges positive definite matrix \\(\\mathbf{Q}\\), \\(\\frac{1}{n}X'\\boldsymbol\\varepsilon_i\\) converges 0. Hence \\(\\mathbf{b}_i \\overset{p}{\\rightarrow} \\beta_i\\).next proposition extends Proposition 1.3 incorporating covariances different \\(\\beta_i\\)’s characterizing asymptotic distribution ML estimator \\(\\Omega\\). words, delivers full asymptotic distribution model parameters. result particularly useful Monte Carlo procedures require drawing asymptotic distribution (see Section 3.1).Proposition 1.4  (Asymptotic distribution OLS estimates) \\(y_t\\) follows VAR model, defined Definition 1.1, :\n\\[\\begin{equation}\n\\sqrt{T}\\left[\n\\begin{array}{c}\nvec(\\hat\\Pi - \\Pi)\\\\\nvec(\\hat\\Omega - \\Omega)\n\\end{array}\n\\right]\n\\sim \\mathcal{N}\\left(0,\n\\left[\n\\begin{array}{cc}\n\\Omega \\otimes \\mathbf{Q}^{-1} & 0\\\\\n0 & \\Sigma_{22}\n\\end{array}\n\\right]\\right),\n\\tag{1.16}\n\\end{equation}\\]\ncomponent \\(\\Sigma_{22}\\) corresponding covariance \\(\\hat\\sigma_{,j}\\) \\(\\hat\\sigma_{k,l}\\) (\\(,j,l,m \\\\{1,\\dots,n\\}^4\\)) equal \\(\\sigma_{,l}\\sigma_{j,m}+\\sigma_{,m}\\sigma_{j,l}\\).Proof. See, e.g., Hamilton (1994), Appendix Chapter 11.practice, use previous proposition (instance implement Monte-Carlo simulations), \\(\\Omega\\) replaced \\(\\hat{\\Omega}\\), \\(\\mathbf{Q}\\) replaced \\(\\hat{\\mathbf{Q}} = \\frac{1}{T}\\sum_{t=p}^T x_t x_t'\\) \\(\\Sigma\\) matrix whose components form \\(\\hat\\sigma_{,l}\\hat\\sigma_{j,m}+\\hat\\sigma_{,m}\\hat\\sigma_{j,l}\\), \\(\\hat\\sigma_{,l}\\)’s components \\(\\hat\\Omega\\).","code":""},{"path":"basics.html","id":"specification-tests","chapter":"1 VARs and IRFs: the basics","heading":"1.4.3 Specification tests","text":"estimating VAR, important check model assumptions hold. particular, verify:Stationarity data (often checked estimation). variables non-stationary cointegrated, asymptotic distributions estimates valid standard inference (including likelihood-based tests) breaks . practice, one typically inspects unit-root tests relies economic knowledge integration cointegration properties. \\(\\Rightarrow\\) Remedy: transform data make stationary (removing deterministic trends, using differences), use Vector Error-Correction Models (VECM).Stationarity data (often checked estimation). variables non-stationary cointegrated, asymptotic distributions estimates valid standard inference (including likelihood-based tests) breaks . practice, one typically inspects unit-root tests relies economic knowledge integration cointegration properties. \\(\\Rightarrow\\) Remedy: transform data make stationary (removing deterministic trends, using differences), use Vector Error-Correction Models (VECM).Absence residual autocorrelation. residuals serially correlated, dynamic specification VAR inadequate (e.g., lags). leads biased estimates constant \\(\\Phi_i\\) matrices , consequently, biased impulse response functions. \\(\\Rightarrow\\) Remedy: increase lag length reconsider model specification (e.g., adding variables, transforming variables).Absence residual autocorrelation. residuals serially correlated, dynamic specification VAR inadequate (e.g., lags). leads biased estimates constant \\(\\Phi_i\\) matrices , consequently, biased impulse response functions. \\(\\Rightarrow\\) Remedy: increase lag length reconsider model specification (e.g., adding variables, transforming variables).Homoskedasticity. residuals exhibit heteroskedasticity, OLS estimates remain consistent mild conditions, longer efficient, standard errors become unreliable finite samples particular. based asymptotic distribution, confidence intervals coefficients impulse responses may misleading. \\(\\Rightarrow\\) Remedy: use heteroskedasticity-robust inference (e.g., bootstrapped inference methods, see Chapter 3), adopt models allowing time-varying volatility (e.g., VAR-GARCH, stochastic-volatility VAR).Homoskedasticity. residuals exhibit heteroskedasticity, OLS estimates remain consistent mild conditions, longer efficient, standard errors become unreliable finite samples particular. based asymptotic distribution, confidence intervals coefficients impulse responses may misleading. \\(\\Rightarrow\\) Remedy: use heteroskedasticity-robust inference (e.g., bootstrapped inference methods, see Chapter 3), adopt models allowing time-varying volatility (e.g., VAR-GARCH, stochastic-volatility VAR).Normality residuals. Non-normality affect consistency OLS estimates, weakens validity inference ML-based tests, especially small samples. Confidence bands impulse responses may distorted heavy tails skewness. \\(\\Rightarrow\\) Remedy: use bootstrap inference hypothesis testing IRFs.Normality residuals. Non-normality affect consistency OLS estimates, weakens validity inference ML-based tests, especially small samples. Confidence bands impulse responses may distorted heavy tails skewness. \\(\\Rightarrow\\) Remedy: use bootstrap inference hypothesis testing IRFs.Example 1.7  (Bivariate model Switzerland) example, consider bivariate model capturing joint dynamics Swiss inflation GDP growth rates.\nFigure 1.7: Inflation GDP growth rates. Percent change year ago.\nEstimate model parameters given lag choice:Check residual autocorrelation (Portmanteau test) using Portmanteau test one developed Breusch (1978) Godfrey (1978) (high p-value indicates evidence serial correlation):Let us now test heteroskedasticity take place form ARCH (see Engle (1982)). , ca use arch.test function (high p-value suggests ARCH effects):Normality test can perform using function normality.test (high p-value suggests residuals consistent normality):","code":"\nFirst.date <- \"1980-01-01\"\nLast.date  <- \"2025-01-01\"\ndata <- international[,c(\"date\",\"CPI_CHE\",\"GDP_CHE\")]\ndata <- data[(data$date >= First.date) & (data$date <= Last.date), ]\ndata <- data[complete.cases(data),]\nY <- data[,c(\"CPI_CHE\",\"GDP_CHE\")]\npar(mfrow=c(1,1));par(plt=c(.1,.95,.15,.95))\nplot(data$date,data$CPI_CHE,type=\"l\",lwd=2,las=1,ylim=c(-7,11),\n     xlab=\"\",ylab=\"\")\ngrid()\nlines(data$date,data$GDP_CHE,lwd=2,col=\"grey\")\nlibrary(tseries)## Registered S3 method overwritten by 'quantmod':\n##   method            from\n##   as.zoo.data.frame zoo\ntest.adf.Y1  <- adf.test(Y[,1],k=16)$p.value\ntest.adf.Y2  <- adf.test(Y[,2],k=16)$p.value\ntest.pp.Y1   <- pp.test(Y[,1])$p.value\ntest.pp.Y2   <- pp.test(Y[,2])$p.value## Warning in pp.test(Y[, 2]): p-value smaller than printed p-value\ntest.kpss.Y1 <- kpss.test(Y[,1])$p.value## Warning in kpss.test(Y[, 1]): p-value smaller than printed p-value\ntest.kpss.Y2 <- kpss.test(Y[,2])$p.value## Warning in kpss.test(Y[, 2]): p-value greater than printed p-value\ntest.adf  <- c(test.adf.Y1,test.adf.Y2)\ntest.pp   <- c(test.pp.Y1,test.pp.Y2)\ntest.kpss <- c(test.kpss.Y1,test.kpss.Y2)\nrbind(test.adf,test.pp,test.kpss)##                 [,1]       [,2]\n## test.adf  0.26339041 0.03910545\n## test.pp   0.06972378 0.01000000\n## test.kpss 0.01000000 0.10000000\np <- 2\nestVAR <- VAR(Y, p = p) # estimate the VAR model\nserial.test(estVAR, lags.pt = 20, type = \"PT.asymptotic\")## \n##  Portmanteau Test (asymptotic)\n## \n## data:  Residuals of VAR object estVAR\n## Chi-squared = 63.047, df = 72, p-value = 0.7653\nserial.test(estVAR, lags.pt = 20, type = \"BG\")## \n##  Breusch-Godfrey LM test\n## \n## data:  Residuals of VAR object estVAR\n## Chi-squared = 35.853, df = 20, p-value = 0.016\narch.test(estVAR, lags.multi = 12, multivariate.only = TRUE)## \n##  ARCH (multivariate)\n## \n## data:  Residuals of VAR object estVAR\n## Chi-squared = 128.59, df = 108, p-value = 0.0861\nnormality.test(estVAR)## $JB\n## \n##  JB-Test (multivariate)\n## \n## data:  Residuals of VAR object estVAR\n## Chi-squared = 2555.7, df = 4, p-value < 2.2e-16\n## \n## \n## $Skewness\n## \n##  Skewness only (multivariate)\n## \n## data:  Residuals of VAR object estVAR\n## Chi-squared = 39.143, df = 2, p-value = 3.164e-09\n## \n## \n## $Kurtosis\n## \n##  Kurtosis only (multivariate)\n## \n## data:  Residuals of VAR object estVAR\n## Chi-squared = 2516.6, df = 2, p-value < 2.2e-16"},{"path":"basics.html","id":"BlockGranger","chapter":"1 VARs and IRFs: the basics","heading":"1.5 Block exogeneity and Granger causality","text":"","code":""},{"path":"basics.html","id":"block-exogeneity","chapter":"1 VARs and IRFs: the basics","heading":"1.5.1 Block exogeneity","text":"unconstrained VAR model, coefficient matrices \\(\\Phi_i\\) full, lags endogenous variables may affect current endogenous variables. However, situations one may suspect causal structures subset variables influences others, reverse. cases, VAR can formulated block exogeneity restrictions.Let us partition \\(y_t\\) two subvectors \\(y^{(1)}_{t}\\) (\\(n_1 \\times 1\\)) \\(y^{(2)}_{t}\\) (\\(n_2 \\times 1\\)), \\(y_t' = [{y^{(1)}_{t}}',{y^{(2)}_{t}}']\\) \\(n = n_1 + n_2\\). VAR(1) representation can written \n\\[\n\\begin{bmatrix}\ny^{(1)}_{t} \\\\\ny^{(2)}_{t}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\Phi^{(1,1)} & \\Phi^{(1,2)} \\\\\n\\Phi^{(2,1)} & \\Phi^{(2,2)}\n\\end{bmatrix}\n\\begin{bmatrix}\ny^{(1)}_{t-1} \\\\\ny^{(2)}_{t-1}\n\\end{bmatrix}\n+\n\\varepsilon_t.\n\\]Block exogeneity \\(y^{(2)}_t\\) (.e., \\(y^{(2)}_t\\) respond past values \\(y^{(1)}_t\\)) corresponds null hypothesis\n\\[\n\\Phi^{(2,1)} = 0.\n\\]\nrestriction can tested using likelihood ratio test (see Def. 11.2). logic extends straightforwardly VAR(\\(p\\)) model imposing \\(\\Phi^{(2,1)}_i = 0\\) \\(= 1,\\dots,p\\).Consider model joint dynamics US Korean GDP growth inflation rates. Figure 1.8 shows data used estimation.\nFigure 1.8: GDP log growth rates inflation rates US Korea. Black lines Korea; grey lines US. series year--year growth rates, expressed percent.\nLet us first estimate unrestricted VAR(\\(p\\)) model using data.model log-likelihood stored L0.Let us now estimate retricted model:model log-likelihood stored Lrestr. can perform Likelihood Ratio (LR) test see whether data call exogeneity US variables. According definition (Def. 11.2), LR test statistics given :\n\\[\n\\xi^{LR} =2 \\times (\\hat{L} - \\hat{L}^*),\n\\]\n\\(\\hat{L}\\) \\(\\hat{L}^*\\) , respectively, unrestricted restricted maximum log-likelihoods. null hypothesis block exogeneity satisfied, :\n\\[\n\\xi^{LR} \\sim \\chi^2\\left[p\\left(\\frac{n}{2}\\right)^2\\right].\n\\]context, can therefore apply test follows:Hence, test reject null hypothesis block exogeneity since p-value larger conventional thresholds. terms coding, used, equivalently, lrtest function perform test:Let us compare IRFs models (w/ w/o block exogeneity). Figure 1.9, grey lines correspond contrained model, Korean variables affect US ones. (words, restricted model, US variables exogenous.) Notice , restricted model, last two shocks (\\(\\eta_{3,t}\\) \\(\\eta_{4,t}\\)) affect US variables.\nFigure 1.9: Impulse response functions context VAR models depicting joint dynamics US Korean GDP growth inflation rates. Structural shocks identified using Cholesky decomposition. grey lines correspond contrained model, Korean variables affect US ones. (words, restricted model, US variables exogenous.)\nFinally, let us compute variance decomposition models. Let us start unrestricted model:\nFigure 1.10: Variance decomposition context VAR model depicting joint dynamics US Korean GDP growth inflation rates. Structural shocks identified using Cholesky decomposition. grey lines correspond contrained model, Korean variables affect US ones. (words, restricted model, US variables exogenous.)\nFigure 1.11 shows variance decomposition context restricted model, entails block exogeneity:\nFigure 1.11: Variance decomposition context VAR model depicting joint dynamics US Korean GDP growth inflation rates. Structural shocks identified using Cholesky decomposition. model contrained way Korean variables affect US ones. (words, restricted model, US variables exogenous.)\n","code":"\nlibrary(IdSS) # this package contains the data\nlibrary(vars) # this package contains standard VAR estimation tools\nnames.var <- c(\"GDP_USA\",\"CPI_USA\",\"GDP_KOR\",\"CPI_KOR\")\ndata <- international[,c(\"date\",names.var)]\ndata <- data[complete.cases(data),]\nY <- data[,c(\"GDP_USA\",\"CPI_USA\",\"GDP_KOR\",\"CPI_KOR\")]\npar(mfrow=c(1,2))\npar(plt=c(.15,.95,.15,.8))\nplot(data$date,data$GDP_KOR,type=\"l\",lwd=2,las=1,\n     main=\"GDP growth rate (y-o-y)\",xlab=\"\",ylab=\"\");grid()\nlines(data$date,data$GDP_USA,col=\"grey\",lwd=2,las=1)\nplot(data$date,data$CPI_KOR,type=\"l\",lwd=2,las=1,\n     main=\"Inflation (y-o-y)\",xlab=\"\",ylab=\"\");grid()\nlines(data$date,data$CPI_USA,col=\"grey\",lwd=2,las=1)\np <- 4\nestVAR <- VAR(Y, p = p) # estimate the VAR model\nL0 <- logLik(estVAR)\nn <- dim(Y)[2]\nrestrict <- matrix(1,n,1+p*n) # 'restrict' specifies restrictions; last column = 'c'.\nfor(i in 1:p){\n  restrict[1:(n/2),(n/2+n*(i-1)+1):(i*n)] <- 0\n}\nestVARrestr <- restrict(estVAR, method = \"man\", resmat = restrict)\nLrestr <- logLik(estVARrestr)\nLRstat <- 2*(L0 - Lrestr)\ndf <- length(restrict)-sum(restrict)\npvalue <- 1 - pchisq(LRstat,df=df)\nprint(c(pvalue))## [1] 0.3977775\nlrtest(estVAR,estVARrestr)## Likelihood ratio test\n## \n## Model 1: VAR(y = Y, p = p)\n## Model 2: VAR(y = Y, p = p)\n##   #Df  LogLik  Df  Chisq Pr(>Chisq)\n## 1  68 -1685.5                      \n## 2  52 -1693.9 -16 16.813     0.3978\n# Unrestricted model:\nPhi <- Acoef(estVAR)\neps <- residuals(estVAR)\nOmega <- var(eps) # covariance matrix of OLS residuals\nB <- t(chol(Omega)) # Cholesky decomposition of Omega (lower triangular)\n# Restricted model:\nPhi_restr <- Acoef(estVARrestr)\neps_restr <- residuals(estVARrestr)\nOmega_restr <- var(eps_restr)\nB_restr <- t(chol(Omega_restr))\n\npar(mfrow = c(n, n))\npar(plt = c(.35, .97, .25, .68))\nModel <- list(c = c, Phi = Phi, B = B)\nfor (shock in 1:n) {\n  eta0 <- rep(0, n)\n  eta0[shock] <- 1\n  res.sim <- simul.VAR(c = NaN, Phi, B, nb.sim = 30, indic.IRF = 1,\n                       u.shock = eta0)\n  res.sim_restr <- simul.VAR(c = NaN, Phi_restr, B_restr, nb.sim = 30,\n                             indic.IRF = 1,u.shock = eta0)\n  for (variable in 1:n) {\n    plot(res.sim[, variable], type = \"l\", lwd = 2, las = 1,\n         ylab = ifelse(variable==1,\n                       paste(\"Shock η\", shock, sep = \"\"),\"\"),xlab=\"\",\n         main = ifelse(shock==1,\n                       paste(\"Resp. of \", names.var[variable],sep = \"\"),\"\"))\n    lines(res.sim_restr[, variable], type = \"l\", lwd = 2,col=\"grey\")\n  }\n}\nmake_variance_decompo(Phi, B, maxHorizon = 50,\n                      names.var = names(Y),mfrow = c(2,2))\nmake_variance_decompo(Phi_restr, B_restr, maxHorizon = 50,\n                      names.var = names(Y),mfrow = c(2,2))"},{"path":"basics.html","id":"granger-causality","chapter":"1 VARs and IRFs: the basics","heading":"1.5.2 Granger Causality","text":"Granger (1969) developed method explore causal relationships among variables. approach consists determining whether past values \\(y_{1,t}\\) can help explain current \\(y_{2,t}\\) (beyond information already included past values \\(y_{2,t}\\)).Formally, let us denote three information sets:\n\\[\\begin{eqnarray*}\n\\mathcal{}_{1,t} & = & \\left\\{ y_{1,t},y_{1,t-1},\\ldots\\right\\} \\\\\n\\mathcal{}_{2,t} & = & \\left\\{ y_{2,t},y_{2,t-1},\\ldots\\right\\} \\\\\n\\mathcal{}_{t} & = & \\left\\{ y_{1,t},y_{1,t-1},\\ldots y_{2,t},y_{2,t-1},\\ldots\\right\\}.\n\\end{eqnarray*}\\]\nsay \\(y_{1,t}\\) Granger-causes \\(y_{2,t}\\) \n\\[\n\\mathbb{E}\\left[y_{2,t}\\mid \\mathcal{}_{2,t-1}\\right]\\neq \\mathbb{E}\\left[y_{2,t}\\mid \\mathcal{}_{t-1}\\right].\n\\]get intuition behind testing procedure, consider following\nbivariate VAR(\\(p\\)) process:\n\\[\\begin{eqnarray*}\ny_{1,t} & = & c_1+\\Sigma_{=1}^{p}\\Phi_i^{(11)}y_{1,t-}+\\Sigma_{=1}^{p}\\Phi_i^{(12)}y_{2,t-}+\\varepsilon_{1,t}\\\\\ny_{2,t} & = & c_2+\\Sigma_{=1}^{p}\\Phi_i^{(21)}y_{1,t-}+\\Sigma_{=1}^{p}\\Phi_i^{(22)}y_{2,t-}+\\varepsilon_{2,t},\n\\end{eqnarray*}\\]\n\\(\\Phi_k^{(ij)}\\) denotes element \\((,j)\\) \\(\\Phi_k\\). , \\(y_{1,t}\\) said Granger-cause \\(y_{2,t}\\) \n\\[\n\\Phi_1^{(21)}=\\Phi_2^{(21)}=\\ldots=\\Phi_p^{(21)}=0.\n\\]\nnull alternative hypotheses therefore :\n\\[\n\\begin{cases}\nH_{0}: & \\Phi_1^{(21)}=\\Phi_2^{(21)}=\\ldots=\\Phi_p^{(21)}=0\\\\\nH_{1}: & \\Phi_1^{(21)}\\neq0\\mbox{ }\\Phi_2^{(21)}\\neq0\\mbox{ }\\ldots\\Phi_p^{(21)}\\neq0.\\end{cases}\n\\]\nLoosely speaking, reject \\(H_{0}\\) coefficients lagged \\(y_{1,t}\\)’s statistically significant. Formally, can tested using \\(F\\)-test asymptotic chi-square test. \\(F\\)-statistic \n\\[\nF=\\frac{(RSS-USS)/p}{USS/(T-2p-1)},\n\\]\nRSS Restricted sum squared residuals USS Unrestricted sum squared residuals. \\(H_{0}\\), \\(F\\)-statistic distributed \\(\\mathcal{F}(p,T-2p-1)\\) (See Table 11.4).6According following lines code, output gap Granger-causes inflation, reverse true:","code":"\ngrangertest(x=as.matrix(US3var[,c(\"y.gdp.gap\",\"infl\")]),order=3)## Granger causality test\n## \n## Model 1: infl ~ Lags(infl, 1:3) + Lags(y.gdp.gap, 1:3)\n## Model 2: infl ~ Lags(infl, 1:3)\n##   Res.Df Df      F   Pr(>F)   \n## 1    214                      \n## 2    217 -3 3.9761 0.008745 **\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\ngrangertest(x=as.matrix(US3var[,c(\"infl\",\"y.gdp.gap\")]),order=3)## Granger causality test\n## \n## Model 1: y.gdp.gap ~ Lags(y.gdp.gap, 1:3) + Lags(infl, 1:3)\n## Model 2: y.gdp.gap ~ Lags(y.gdp.gap, 1:3)\n##   Res.Df Df      F Pr(>F)\n## 1    214                 \n## 2    217 -3 1.5451 0.2038"},{"path":"identifStruct.html","id":"identifStruct","chapter":"2 Identification problem and standard identification techniques","heading":"2 Identification problem and standard identification techniques","text":"","code":""},{"path":"identifStruct.html","id":"IdentifPbm","chapter":"2 Identification problem and standard identification techniques","heading":"2.1 The identification problem","text":"Section 1.4, seen estimate \\(\\mathbb{V}ar(\\varepsilon_t) =\\Omega\\) \\(\\Phi_k\\) matrices context VAR model. IRFs functions \\(B\\) \\(\\Phi_k\\)’s, \\(\\Omega\\) \\(\\Phi_k\\)’s (see Section 1.2). \\(\\Omega = BB'\\), provides restrictions components \\(B\\), sufficient fully identify \\(B\\). Indeed, seen system equations whose unknowns \\(b_{,j}\\)’s (components \\(B\\)), system \\(\\Omega = BB'\\) contains \\(n(n+1)/2\\) linearly independent equations. instance, \\(n=2\\):\n\\[\\begin{eqnarray*}\n&&\\left[\n\\begin{array}{cc}\n\\omega_{11} & \\omega_{12} \\\\\n\\omega_{12} & \\omega_{22}\n\\end{array}\n\\right] = \\left[\n\\begin{array}{cc}\nb_{11} & b_{12} \\\\\nb_{21} & b_{22}\n\\end{array}\n\\right]\\left[\n\\begin{array}{cc}\nb_{11} & b_{21} \\\\\nb_{12} & b_{22}\n\\end{array}\n\\right]\\\\\n&\\Leftrightarrow&\\left[\n\\begin{array}{cc}\n\\omega_{11} & \\omega_{12} \\\\\n\\omega_{12} & \\omega_{22}\n\\end{array}\n\\right] = \\left[\n\\begin{array}{cc}\nb_{11}^2+b_{12}^2 & \\color{red}{b_{11}b_{21}+b_{12}b_{22}} \\\\\n\\color{red}{b_{11}b_{21}+b_{12}b_{22}} & b_{22}^2 + b_{21}^2\n\\end{array}\n\\right].\n\\end{eqnarray*}\\]3 linearly independent equations 4 unknowns. Therefore, \\(B\\) identified based second-order moments. Additional restrictions required identify \\(B\\). section covers two standard identification schemes: short-run long-run restrictions.short-run restriction (SRR) prevents structural shock affecting endogenous variable contemporaneously.easy implement: appropriate entries \\(B\\) set 0.particular (popular) case Cholesky, recursive approach (Sections 2.3 2.4).Examples include Bernanke (1986), Sims (1986), Galí (1992), Ruibio-Ramírez, Waggoner, Zha (2010).long-run restriction (LRR) prevents structural shock cumulative impact one endogenous variables (Section 2.5).Additional computations required implement . One needs compute cumulative effect one structural shocks \\(u_{t}\\) one endogenous variable.Examples include Blanchard Quah (1989), Faust Leeper (1997), Galí (1999), Erceg, Guerrieri, Gust (2005), Christiano, Eichenbaum, Vigfusson (2007).illustrated Section 2.6, two approaches can combined (see, e.g., Gerlach Smets (1995)).","code":""},{"path":"identifStruct.html","id":"a-stylized-example-motivating-short-run-restrictions","chapter":"2 Identification problem and standard identification techniques","heading":"2.2 A stylized example motivating short-run restrictions","text":"Let us consider simple example motivate short-run restrictions. Consider following stylized macro model:\n\\[\\begin{equation}\n\\begin{array}{clll}\ng_{t}&=& \\bar{g}-\\lambda(i_{t-1}-\\mathbb{E}_{t-1}\\pi_{t})+ \\underbrace{{\\color{blue}\\sigma_d \\eta_{d,t}}}_{\\mbox{demand shock}}& (\\mbox{curve})\\\\\n\\Delta \\pi_{t} & = & \\beta (g_{t} - \\bar{g})+ \\underbrace{{\\color{blue}\\sigma_{\\pi} \\eta_{\\pi,t}}}_{\\mbox{cost push shock}} & (\\mbox{Phillips curve})\\\\\ni_{t} & = & \\rho i_{t-1} + \\left[ \\gamma_\\pi \\mathbb{E}_{t}\\pi_{t+1}  + \\gamma_g (g_{t} - \\bar{g}) \\right]\\\\\n&& \\qquad \\qquad+\\underbrace{{\\color{blue}\\sigma_{mp} \\eta_{mp,t}}}_{\\mbox{Mon. Pol. shock}} & (\\mbox{Taylor rule}),\n\\end{array}\\tag{2.1}\n\\end{equation}\\]\n:\n\\[\\begin{equation}\n\\eta_t =\n\\left[\n\\begin{array}{c}\n\\eta_{\\pi,t}\\\\\n\\eta_{d,t}\\\\\n\\eta_{mp,t}\n\\end{array}\n\\right]\n\\sim ..d.\\,\\mathcal{N}(0,).\\tag{2.2}\n\\end{equation}\\]Vector \\(\\eta_t\\) assumed vector structural shocks, mutually serially independent. date \\(t\\):\\(g_t\\) contemporaneously affected \\(\\eta_{d,t}\\) ;\\(\\pi_t\\) contemporaneously affected \\(\\eta_{\\pi,t}\\) \\(\\eta_{d,t}\\);\\(i_t\\) contemporaneously affected \\(\\eta_{mp,t}\\), \\(\\eta_{\\pi,t}\\) \\(\\eta_{d,t}\\).System (2.1) rewritten follows:\n\\[\\begin{equation}\n\\left[\\begin{array}{c}\ng_t\\\\\n\\pi_t\\\\\ni_t\n\\end{array}\\right]\n= \\Phi(L)\n\\left[\\begin{array}{c}\ng_{t-1}\\\\\n\\pi_{t-1}\\\\\ni_{t-1} +\n\\end{array}\\right] +\\underbrace{\\underbrace{\n\\left[\n\\begin{array}{ccc}\n0 & \\bullet & 0 \\\\\n\\bullet & \\bullet & 0 \\\\\n\\bullet & \\bullet & \\bullet\n\\end{array}\n\\right]}_{=B} \\eta_t.}_{=\\varepsilon_t}\\tag{2.3}\n\\end{equation}\\]reduced-form model. representation suggests three additional restrictions entries \\(B\\); latter matrix therefore identified soon \\(\\Omega = BB'\\) known (signs columns).","code":""},{"path":"identifStruct.html","id":"Cholesky","chapter":"2 Identification problem and standard identification techniques","heading":"2.3 Cholesky: a specific short-run-restriction situation","text":"particular cases well-known matrix decomposition \\(\\Omega=\\mathbb{V}ar(\\varepsilon_t)\\) can used easily estimate specific SVAR. case -called Cholesky decomposition. Consider following context:first shock (say, \\(\\eta_{n_1,t}\\)) can affect instantaneously\n(.e., date \\(t\\)) one endogenous variable (say, \\(y_{n_1,t}\\));second shock (say, \\(\\eta_{n_2,t}\\)) can affect instantaneously\n(.e., date \\(t\\)) two endogenous variables, \\(y_{n_1,t}\\) () \\(y_{n_2,t}\\);\\(\\dots\\)impliesthat column \\(n_1\\) \\(B\\) 1 non-zero entry (\\(n_1^{th}\\) entry),column \\(n_2\\) \\(B\\) 2 non-zero entries (\\(n_1^{th}\\) \\(n_2^{th}\\) ones), etc.Without loss generality, can set \\(n_1=n\\), \\(n_2=n-1\\), etc. context, matrix \\(B\\) lower triangular. Cholesky decomposition \\(\\Omega_{\\varepsilon}\\) provides appropriate estimate \\(B\\), since matrix decomposition yields lower triangular matrix satisfying:\n\\[\n\\Omega_\\varepsilon = BB'.\n\\]instance, Dedola Lippi (2005) estimate 5 structural VAR models US, UK, Germany, France Italy analyse monetary-policy transmission mechanisms. estimate SVAR(5) models period 1975-1997. shock-identification scheme based Cholesky decompositions, ordering endogenous variables : (1) industrial production, (2) consumer price index, (3) commodity price index, (4) short-term rate, (5) monetary aggregate (6) effective exchange rate (except US). ordering implies monetary policy (.e., short-term rate) reacts shocks affecting first three variables latter react monetary policy shocks one-period lag .Consider small structural example discussed , whose reduced-form VAR representation given Eq. (2.3). reorder structural shocks \\(\\eta_t = (\\eta_{d,t},\\eta_{\\pi,t},\\eta_{mp,t})'\\) (order arbitrary), impact matrix \\(B\\) can written \n\\[\nB = \\begin{bmatrix}\n\\bullet & 0 & 0 \\\\\n\\bullet & \\bullet & 0 \\\\\n\\bullet & \\bullet & \\bullet\n\\end{bmatrix},\n\\]\n, \\(B\\) corresponds Cholesky factor \\(\\Omega\\).However, always possible obtain lower-triangular \\(B\\) matrix reordering structural shocks. instance, given ordering \n\\[\nB = \\begin{bmatrix}\n0 & \\bullet & \\bullet \\\\\n\\bullet & 0 & \\bullet \\\\\n\\bullet & \\bullet & 0\n\\end{bmatrix},\n\\]\nreordering shocks produce lower-triangular structure.Cholesky approach used previous chapter, Example 1.6 (see Figure 1.5).","code":""},{"path":"identifStruct.html","id":"Cholesky1","chapter":"2 Identification problem and standard identification techniques","heading":"2.4 Using Cholesky to identify a single shock","text":"cases, Cholesky approach can employed interest centers single structural shock. case, example, Christiano, Eichenbaum, Evans (1996). identification based following relationship \\(\\varepsilon_t\\) \\(\\eta_t\\):\n\\[\\begin{equation}\n\\left[\\begin{array}{c}\n\\boldsymbol\\varepsilon_{S,t}\\\\\n\\varepsilon_{r,t}\\\\\n\\boldsymbol\\varepsilon_{F,t}\n\\end{array}\\right] =\n\\left[\\begin{array}{ccc}\nB_{SS} & 0 & 0 \\\\\nB_{rS} & B_{rr} & 0 \\\\\nB_{FS} & B_{Fr} & B_{FF}\n\\end{array}\\right]\n\\left[\\begin{array}{c}\n\\boldsymbol\\eta_{S,t}\\\\\n\\eta_{r,t}\\\\\n\\boldsymbol\\eta_{F,t}\n\\end{array}\\right],\\tag{2.4}\n\\end{equation}\\]\n\\(S\\), \\(r\\) \\(F\\) respectively correspond slow-moving variables, policy variable (short-term rate) fast-moving variables. \\(\\eta_{r,t}\\) scalar, \\(\\boldsymbol\\eta_{S,t}\\) \\(\\boldsymbol\\eta_{F,t}\\) may vectors. space spanned \\(\\boldsymbol\\varepsilon_{S,t}\\) spanned \\(\\boldsymbol\\eta_{S,t}\\). result, \\(\\varepsilon_{r,t}\\) linear combination \\(\\eta_{r,t}\\) \\(\\boldsymbol\\eta_{S,t}\\) (\\(\\perp\\)), comes \\(B_{rr}\\eta_{r,t}\\)’s (population) residuals regression \\(\\varepsilon_{r,t}\\) \\(\\boldsymbol\\varepsilon_{S,t}\\). \\(\\mathbb{V}ar(\\eta_{r,t})=1\\), \\(B_{rr}\\) given square root variance \\(B_{rr}\\eta_{r,t}\\). \\(B_{F,r}\\) finally obtained regressing components \\(\\boldsymbol\\varepsilon_{F,t}\\) estimates \\(\\eta_{r,t}\\).critical observation matrices \\(B\\) block form Eq. ((ref?)(eq:BCEE)) (\\(B_{rr}>0\\)) satisfying \\(\\Omega=BB'\\) share intermediate column (, \\(B_{rr}\\) \\(B_{Fr}\\)). One \\(B\\) obtained directly Cholesky decomposition \\(\\Omega=BB'\\).\nFigure 2.1: Response monetary-policy shock. Identification approach Christiano, Eichenbaum Evans (1996). Confidence intervals obtained boostrapping estimated VAR model (see inference section).\n","code":"\nlibrary(IdSS)\nlibrary(vars)\ndata(\"USmonthly\")\n# Select sample period:\nFirst.date <- \"1965-01-01\";Last.date <- \"1995-06-01\"\nindic.first <- which(USmonthly$DATES==First.date)\nindic.last  <- which(USmonthly$DATES==Last.date)\nUSmonthly   <- USmonthly[indic.first:indic.last,]\nconsidered.variables <- c(\"LIP\",\"UNEMP\",\"LCPI\",\"LPCOM\",\"FFR\",\"NBR\",\"TTR\",\"M1\")\ny <- as.matrix(USmonthly[considered.variables])\nres.svar.ordering <- svar.ordering(y,p=3,\n                                   posit.of.shock = 5,\n                                   nb.periods.IRF = 20,\n                                   nb.bootstrap.replications = 100,\n                                   confidence.interval = 0.90, # expressed in pp.\n                                   indic.plot = 1 # Plots are displayed if = 1.\n                                   )"},{"path":"identifStruct.html","id":"LRrestr","chapter":"2 Identification problem and standard identification techniques","heading":"2.5 Long-run restrictions","text":"Let us now turn long-run restrictions. restriction concerns long-run influence shock endogenous variable. Let us consider instance structural shock assumed “long-run influence” GDP. express ? long-run change GDP can expressed \\(GDP_{t+h} - GDP_t\\), \\(h\\) large. Note :\n\\[\nGDP_{t+h} - GDP_t = \\Delta GDP_{t+h} +\\Delta GDP_{t+h-1} + \\dots + \\Delta GDP_{t+1}.\n\\]\nHence, fact given structural shock (\\(\\eta_{,t}\\), say) long-run influence GDP means \n\\[\n\\lim_{h\\rightarrow\\infty}\\frac{\\partial GDP_{t+h}}{\\partial \\eta_{,t}} = \\lim_{h\\rightarrow\\infty} \\frac{\\partial}{\\partial \\eta_{,t}}\\left(\\sum_{k=1}^h \\Delta  GDP_{t+k}\\right)= 0.\n\\]shown following sections, long-run effect can formulated function \\(B\\) matrices \\(\\Phi_i\\) \\(y_t\\) (including \\(\\Delta GDP_t\\)) follows VAR process.","code":""},{"path":"identifStruct.html","id":"the-var1-case","chapter":"2 Identification problem and standard identification techniques","heading":"2.5.1 The VAR(1) case","text":"Let us start VAR(1) case. :\n\\[\\begin{eqnarray*}\ny_{t} &=& c+\\Phi y_{t-1}+\\varepsilon_{t}\\\\\n& = & c+\\varepsilon_{t}+\\Phi(c+\\varepsilon_{t-1})+\\ldots+\\Phi^{k}(c+\\varepsilon_{t-k})+\\ldots \\\\\n& = & \\mu +\\varepsilon_{t}+\\Phi\\varepsilon_{t-1}+\\ldots+\\Phi^{k}\\varepsilon_{t-k}+\\ldots \\\\\n& = & \\mu +B\\eta_{t}+\\Phi B\\eta_{t-1}+\\ldots+\\Phi^{k}B\\eta_{t-k}+\\ldots,\n\\end{eqnarray*}\\]\nWold representation \\(y_t\\).sequence shocks \\(\\{\\eta_t\\}\\) determines sequence \\(\\{y_t\\}\\). \\(\\{\\eta_t\\}\\) replaced \\(\\{\\tilde{\\eta}_t\\}\\), \\(\\tilde{\\eta}_t=\\eta_t\\) \\(t \\ne s\\) \\(\\tilde{\\eta}_s=\\eta_s + \\gamma\\)? Assume \\(\\{\\tilde{y}_t\\}\\) associated “perturbated” sequence. \\(\\tilde{y}_t = y_t\\) \\(t<s\\). \\(t \\ge s\\), Wold decomposition \\(\\{\\tilde{y}_t\\}\\) implies:\n\\[\n\\tilde{y}_t = y_t + \\Phi^{t-s} B \\gamma.\n\\]\nTherefore, cumulative impact \\(\\gamma\\) \\(\\tilde{y}_t\\) (\\(t \\ge s\\)):\n\\[\\begin{eqnarray}\n(\\tilde{y}_t - y_t) +  (\\tilde{y}_{t-1} - y_{t-1}) + \\dots +  (\\tilde{y}_s - y_s) &=& \\nonumber \\\\\n(Id + \\Phi + \\Phi^2 + \\dots + \\Phi^{t-s}) B \\gamma.&& \\tag{2.5}\n\\end{eqnarray}\\]Consider shock \\(\\eta_{1,t}\\), magnitude \\(1\\). shock corresponds \\(\\gamma = [1,0,\\dots,0]'\\). Given Eq. (2.5), long-run cumulative effect shock endogenous variables given :\n\\[\n\\underbrace{\\underbrace{(Id+\\Phi+\\ldots+\\Phi^{k}+\\ldots)}_{=(Id - \\Phi)^{-1}}B}_{=: \\Theta}\\left[\\begin{array}{c}\n1\\\\\n0\\\\\n\\vdots\\\\\n0\\end{array}\\right],\n\\]\nfirst column \\(n \\times n\\) matrix \\(\\Theta := (Id - \\Phi)^{-1}B\\).context, consider following long-run restriction: “\\(j^{th}\\) structural shock cumulative impact \\(^{th}\\) endogenous variable”. equivalent \n\\[\n\\Theta_{ij}=0,\n\\]\n\\(\\Theta_{ij}\\) element \\((,j)\\) \\(\\Theta\\).\\(n(n-1)/2\\) restrictions type can made, \\(B\\) identified. particular, case case \\(\\Theta\\) lower-triangular, problem admits analytical solution. Indeed, let \\(J = (- \\Phi)^{-1}\\) (assumed invertible). want \\(\\Theta = JB\\) lower triangular \\(\\Omega = BB'\\). Since \\(B = J^{-1} \\Theta\\), \n\\[\n\\Omega = BB' = J^{-1} \\Theta \\Theta' (J^{-1})' \\;\\Rightarrow\\; J\\Omega J' = \\Theta \\Theta'.\n\\]\nSince \\(\\Omega\\) positive definite \\(J\\) invertible, \\(\\Sigma = J\\Omega J'\\) positive definite. Take (unique, positive diagonal) Cholesky factorization \\(\\Sigma = \\Theta \\Theta'\\) \\(\\Theta\\) lower triangular. set \\(B = J^{-1} \\Theta\\). \\(B\\) satisfies \\(\\Omega = BB'\\) \\((- \\Phi)^{-1}B = \\Theta\\) lower triangular.","code":""},{"path":"identifStruct.html","id":"the-varp-case","chapter":"2 Identification problem and standard identification techniques","heading":"2.5.2 The VAR(\\(p\\)) case","text":"Several developments made still valid VAR(\\(p\\)) case since VAR(\\(p\\)) process can rewritten VAR(1) process augmenting state vector. specifically, stack last \\(p\\) values vector \\(y_t\\) vector \\(y_{t}^{*}=[y_t',\\dots,y_{t-p+1}']'\\); Eq. (1.1) can rewritten companion form:\n\\[\\begin{equation}\ny_{t}^{*} =\n\\underbrace{\\left[\\begin{array}{c}\nc\\\\\n0_{n \\times 1}\\\\\n\\vdots\\\\\n0_{n \\times 1}\\end{array}\\right]}_{=c^*}+\n\\underbrace{\\left[\\begin{array}{cccc}\n\\Phi_{1} & \\Phi_{2} & \\cdots & \\Phi_{p}\\\\\n& 0 & \\cdots & 0\\\\\n0 & \\ddots & 0 & 0\\\\\n0 & 0 & & 0\\end{array}\\right]}_{=\\Phi}\ny_{t-1}^{*}+\n\\underbrace{\\left[\\begin{array}{c}\n\\varepsilon_{t}\\\\\n0_{n \\times 1}\\\\\n\\vdots\\\\\n0_{n \\times 1}\\end{array}\\right]}_{\\varepsilon_t^*},\\tag{2.6}\n\\end{equation}\\]\nmatrices \\(\\Phi\\) \\(\\Omega^*:= \\mathbb{V}ar(\\varepsilon_t^*)\\) dimension \\(np \\times np\\). Matrix \\(\\Phi\\) introduced Eq. (1.9), \n\\[\n\\Omega^* := \\mathbb{V}ar(\\varepsilon_t^*)=\n\\left[\\begin{array}{c}\nB \\\\\n0_{n \\times n}  \\\\\n\\vdots \\\\\n0_{n \\times n}\n\\end{array}\\right]\\left[\\begin{array}{cccc}\nB & 0_{n \\times n} & \\dots & 0_{n \\times n}\n\\end{array}\\right]\n= \\left[\\begin{array}{cccc}\n\\Omega & 0_{n \\times n} & \\dots & 0_{n \\times n} \\\\\n0_{n \\times n} & 0_{n \\times n} \\\\\n\\vdots &&\\ddots\\\\\n0_{n \\times n} & \\dots &&0_{n \\times n}\n\\end{array}\\right].\n\\]\ncontext, long-run effect \\(y_t^*+y_{t+1}^*+y_{t+2}^*+\\dots\\) change \\(\\eta_t\\) \\(\\gamma\\) (happens date \\(t\\)) given :\n\\[\n\\underbrace{(Id+\\Phi+\\ldots+\\Phi^{k}+\\ldots)}_{=(Id - \\Phi)^{-1}}\\left[\\begin{array}{c}\nB \\\\\n0_{n \\times n}  \\\\\n\\vdots \\\\\n0_{n \\times n}\n\\end{array}\\right]\\gamma.\n\\]\nparticular, cumulated effect shock \\(y_t\\) (\\(y_t^*\\) anymore) \\(J B \\gamma\\), \\(J\\) upper-left \\(n \\times n\\) submatrix \\((- \\Phi)^{-1}\\). Let us denote matrix \\(J B\\) \\(\\Theta\\). notations, long-run restriction: \\(j^{th}\\) structural shock cumulative impact \\(^{th}\\) endogenous variable equivalent fact component \\((,j)\\) \\(\\Theta\\) equal zero.\\(n(n-1)/2\\) restrictions type can made, \\(B\\) identified. particular, case \\(\\Theta\\) lower triangular, problem admits analytical solution. Using \\(B = J^{-1} \\Theta\\), get\n\\[\n\\Omega = B B' = J^{-1} \\Theta \\Theta' (J^{-1})' \\;\\Rightarrow\\; J \\Omega J' = \\Theta \\Theta'.\n\\]\nSince \\(\\Omega\\) positive definite, \\(\\Sigma = J \\Omega J'\\) positive definite. Take (unique, positive diagonal) Cholesky factorization \\(\\Sigma = \\Theta \\Theta'\\) \\(\\Theta\\) lower triangular. set \\(B = J^{-1} \\Theta\\). \\(B\\) satisfies \\(\\Omega = B B'\\) \\(J B = \\Theta\\) lower triangular.","code":""},{"path":"identifStruct.html","id":"blanchard-and-quah-1989","chapter":"2 Identification problem and standard identification techniques","heading":"2.5.3 Blanchard and Quah (1989)","text":"Blanchard Quah (1989) implemented long-run restrictions small-scale VAR. Two variables considered: GDP unemployment. Consequently, VAR affected two types shocks. Specifically, authors want identify supply shocks (can permanent effect output) demand shocks (permanent effect output).7Blanchard Quah (1989)’s dataset quarterly, spanning period 1950:2 1987:4. VAR features 8 lags. data use:\nFigure 2.2: data come Blanchard Quah (1989). GDP growth rates calculated first differences logarithm GDP.\nEstimate reduced-form VAR(8) model:Let us employ approach described :Alternatively, one may adopt numerical approach defining loss function (loss) equals zero conditions satisfied: () \\(BB' = \\Omega\\), (b) \\((1,1)\\) element \\(\\Theta = (- \\Phi)^{-1}B\\) equal zero.(Note: one can use type approach, based loss function, mix short- long-run restrictions.)Figure 2.3 displays resulting IRFs. Note , GDP, cumulate GDP growth IRF, response GDP level.\nFigure 2.3: IRF GDP unemployment demand supply shocks.\n","code":"\nlibrary(IdSS)\ndata(BQ)\npar(mfrow=c(1,2))\nplot(BQ$Date,BQ$Dgdp,type=\"l\",main=\"GDP quarterly growth rate\",\n     xlab=\"\",ylab=\"\",lwd=2)\nplot(BQ$Date,BQ$unemp,type=\"l\",ylim=c(-3,6),main=\"Unemployment rate (gap)\",\n     xlab=\"\",ylab=\"\",lwd=2)\nlibrary(vars)\ny <- BQ[,2:3]\nn <- dim(y)[2]\np <- 8\nest.VAR <- VAR(y,p=p)\nOmega <- var(residuals(est.VAR))\nPhi <- Acoef(est.VAR)\nPHI <- make.PHI(Phi)\nA <- diag(n*p) - PHI\nJ <- solve(A)[1:n,1:n]\nSigma <- J %*% Omega %*% t(J)\nL <- t(chol(Sigma))\nB <- solve(J) %*% L\nprint(B)##           [,1]       [,2]\n## [1,] 0.1541392 -0.8570365\n## [2,] 0.1921245  0.2396346\n# Compute (Id - Phi)^{-1}:\nPhi <- Acoef(est.VAR)\nPHI <- make.PHI(Phi)\nsum.PHI.k <- solve(diag(dim(PHI)[1]) - PHI)[1:2,1:2]\nloss <- function(param){\n  B <- matrix(param,2,2)\n  X <- Omega - B %*% t(B)\n  Theta <- sum.PHI.k[1:2,1:2] %*% B\n  loss <- 10000 * ( X[1,1]^2 + X[2,1]^2 + X[2,2]^2 + Theta[1,1]^2 )\n  return(loss)\n}\nres.opt <- optim(c(1,0,0,1),loss,method=\"BFGS\",hessian=FALSE)\nprint(res.opt$par)## [1]  0.8570358 -0.2396345  0.1541395  0.1921221\nB.hat <- matrix(res.opt$par,2,2)\nprint(cbind(Omega,B.hat %*% t(B.hat)))##             Dgdp       unemp                       \n## Dgdp   0.7582704 -0.17576173  0.7582694 -0.17576173\n## unemp -0.1757617  0.09433658 -0.1757617  0.09433558\nnb.sim <- 40\npar(mfrow=c(2,2));par(plt=c(.15,.95,.15,.8))\nY <- simul.VAR(c=matrix(0,2,1),Phi,B.hat,nb.sim,y0.star=rep(0,2*8),\n               indic.IRF = 1,u.shock = c(1,0))\nplot(cumsum(Y[,1]),type=\"l\",lwd=2,xlab=\"\",ylab=\"\",main=\"Demand shock on GDP\")\nplot(Y[,2],type=\"l\",lwd=2,xlab=\"\",ylab=\"\",main=\"Demand shock on UNEMP\")\nY <- simul.VAR(c=matrix(0,2,1),Phi,B.hat,nb.sim,y0.star=rep(0,2*8),\n               indic.IRF = 1,u.shock = c(0,1))\nplot(cumsum(Y[,1]),type=\"l\",lwd=2,xlab=\"\",ylab=\"\",main=\"Supply shock on GDP\")\nplot(Y[,2],type=\"l\",lwd=2,xlab=\"\",ylab=\"\",main=\"Supply shock on UNEMP\")"},{"path":"identifStruct.html","id":"Mixing","chapter":"2 Identification problem and standard identification techniques","heading":"2.6 Mixing short- and long-run restrictions","text":"seen matrix \\(B\\) can identified imposing \\(n(n-1)/2\\) restrictions short-run long-run impacts shocks. also possible combine short-run long-run restrictions, provided total number still equals \\(n(n-1)/2\\).practice, hybrid identification schemes often need implemented numerically, closed-form expressions always available. illustrate, consider three-variable model including first difference log GDP, inflation, short-term interest rate. aim identify three structural shocks. assume existence supply shock, shock long-run effect GDP, two demand shocks (provides two restrictions). Among latter, one monetary policy shock, identified imposing contemporaneous impact GDP.following code chunk implements identification strategy using Canadian data:","code":"\nlibrary(vars)   # provides 'VARselect' function\nlibrary(IdSS)   # dataset is included here\n# Make dataset: ----------------------------------------------------------------\nctry <- \"CAN\" # select country in the \"international\" dataset\ndata <- international[,c(\"date\",\n                         paste(\"GDP_\",ctry,sep=\"\"),\n                         paste(\"CPI_\",ctry,sep=\"\"),\n                         paste(\"STR_\",ctry,sep=\"\"))]\ndata <- data[complete.cases(data),] # remove missing data\nTT <- dim(data)[1] # sample length\ndata$dy <- data$GDP_\ndata$pi <- data$CPI_\ndata$str <- data$STR_\nY <- data[c(\"dy\",\"pi\",\"str\")]\nn <- dim(Y)[2]\n# VAR estimation: --------------------------------------------------------------\np <- 4\nestVAR <- VAR(Y, p = p) # estimate the VAR model\nPhi <- Acoef(estVAR)\neps <- residuals(estVAR)\nOmega <- var(eps) # covariance matrix of OLS residuals\n# Indentify B using mixed approach: --------------------------------------------\n# Compute (Id - Phi)^{-1}:\nPHI <- make.PHI(Phi)\nsum.PHI.k <- solve(diag(dim(PHI)[1]) - PHI)[1:n,1:n]\n# Define loss function:\nloss <- function(param){\n  B <- matrix(param,n,n)\n  X <- Omega - B %*% t(B)\n  Theta <- sum.PHI.k[1:n,1:n] %*% B\n  loss <- 100000 * ( sum(X^2) + Theta[1,2]^2 + Theta[1,3]^2 + B[1,3]^2)\n  return(loss)\n}\nres.opt <- optim(c(diag(n)),loss,method=\"BFGS\",hessian=FALSE)\nB <- matrix(res.opt$par,n,n)\n# Make IRFs: -------------------------------------------------------------------\nn <- dim(Y)[2] # number of endogenous variables\npar(mfrow = c(n, n))\npar(plt = c(.3, .95, .2, .75))\nModel <- list(c = c, Phi = Phi, B = B)\nnames.var <- c(\"log(GDP)\", \"Inflation\", \"Interest rate\")\nfor (variable in 1:n) {\n  for (shock in 1:n) {\n    eta0 <- rep(0, n)\n    eta0[shock] <- 1\n    res.sim <- simul.VAR(c = NaN, Phi, B, nb.sim = 60, indic.IRF = 1,\n                         u.shock = eta0)\n    if(variable==1){# For GDP, show cumulated growth rates (i.e., log(GDP)):\n      irf <- cumsum(res.sim[, variable])\n    }else{\n      irf <- res.sim[, variable]\n    }\n    plot(irf, type = \"l\", lwd = 2, las = 1,\n         xlab = \"\", ylab = ifelse(shock==1,\n                                  paste(\"Resp. of \",\n                                              names.var[variable],\"...\",sep=\"\"),\"\"),\n         main = ifelse(variable==1,paste(\"... to shock η\", shock, sep = \"\"),\"\"))}}"},{"path":"Inference.html","id":"Inference","chapter":"3 Inference","heading":"3 Inference","text":"Consider following SVAR model:\n\\[y_t = \\Phi_1 y_{t-1} + \\dots + \\Phi_p y_{t-p} + \\varepsilon_t\\]\n\\(\\varepsilon_t\\) white noise sequence. vector rewrites \\(\\varepsilon_t=B\\eta_t\\) \\(\\mathbb{V}ar(\\eta_t)=\\), implies \\(\\Omega_\\varepsilon=BB'\\).corresponding infinite MA representation (Eq. (1.4), Wold representation, :\n\\[\ny_t = \\sum_{h=0}^\\infty\\Psi_h \\eta_{t-h},\n\\]\n\\(\\Psi_0=B\\) \\(h=1,2,\\dots\\):\n\\[\n\\Psi_h = \\sum_{j=1}^h\\Psi_{h-j}\\Phi_j,\n\\]\n\\(\\Phi_j=0\\) \\(j>p\\) (see Prop. 1.1 recursive computation \\(\\Psi_j\\)’s).Inference VAR coefficients \\(\\{\\Phi_j\\}_{j=1,...,p}\\) straightforward (standard OLS inference). inference complicated regarding IRFs. Indeed, shown previous equation, (infinite) MA coefficients \\(\\{\\Psi_j\\}_{j=1,...}\\) non-linear functions \\(\\{\\Phi_j\\}_{j=1,...,p}\\) \\(\\Omega_\\varepsilon\\). issue pertain small sample bias: typically, persistent processes, auto-regressive parameters known downward biased.main inference methods following:Monte Carlo method (Hamilton (1994)) (Section 3.1);Asymptotic normal approximation (Lütkepohl (1990)), Delta method (Section 3.2);Bootstrap method (Kilian (1998)) (Sections 3.3 3.4).","code":""},{"path":"Inference.html","id":"MonteCarlo","chapter":"3 Inference","heading":"3.1 Monte Carlo method","text":"use Monte Carlo need approximate distribution variable whose distribution unknown (: \\(\\Psi_j\\)’s) function another variable whose distribution known (, \\(\\Phi_j\\)’s).instance, suppose know distribution random variable \\(X\\), takes values \\(\\mathbb{R}\\), density function \\(p\\). Assume want compute mean \\(\\varphi(X)\\). :\n\\[\n\\mathbb{E}(\\varphi(X))=\\int_{-\\infty}^{+\\infty}\\varphi(x)p(x)dx\n\\]\nSuppose integral simple expression. compute \\(\\mathbb{E}(\\varphi(X))\\) , virtue law large numbers, can approximate follows:\n\\[\n\\mathbb{E}(\\varphi(X))\\approx\\frac{1}{N}\\sum_{=1}^N\\varphi(X^{()}),\n\\]\n\\(\\{X^{()}\\}_{=1,...,N}\\) \\(N\\) independent draws \\(X\\). generally, distribution \\(\\varphi(X)\\) can approximated empirical distribution \\(\\varphi(X^{()})\\)’s. Typically, 10’000 values \\(\\varphi(X^{()})\\) drawn, \\(5^{th}\\) percentile p.d.f. \\(\\varphi(X)\\) can approximated \\(500^{th}\\) value 10’000 draws \\(\\varphi(X^{()})\\) (arranging values ascending order).regards computation confidence intervals around IRFs, one think \\(\\{\\widehat{\\Phi}_j\\}_{j=1,...,p}\\), \\(\\widehat{\\Omega}\\) \\(X\\) \\(\\{\\widehat{\\Psi}_j\\}_{j=1,...}\\) \\(\\varphi(X)\\). (Proposition 1.4 provides us asymptotic distribution “\\(X\\).”)summarize, steps one can implement derive confidence intervals IRFs using Monte-Carlo approach: iteration \\(k\\),Draw \\(\\{\\widehat{\\Phi}_j^{(k)}\\}_{j=1,...,p}\\) \\(\\widehat{\\Omega}^{(k)}\\) asymptotic distribution (using Proposition 1.4).Compute matrix \\(B^{(k)}\\) \\(\\widehat{\\Omega}^{(k)}=B^{(k)}B^{(k)'}\\), according identification strategy.Compute associated IRFs \\(\\{\\widehat{\\Psi}_j\\}^{(k)}\\).Perform \\(N\\) replications report median impulse response (confidence intervals).following code implements Monte Carlo method.\nFigure 3.1: IRF associated monetary policy shock; Monte Carlo method.\nplot estimated IRFs mom^netary policy shock, together sample 50 IRFs simulated Monte Carlo method, median simulated IRFs.\nFigure 3.2: Estimated IRF associated monetary policy shock (black line) simulated IRFs (red lines) median IRF (blue line); Monte Carlo method.\n","code":"\nlibrary(IdSS);library(vars);library(Matrix)\ndata(\"USmonthly\")\nFirst.date <- \"1965-01-01\"\nLast.date <- \"1995-06-01\"\nindic.first <- which(USmonthly$DATES==First.date)\nindic.last  <- which(USmonthly$DATES==Last.date)\nUSmonthly   <- USmonthly[indic.first:indic.last,]\nconsidered.variables<-c(\"LIP\",\"UNEMP\",\"LCPI\",\"LPCOM\",\"FFR\",\"NBR\",\"TTR\",\"M1\")\ny <- as.matrix(USmonthly[considered.variables])\n# ===================================\n# CEE with Monte Carlo\n# ===================================\nres.svar.ordering <-\n  svar.ordering.2(y,p=3,\n                  posit.of.shock = 5,\n                  nb.periods.IRF = 20,\n                  inference = 3,# 0 -> no inference, 1 -> parametric bootst.,\n                  # 2 <- non-parametric bootstrap, 3 <- Monte Carlo,\n                  # 4 <- bootstrap-after-bootstrap\n                  nb.draws = 200,\n                  confidence.interval = 0.90, # expressed in pp.\n                  indic.plot = 1 # Plots are displayed if = 1.\n  )\nIRFs.ordering <- res.svar.ordering$IRFs\nmedian.IRFs.ordering <- res.svar.ordering$all.CI.median\nsimulated.IRFs.ordering <- res.svar.ordering$simulated.IRFs\npar(mfrow=c(1,1))\nplot(IRFs.ordering[,5],type=\"l\")\nfor(i in 1:50){\n  lines(simulated.IRFs.ordering[,5,i],col=\"red\",type=\"l\")\n}\nlines(IRFs.ordering[,5],col=\"black\",type=\"l\")\nlines(median.IRFs.ordering[,5],col=\"blue\",type=\"l\")"},{"path":"Inference.html","id":"Delta","chapter":"3 Inference","heading":"3.2 Delta method","text":"Suppose \\(\\beta\\) vector parameters \\(\\hat\\beta\\) estimator \n\\[\n\\sqrt{T}\\,(\\hat\\beta-\\beta)\\overset{d}{\\longrightarrow}\\mathcal{N}(0,\\Sigma_\\beta),\n\\]\n\\(\\overset{d}{\\longrightarrow}\\) denotes convergence distribution, \\(\\mathcal{N}(0,\\Sigma_\\beta)\\) denotes multivariate normal distribution mean vector \\(0\\) covariance matrix \\(\\Sigma_\\beta\\), \\(T\\) sample size.Let \\(\\varphi(\\beta) = (\\varphi_1(\\beta),\\dots,\\varphi_m(\\beta))'\\) continuously differentiable function values \\(\\mathbb{R}^m\\), assume \\(\\partial \\varphi_i/\\partial \\beta'\\) nonzero \\(\\beta\\) \\(= 1,\\dots,m\\). , \\(\\varphi\\) nonlinear neighborhood \\(\\beta\\), approximately \n\\[\n\\sqrt{T}\\,(\\varphi(\\hat\\beta)-\\varphi(\\beta))\n\\overset{d}{\\approx}\n\\mathcal{N}\\!\\left(\n0,\\;\n\\frac{\\partial \\varphi}{\\partial \\beta'}\n\\Sigma_\\beta\n\\frac{\\partial \\varphi'}{\\partial \\beta}\n\\right),\n\\]\n\\(\\frac{\\partial \\varphi}{\\partial \\beta'}\\) denotes Jacobian \\(\\varphi\\) respect \\(\\beta'\\).Using property, Lütkepohl (1990) provides asymptotic distributions \\(\\Psi_j\\)’s.limit last two approaches (Monte Carlo Delta method) rely asymptotic results normality assumption. Boostrapping approaches robust small-sample non-normal situations.","code":""},{"path":"Inference.html","id":"Bootstrap1","chapter":"3 Inference","heading":"3.3 Bootstrap","text":"IRFs’ confidence intervals intervals 90% (95%, 75%, …) IRFs lie, repeat estimation large number times similar conditions (\\(T\\) observations). obviously , one sample: \\(\\{y_t\\}_{t=1,..,T}\\). can try construct samples.Bootstrapping consists :re-sampling \\(N\\) times, .e., constructing \\(N\\) samples \\(T\\) observations, using estimated\nVAR coefficients anda sample residuals distribution \\(\\mathcal{N}(0,BB')\\) (parametric approach), ora sample residuals drawn randomly set actual estimated residuals \\(\\{\\hat\\varepsilon_t\\}_{t=1,..,T}\\). (non-parametric approach).re-estimating SVAR \\(N\\) times.algorithm non-parametric approach:Construct sample\n\\[\ny_t^{(k)}=\\widehat{\\Phi}_1 y_{t-1}^{(k)} + \\dots + \\widehat{\\Phi}_p y_{t-p}^{(k)} + \\hat\\varepsilon_t^{(k)},\n\\]\n\\(\\hat\\varepsilon_{t}^{(k)}=\\hat\\varepsilon_{s_t^{(k)}}\\), \\(\\{s_1^{(k)},..,s_T^{(k)}\\}\\) random set \\(\\{1,..,T\\}^T\\). (Note: parametric approach, draw \\(\\hat\\varepsilon_{t}^{(k)}\\) \\(\\mathcal{N}(0,BB')\\) distribution)Re-estimate SVAR compute IRFs \\(\\{\\widehat{\\Psi}_j\\}^{(k)}\\).Perform \\(N\\) replications report median impulse response (confidence intervals).following code implements bootstrap method.\nFigure 3.3: IRF associated monetary policy shock; bootstrap method.\n","code":"\nlibrary(IdSS);library(vars);library(Matrix)\ndata(\"USmonthly\")\nFirst.date <- \"1965-01-01\"\nLast.date <- \"1995-06-01\"\nindic.first <- which(USmonthly$DATES==First.date)\nindic.last  <- which(USmonthly$DATES==Last.date)\nUSmonthly   <- USmonthly[indic.first:indic.last,]\nconsidered.variables<-c(\"LIP\",\"UNEMP\",\"LCPI\",\"LPCOM\",\"FFR\",\"NBR\",\"TTR\",\"M1\")\ny <- as.matrix(USmonthly[considered.variables])\n# ===================================\n# CEE with bootstrap\n# ===================================\nres.svar.ordering <-\n  svar.ordering.2(y,p=3,\n                  posit.of.shock = 5,\n                  nb.periods.IRF = 20,\n                  inference = 2,# 0 -> no inference, 1 -> parametric bootstr.,\n                  # 2 <- non-parametric bootstrap, 3 <- monte carlo,\n                  # 4 <- bootstrap-after-bootstrap\n                  nb.draws = 200,\n                  confidence.interval = 0.90, # expressed in pp.\n                  indic.plot = 1 # Plots are displayed if = 1.\n  )\nIRFs.ordering.bootstrap <- res.svar.ordering$IRFs\nmedian.IRFs.ordering.bootstrap <- res.svar.ordering$all.CI.median\nsimulated.IRFs.ordering.bootstrap <- res.svar.ordering$simulated.IRFs"},{"path":"Inference.html","id":"BootstrapAfter","chapter":"3 Inference","heading":"3.4 Bootstrap-after-bootstrap","text":"previous simple bootstrapping procedure deals non-normality small sample distribution, since use actual residuals. However, deal small sample bias, stemming, particular, small-sample bias associated OLS coefficient estimates \\(\\{\\widehat{\\Phi}_j\\}_{j=1,..,p}\\).code illustrates small sample bias.\nFigure 3.4: Estimated bootstrapped coefficients.\ndistribution bootstrapped coefficients centered around estimated coefficients.following code, perform VAR estimation bootstrap inference generating artificial data. can compare IRFs confidence intervals ``true’’ parameters used generate data.\nFigure 3.5: Simulated IRF associated monetary policy shock.\n\nFigure 3.6: IRF associated monetary policy shock; sign-restriction approach.\nmain idea bootstrap--bootstrap Kilian (1998) run two consecutive boostraps: objective first compute bias, can used correct initial estimates \\(\\Phi_i\\)’s. , corrected estimates used —second boostrap— compute set IRFs (standard boostrap).formally, algorithm follows:Estimate SVAR coefficients \\(\\{\\widehat{\\Phi}_j\\}_{j=1,..,p}\\) \\(\\widehat{\\Omega}\\)First bootstrap. iteration \\(k\\):Construct sample\n\\[\ny_t^{(k)}=\\widehat{\\Phi}_1 y_{t-1}^{(k)} + \\dots + \\widehat{\\Phi}_p y_{t-p}^{(k)} + \\hat\\varepsilon_t^{(k)},\n\\]\n\\(\\hat\\varepsilon_{t}^{(k)}=\\hat\\varepsilon_{s_t^{(k)}}\\), \\(\\{s_1^{(k)},..,s_T^{(k)}\\}\\) random set \\(\\{1,..,T\\}^T\\).Re-estimate VAR compute coefficients \\(\\{\\widehat{\\Phi}_j\\}_{j=1,..,p}^{(k)}\\).Perform \\(N\\) replications compute median coefficients \\(\\{\\widehat{\\Phi}_j\\}_{j=1,..,p}^*\\).Approximate bias terms \\(\\widehat{\\Theta}_j=\\widehat{\\Phi}_j^*-\\widehat{\\Phi}_j\\).Construct bias-corrected terms \\(\\widetilde{\\Phi}_j=\\widehat{\\Phi}_j-\\widehat{\\Theta}_j\\).Second bootstrap. iteration \\(k\\):Construct sample now \n\\[\ny_t^{(k)}=\\widetilde{\\Phi}_1 y_{t-1}^{(k)} + \\dots + \\widetilde{\\Phi}_p y_{t-p}^{(k)} + \\hat\\varepsilon_t^{(k)}.\n\\]Re-estimate VAR compute coefficients \\(\\{\\widehat{\\Phi}^*_j\\}_{j=1,..,p}^{(k)}\\).Construct bias-corrected estimates \\(\\widetilde{\\Phi}_j^{*(k)}=\\widehat{\\Phi}_j^{*(k)}-\\widehat{\\Theta}_j\\).Compute associated IRFs \\(\\{\\widetilde{\\Psi}_j^{*(k)}\\}_{j\\ge 1}\\).Perform \\(N\\) replications compute median confidence interval set IRFs.noted correcting bias can generate non-stationary results (\\(\\tilde \\Phi\\) eigenvalue modulus \\(>1\\)). Solution (Kilian (1998)):step 5, check largest eigenvalue \\(\\tilde\\Phi\\) modulus <1.\n, shrink bias: \\(j\\)s, set \\(\\widehat{\\Theta}_j^{(+1)}=\\delta_{+1}\\widehat{\\Theta}_j^{()}\\), \\(\\delta_{+1}=\\delta_i-0.01\\), starting \\(\\delta_1=1\\) \\(\\widehat{\\Theta}_j^{(1)} =\\widehat{\\Theta}_j\\), compute \\(\\widetilde{\\Phi}_j^{(+1)}=\\widehat{\\Phi}_j-\\widehat{\\Theta}_j^{(+1)}\\) largest eigenvalue \\(\\tilde\\Phi^{(+1)}\\) modulus <1.following code implements bootstrap--bootrap method.\nFigure 3.7: IRF associated monetary policy shock; sign-restriction approach.\nalternative, function VAR.Boot package VAR.etp (Kim (2022)) can used operate bias-correction approach Kilian (1998):","code":"\n# Distribution of coefficients stemming from non-parametric bootstrap\nn <- length(considered.variables)\nh <- 5\npar(mfrow=c(2,ifelse(round(n/2)==n/2,n/2,(n+1)/2)))\nfor (i in 1:n){\n  hist(simulated.IRFs.ordering.bootstrap[h,i,],xlab=\"\",ylab=\"\",\n       main=paste(\"Effect at h = \",h,\" on \",\n                  considered.variables[i],sep=\"\"),cex.main=.9)\n  lines(array(c(IRFs.ordering.bootstrap[h,i],\n                IRFs.ordering.bootstrap[h,i],0,100),c(2,2)),col=\"red\")\n  lines(array(c(median.IRFs.ordering.bootstrap[h,i],\n                median.IRFs.ordering.bootstrap[h,i],0,100),c(2,2)),col=\"blue\")\n  text(IRFs.ordering.bootstrap[h,i],25,label=\"Estimated coef.\",col=\"red\")\n}\n# Simulate a small sample\nest.VAR <- VAR(y,p=3)\nPhi     <- Acoef(est.VAR)\ncst     <- Bcoef(est.VAR)[,3*n+1]\nresids  <- residuals(est.VAR)\nOmega   <- var(resids)\nB.hat   <- t(chol(Omega))\ny0.star <- NULL\nfor(k in 3:1){\n  y0.star <- c(y0.star,y[k,])\n}\nsmall.sample <- simul.VAR(c=rep(0,dim(y)[2]),\n                          Phi,\n                          B.hat,\n                          nb.sim = 100,\n                          y0.star,\n                          indic.IRF = 0)\ncolnames(small.sample)  <- considered.variables\n# Estimate the VAR with the small sample\nres.svar.small.sample <-\n  svar.ordering.2(small.sample,p=3,\n                  posit.of.shock = 5,\n                  nb.periods.IRF = 20,\n                  inference = 2,# 0 -> no inference, 1 -> parametric bootstr.,\n                  # 2 <- non-parametric bootstrap, 3 <- monte carlo\n                  nb.draws = 200,\n                  confidence.interval = 0.90, # expressed in pp.\n                  indic.plot = 1 # Plots are displayed if = 1.\n  )\nIRFs.small.sample <- res.svar.small.sample$IRFs\nmedian.IRFs.small.sample <- res.svar.small.sample$all.CI.median\nsimulated.IRFs.small.sample <- res.svar.small.sample$simulated.IRFs\n# True IRFs\nres.svar.ordering <-\n  svar.ordering.2(y,p=3,\n                  posit.of.shock = 5,\n                  nb.periods.IRF = 20,\n                  inference = 0,# 0 -> no inference, 1 -> parametric bootstr.,\n                  # 2 <- non-parametric bootstrap, 3 <- monte carlo,\n                  # 4 <- bootstrap-after-bootstrap\n                  indic.plot = 0 # Plots are displayed if = 1.\n  )\nIRFs.ordering.true <- res.svar.ordering$IRFs\n\n# Distribution of coefficients resulting from the small sample VAR\nh <- 5\npar(mfrow=c(2,ifelse(round(n/2)==n/2,n/2,(n+1)/2)))\nfor (i in 1:n){\n  hist(simulated.IRFs.small.sample[h,i,],xlab=\"\",ylab=\"\",\n       main=paste(\"Effect at h = \",h,\" on \",\n                  considered.variables[i],sep=\"\"),cex.main=.9)\n  lines(array(c(IRFs.small.sample[h,i],\n                IRFs.small.sample[h,i],0,100),c(2,2)),col=\"red\")\n  lines(array(c(median.IRFs.small.sample[h,i],\n                median.IRFs.small.sample[h,i],0,100),c(2,2)),col=\"blue\")\n  lines(array(c(IRFs.ordering.true[h,i],\n                IRFs.ordering.true[h,i],0,100),c(2,2)),col=\"black\")\n  text(IRFs.small.sample[h,i],25,label=\"Estimated coef.\",col=\"red\")\n  text(IRFs.ordering.true[h,i],30,label=\"True coef.\",col=\"black\")\n}\nlibrary(IdSS);library(vars);library(Matrix)\ndata(\"USmonthly\")\nFirst.date <- \"1965-01-01\"\nLast.date <- \"1995-06-01\"\nindic.first <- which(USmonthly$DATES==First.date)\nindic.last  <- which(USmonthly$DATES==Last.date)\nUSmonthly   <- USmonthly[indic.first:indic.last,]\nconsidered.variables<-c(\"LIP\",\"UNEMP\",\"LCPI\",\"LPCOM\",\"FFR\",\"NBR\",\"TTR\",\"M1\")\ny <- as.matrix(USmonthly[considered.variables])\n# ===================================\n# CEE with bootstrap-after-bootstrap\n# ===================================\nres.svar.ordering <-\n  svar.ordering.2(y,p=3,\n                  posit.of.shock = 5,\n                  nb.periods.IRF = 20,\n                  inference = 4,# 0 -> no inference, 1 -> parametric bootstr.,\n                  # 2 <- non-parametric bootstrap, 3 <- monte carlo,\n                  # 4 <- bootstrap-after-bootstrap\n                  nb.draws = 200,\n                  confidence.interval = 0.90, # expressed in pp.\n                  indic.plot = 1 # Plots are displayed if = 1.\n  )\nIRFs.ordering <- res.svar.ordering$IRFs\nmedian.IRFs.ordering <- res.svar.ordering$all.CI.median\nsimulated.IRFs.ordering <- res.svar.ordering$simulated.IRFs\nlibrary(VAR.etp)\nlibrary(vars) #standard VAR models\ndata(dat) # part of VAR.etp package\ncorrected <- VAR.Boot(dat,p=2,nb=200,type=\"const\")\nnoncorrec <- VAR(dat,p=2)\nrbind(corrected$coef[1,],\n      (corrected$coef+corrected$Bias)[1,],\n      noncorrec$varresult$inv$coefficients)##         inv(-1)   inc(-1)  con(-1)    inv(-2)   inc(-2)   con(-2)       const\n## [1,] -0.3208081 0.1546886 1.048479 -0.1451155 0.1356996 0.9721137 -0.02042110\n## [2,] -0.3196310 0.1459888 0.961219 -0.1605511 0.1146050 0.9343938 -0.01672199\n## [3,] -0.3196310 0.1459888 0.961219 -0.1605511 0.1146050 0.9343938 -0.01672199"},{"path":"Signs.html","id":"Signs","chapter":"4 Sign restrictions","heading":"4 Sign restrictions","text":"identifiy structural shocks, need find matrix \\(B\\) satisfies \\(\\Omega = BB'\\) (\\(\\Omega = \\mathbb{V}ar(\\varepsilon_t)\\)) restrictions. Indeed, explained , \\(\\Omega = BB'\\) sufficient identify \\(B\\) since, take orthogonal matrix \\(Q\\) (see Def. 4.1), \\(\\mathcal{P}=BQ\\) also satisfies \\(\\Omega = \\mathcal{P}\\mathcal{P}'\\).Definition 4.1  (Orthogonal matrix) orthogonal matrix \\(Q\\) matrix \\(QQ' = ,\\) .e., columns (rows) \\(Q\\) \northogonal unit vectors:\n\\[q_i'q_j=0\\text{ }\\neq j\\text{ }q_i'q_j=1\\text{ }= j,\\]\n\\(q_i\\) \\(^{th}\\) column \\(Q\\).","code":""},{"path":"Signs.html","id":"the-approach","chapter":"4 Sign restrictions","heading":"4.1 The approach","text":"idea behind sign-restriction approach “draw” random matrices \\(\\mathcal{P}\\) satisfy \\(\\Omega = \\mathcal{P}\\mathcal{P}'\\), constitute set admissible matrices, keeping set simulated \\(\\mathcal{P}\\) matrices satisfy predefined sign-based restriction. example restriction “one year, contractionary monetary-policy shocks negative impact inflation”.suggested , \\(B\\) matrix satisfies \\(\\Omega = BB'\\) (instance, \\(B\\) can based Cholesky decomposition \\(\\Omega\\)), also \\(\\Omega = \\mathcal{P}\\mathcal{P}'\\) soon \\(\\mathcal{P}=BQ\\), \\(Q\\) orthogonal matrix. Therefore, draw \\(\\mathcal{P}\\) matrices, suffices draw set orthogonal matrices.fix ideas, consider dimension 2. case, orthogonal matrices rotation matrices, set orthogonal matrices can parameterized angle \\(x\\), :\n\\[\nQ_x=\\begin{pmatrix}\\cos(x)&\\cos\\left(x+\\frac{\\pi}{2}\\right)\\\\\n\\sin(x)&\\sin\\left(x+\\frac{\\pi}{2}\\right)\\end{pmatrix}=\\begin{pmatrix}\\cos(x)&-\\sin(x)\\\\\n\\sin(x)&\\cos(x)\\end{pmatrix}.\n\\]\n(angle-\\(x\\) counter-clockwise rotation.) Hence, case, drawing \\(x\\) randomly \\([0,2\\pi]\\), draw randomly set \\(2\\times2\\) rotation matrices. high-dimensional VAR, lose simple geometrical representation, though. always possible parametrize rotation matrix (high-dimensional VARs).proceed, ? Arias, Rubio-Ramírez, Waggoner (2018) provide procedure. approach based -called \\(QR\\) decomposition: square matrix \\(X\\) may decomposed \\(X=QR\\) \\(Q\\) orthogonal matrix \\(R\\) upper diagonal matrix. mind, propose two-step approach:Draw random matrix \\(X\\) drawing element independent standard normal distribution.Let \\(X = QR\\) \\(QR\\) decomposition \\(X\\) diagonal \\(R\\) normalized \npositive. random matrix \\(Q\\) orthogonal draw uniform distribution set orthogonal matrices.Equipped procedure, sign-restriction based following algorithm:Draw random orthogonal matrix \\(Q\\) (using step . ii. described ).Compute \\(B = PQ\\) \\(P\\) Cholesky decomposition reduced form residuals \\(\\Omega_{\\varepsilon}\\).Compute impulse response associated \\(B\\) \\(y_{t,t+k}=\\Phi^kB\\) cumulated response \\(\\bar y_{t,t+k}=\\sum_{j=0}^{k}\\Phi^jB\\).sign restrictions satisfied?Yes. Store impulse response set admissible response.. Discard impulse response.Perform \\(N\\) replications report median impulse response (“confidence” intervals).Note: take account uncertainty \\(B\\) \\(\\Phi\\), can draw \\(B\\) \\(\\Phi\\) Steps 2 3 using inference method (see Section 3).sign-restriction approach method advantage relatively agnostic. Moreover, fairly flexible, one can impose sign restrictions variable, horizon.","code":""},{"path":"Signs.html","id":"an-example","chapter":"4 Sign restrictions","heading":"4.2 An example","text":"prominent example Uhlig (2005). Using US monthly data 1965.2003.XII, employs sign restrictions estimate effect monetary policy shocks.According conventional wisdom, monetary contractions :8Raise federal funds rate,Lower prices,Decrease non-borrowed reserves,Reduce real output.restrictions considered Uhlig (2005) follows: expansionary monetary policy shock leads :Increases pricesIncrease nonborrowed reservesDecreases federal funds rateWhat output? Since response interest, leave un-restricted.\nFigure 4.1: IRF associated monetary policy shock; sign-restriction approach.\nstressed sign restriction approach lead unique IRF, set admissible IRFs. Accordingly, say approach set-identified, point-identified.","code":"\nlibrary(IdSS);library(vars);library(Matrix)\ndata(\"USmonthly\")\nFirst.date <- \"1965-01-01\"\nLast.date <- \"1995-06-01\"\nindic.first <- which(USmonthly$DATES==First.date)\nindic.last  <- which(USmonthly$DATES==Last.date)\nUSmonthly   <- USmonthly[indic.first:indic.last,]\nconsidered.variables<-c(\"LIP\",\"UNEMP\",\"LCPI\",\"LPCOM\",\"FFR\",\"NBR\",\"TTR\",\"M1\")\nn <- length(considered.variables)\ny <- as.matrix(USmonthly[considered.variables])\nsign.restrictions <- list()\nhorizon <- list()\n#Define sign restrictions and horizon for restrictions\nfor(i in 1:n){\n  sign.restrictions[[i]] <- matrix(0,n,n)\n  horizon[[i]] <- 1\n}\n# Sign restrictions on shock 1 (monetary shock)\nsign.restrictions[[1]][1,3] <- 1  # positive impact on price level\nsign.restrictions[[1]][2,5] <- -1 # negative impact on interest rate\nsign.restrictions[[1]][3,6] <- 1 # positive impact on non-borrowed reserves\nhorizon[[1]] <- 1:5 # from horizon 1 to 5\nres.svar.signs <- \n  svar.signs(y,p=3,\n             nb.shocks = 1, #number of identified shocks\n             nb.periods.IRF = 20,\n             bootstrap.replications = 1, # = 1 if no bootstrap, = N if bootstrap\n             confidence.interval = 0.90, # expressed in pp.\n             indic.plot = 1, # Plots are displayed if = 1.\n             nb.draws = 10000, # number of draws\n             sign.restrictions,\n             horizon,\n             recursive =1 #  =0 <- draw Q directly, =1 <- draw q recursively\n  )\n# Output\nIRFs.signs <- res.svar.signs$IRFs.signs # all the simulated IRFs\nnb.rotations <- res.svar.signs$xx # total number of rotations\nall.CI.median <- res.svar.signs$all.CI.median # median IRFs for the selected shocks\nall.CI.lower.bounds <- res.svar.signs$all.CI.lower.bounds # lower-bound IRFs for the selected shocks\nall.CI.upper.bounds <- res.svar.signs$all.CI.upper.bounds # upper-bound IRFs for the selected shocks"},{"path":"Signs.html","id":"the-penalty-function-approach-pfa","chapter":"4 Sign restrictions","heading":"4.3 The penalty-function approach (PFA)","text":"alternative approach -called penalty-function approach (PFA, Uhlig (2005), present Danne (2015)’s package). approach relies penalty function:\n\\[\nf(x)=\n\\begin{cases}\nx, & \\text{} x \\le 0,\\\\[6pt]\n100x, & \\text{} x > 0.\n\\end{cases}\n\\]penalizes positive responses rewards negative responses.Let \\(\\psi_k^j(q)\\) impulse response variable \\(j\\). \\(\\psi_k^j(q)\\)’s elements \\(\\psi_k(q)=\\Psi_kq\\).Let \\(\\sigma_j\\) standard deviation variable \\(j\\). Let \\(\\iota_{j,k}=1\\) restrict response variable \\(j\\) \\(k^th\\) horizon negative, \\(\\iota_{j,k}=-1\\) restrict positive, \\(\\iota_{j,k}=0\\) restriction. total penalty given \\[\n\\mathbf{P}(q)=\\sum_{j=1}^m\\sum_{k=0}^Kf\\left(\\iota_{j,k}\\frac{\\psi_k^j(q)}{\\sigma_j}\\right).\n\\]looking solution \n\\[\\begin{array}{ll}\n&\\min_q \\mathbf{P}(q)\\\\\n\\text{s.t. }&q'q=1.\\end{array}\\]problem solved numerically.","code":""},{"path":"Signs.html","id":"NarrativeSign","chapter":"4 Sign restrictions","heading":"4.4 Narrative sign restrictions","text":"related approach, introduced Antolín-Díaz Rubio-Ramírez (2018), consists imposing , specific dates (based narrative evidence), signs shocks positive (negative).9 instance, Antolín-Díaz Rubio-Ramírez (2018) argue one rule structural parameters disagree view “negative oil supply shock occurred outbreak Gulf War August 1990.”Suppose want impose restriction , dates \\(\\{t_1,\\dots,t_J\\}\\), signs \\(j^{th}\\) shock positive. , narrative sign restrictions simply imposed :\n\\[\n\\hat{\\eta}_{j,t}(B) = e_j'\\hat\\eta_{t}(B) > 0,\n\\]\n\\(\\hat\\eta_{t}(B)\\) vector structural shock associated given matrix \\(B\\) (\\(e_j\\) \\(j^{th}\\) column \\(n \\times n\\) identity matrix).","code":""},{"path":"SignsZeros.html","id":"SignsZeros","chapter":"5 Combining sign and zero restrictions","heading":"5 Combining sign and zero restrictions","text":"Sometimes need combine different types restrictions. instance:One shock satisfies zero sign restrictions.shocks can identified zero restrictions (SR LR), others sign restrictions.shocks satisfy zero restrictions (e.g. LR effect output) can distinguished sign restrictions.instances, must make independent draws set structural parameters satisfying zero restrictions. ? Arias, Rubio-Ramírez, Waggoner (2018) propose impose zero restrictions \\(B\\), check signs. Remember, \\(\\mathcal{P}=BQ\\) candidate impact IRF. structural shock \\(j\\), define \\(m\\)-column matrices \\(Z_j\\) (zero restrictions) \\(S_j\\) (sign restrictions). row \\(Z_j\\) (resp. \\(S_j\\)) defines zero (resp. sign) restriction. \\(Z_j\\) \\(m-j\\) rows (.e., \\(m-j\\) zero restriction ).Example 5.1  4-variable VAR, want impose first structural shock effect variable 1, affects positively variable 2 negatively variable 3 impact:\n\\[Z_1 = \\begin{pmatrix}1 & 0 & 0 & 0\\end{pmatrix}, \\]\n\\[S_1 = \\begin{pmatrix}0 & 1 & 0 & 0\\\\\n0 & 0 & -1 & 0\\end{pmatrix}. \\]zero sign restrictions satisfied, must \n\\[\nZ_jb_j=0 \\quad \\mbox{} \\quad S_jb_j>0,\n\\]\n\\(b_j\\) \\(j^{th}\\) column \\(B\\), .e. impact effect \\(j^{th}\\) structural shock.algorithm follows:\\(1\\le j\\le m\\), draw \\(u_j\\\\mathbb{R}^{m+1-j-z_j}\\) standard normal distribution (\\(z_j\\) number zero restrictions imposed \\(j^{th}\\) shock) set \\(w_j = u_j/||u_j||\\).Define \\(Q= \\begin{pmatrix}q_1&...&q_m\\end{pmatrix}\\) recursively \\(q_j = K_jw_j\\) matrix \\(K_j\\) whose columns form orthogonal basis null space matrix \\[M_j =\n\\begin{pmatrix} q_1&...&q_{j-1}&\\color{blue}{(Z_jP)'}\\end{pmatrix}'.\\] (Vector \\(q_j\\) orthogonal \\(\\begin{pmatrix} q_1&...&q_{j-1}\\end{pmatrix}\\) satisfy zero restriction.)Set \\(B=PQ\\).Check sign restrictions (\\(S_jb_j>0\\) \\(j\\)?).Perform \\(N\\) replications report median impulse response (confidence intervals).Function svar.signs can run algorithm. called follows:10\nFigure 5.1: IRFs; sign-restriction approach.\n\nFigure 5.2: IRFs; sign-restriction approach.\n\nFigure 5.3: IRFs; sign-restriction approach.\n\nFigure 5.4: IRFs; sign-restriction approach.\n\nFigure 5.5: IRFs; sign-restriction approach.\n\nFigure 5.6: IRFs; sign-restriction approach.\n","code":"\nlibrary(IdSS);library(vars);library(Matrix)\ndata(\"USmonthly\")\nFirst.date <- \"1965-01-01\"\nLast.date <- \"1995-06-01\"\nindic.first <- which(USmonthly$DATES==First.date)\nindic.last  <- which(USmonthly$DATES==Last.date)\nUSmonthly   <- USmonthly[indic.first:indic.last,]\nconsidered.variables<-c(\"LIP\",\"UNEMP\",\"LCPI\",\"LPCOM\",\"FFR\",\"NBR\",\"TTR\",\"M1\")\nn <- length(considered.variables)\ny <- as.matrix(USmonthly[considered.variables])\nsign.restrictions <- list()\nSR.restrictions <- list()\nhorizon <- list()\n\n#Define sign restrictions and horizon for restrictions\nfor(i in 1:n){\n  sign.restrictions[[i]] <- matrix(0,n,n)\n  horizon[[i]] <- 1\n}\n# 2 shocks on the demand for reserves\nsign.restrictions[[1]][1,6] <- 1 \nsign.restrictions[[2]][1,7] <- 1\n# 3 shocks that drive an endogenous response of the interest rate\nsign.restrictions[[3]][1,1] <- 1\nsign.restrictions[[3]][2,5] <- 1\nsign.restrictions[[4]][1,2] <- -1\nsign.restrictions[[4]][2,5] <- 1\nsign.restrictions[[5]][1,3] <- 1\nsign.restrictions[[5]][2,5] <- 1\n# monetary policy shock\nsign.restrictions[[6]][1,5] <- -1\nsign.restrictions[[6]][2,3] <- 1\nsign.restrictions[[6]][3,6] <- 1\n# horizon for sign restrictions on monetary policy shock\nhorizon[[6]] <- 1:5\n\n#Define zero restrictions\n# 2 shocks on the demand for reserves\nSR.restrictions[[1]] <- array(0,c(1,n))\nSR.restrictions[[1]][1,5] <- 1 # no impact on the interest rate\nSR.restrictions[[2]] <- array(0,c(1,n))\nSR.restrictions[[2]][1,5] <- 1 # no impact on the interest rate\nfor(i in 3:n){\n  SR.restrictions[[i]] <- array(0,c(0,n))\n}\n\nres.svar.signs.zeros <- svar.signs(y,p=3,\n                                  nb.shocks = 6, #number of identified shocks\n                                  nb.periods.IRF = 20,\n                                  bootstrap.replications = 100, # = 1 if no bootstrap, = N if bootstrap\n                                  confidence.interval = 0.90, # expressed in pp.\n                                  indic.plot = 1, # Plots are displayed if = 1.\n                                  nb.draws = 10000, # number of draws\n                                  sign.restrictions,\n                                  horizon,\n                                  recursive =0,\n                                  SR.restrictions\n)\n# Output\nIRFs.signs <- res.svar.signs.zeros$IRFs.signs # all the simulated IRFs\nnb.rotations <- res.svar.signs.zeros$xx # total number of rotations\nall.CI.median <- res.svar.signs.zeros$all.CI.median # median IRFs for the selected shocks\nall.CI.lower.bounds <- res.svar.signs.zeros$all.CI.lower.bounds # lower-bound IRFs for the selected shocks\nall.CI.upper.bounds <- res.svar.signs.zeros$all.CI.upper.bounds # upper-bound IRFs for the selected shocks"},{"path":"forecast-error-variance-maximization.html","id":"forecast-error-variance-maximization","chapter":"6 Forecast error variance maximization","heading":"6 Forecast error variance maximization","text":"","code":""},{"path":"forecast-error-variance-maximization.html","id":"the-main-unconditional-approach","chapter":"6 Forecast error variance maximization","heading":"6.1 The main (unconditional) approach","text":"approach presented section exploits derivations Uhlig (2004).Consider process \\(\\{y_t\\}\\) admits infinite MA representation Eq. (1.4). Let \\(Q\\) orthogonal matrix, alternative decomposition \\(y_t\\) :\n\\[\\begin{eqnarray}\ny_t&=&\\sum_{h=0}^{+\\infty}\\Psi_h\\underbrace{\\eta_{t-h}}_{Q\\tilde \\eta_{t-h}} = \\sum_{h=0}^{+\\infty}\\underbrace{\\Psi_hQ}_{\\tilde\\Psi_h}\\tilde\n\\eta_{t-h} = \\sum_{h=0}^{+\\infty}\\tilde\\Psi_h\\tilde \\eta_{t-h},\n\\end{eqnarray}\\]\n\\(\\tilde \\eta_{t-h}=Q'\\eta_{t-h}\\) white-noise shocks associated new MA representation, \\(Q\\) orthgonal matrix. (also satisfy \\(\\mathbb{V}ar(\\tilde\\eta_t)=Id\\).)\\(h\\)-step ahead prediction error \\(y_{t+h}\\), given data , including, \\(t-1\\) given \n\\[\ne_{t+h}(h)=y_{t+h}-\\mathbb{E}_{t-1}(y_{t+h})=\\sum_{j=0}^h\\tilde \\Psi_h\\tilde \\eta_{t+h-j}.\n\\]variance-covariance matrix \\(e_{t+h}(h)\\) \n\\[\n\\Omega^{(h)}=\\sum_{j=0}^h\\tilde \\Psi_j\\tilde \\Psi_j'=\\sum_{j=0}^h \\Psi_j \\Psi_j'.\n\\]can decompose \\(\\Omega^{(h)}\\) contribution shock \\(l\\) (\\(l^{th}\\) component \\(\\tilde{\\eta}_t\\)):\n\\[\n\\Omega^{(h)}=\\sum_{l=1}^n\\Omega_l^{(h)}(Q)\n\\]\n\n\\[\n\\Omega_l^{(h)}(Q) =\\sum_{j=0}^h(\\Psi_jq_l)(\\Psi_jq_l)',\n\\]\n\\(q_l\\) \\(l^{th}\\) column \\(Q\\).decomposition can used objective finding impulse vector \\(b\\) s.t. explains much possible sum \\(h\\)-step ahead prediction error variance variable \\(\\), say, prediction horizons \\(h \\[\\underline{h} , \\overline{h}]\\).Formally, task explain much possible variance\n\\[\n\\sigma^2(\\underline{h},\\overline{h},q_1)=\\sum_{h=\\underline{h}}^{\\overline{h}} \\sum_{j=0}^h\\left[(\\Psi_jq_1)(\\Psi_jq_1)'\\right]_{,}\n\\]\nsingle impulse vector \\(q_1\\).Denote \\(E_{ii}\\) matrix filled zeros, except (\\(,\\)) entry, set 1. :\n\\[\\begin{eqnarray*}\n\\sigma^2(\\underline{h},\\overline{h},q_1)&=&\\sum_{h=\\underline{h}}^{\\overline{h}} \\sum_{j=0}^h\\left[(\\Psi_jq_1)(\\Psi_jq_1)'\\right]_{,}=\\sum_{h=\\underline{h}}^{\\overline{h}} \\sum_{j=0}^h Tr\\left[E_{ii}(\\Psi_jq_1)(\\Psi_jq_1)'\\right]\\\\\n&=&\\sum_{h=\\underline{h}}^{\\overline{h}} \\sum_{j=0}^h Tr\\left[q_1'\\Psi_j'E_{ii}\\Psi_j q_1\\right]\\\\\n&=& q_1'Sq_1,\n\\end{eqnarray*}\\]\n\n\\[\\begin{eqnarray*}\n\\begin{array}{lll}S&=&\\sum_{h=\\underline{h}}^{\\overline{h}}\\sum_{j=0}^{h}\\Psi_j'E_{ii}\\Psi_j\\\\\n&=&\\sum_{j=0}^{\\overline{h}}(\\overline{h}+1-max(\\underline{h},j))\\Psi_j'E_{ii}\\Psi_j\\\\\n&=&\\sum_{j=0}^{\\overline{h}}(\\overline{h}+1-max(\\underline{h},j))\\Psi_{j,}'\\Psi_{j,}\\\\\n\\end{array}\n\\end{eqnarray*}\\]\n\\(\\Psi_{j,}\\) denotes row \\(\\) \\(\\Psi_{j}\\), .e., response variable \\(\\) horizon \\(j\\) (\\(Q=Id\\)).maximization problem subject side constraint \\(q_1'q_1=1\\) can written Lagrangian: \\[\nL=q_1'Sq_1-\\lambda(q_1'q_1-1),\n\\]\nfirst-order condition \\(Sq_1=\\lambda q_1\\) (side constraint \\(q_1'q_1=1\\)). equation, see solution \\(q_1\\) eigenvector \\(S\\), one associated eigenvalue \\(\\lambda\\). also see \\(\\sigma^2(\\underline{h},\\overline{h},q_1)=\\lambda\\). Thus, maximize variance, need find eigenvector \\(S\\) associated maximal eigenvalue \\(\\lambda\\). defines first principal component (see Section 10.1). , \\(S\\) admits following spectral decomposition:\n\\[\nS = \\mathcal{P}D\\mathcal{P}',\n\\]\n\\(D\\) diagonal matrix whose entries (ordered) eigenvalues: \\(\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_n \\ge 0\\), \\(\\sigma^2(\\underline{h},\\overline{h},q_1)\\) maximized \\(q_1 = p_1\\), \\(p_1\\) first column \\(\\mathcal{P}\\).following code identifies ``main GDP shock’’ using Uhlig’s method.\nFigure 6.1: Main GDP shock.\n``main GDP shock’’ explains 81% variance GDP horizon 20.Barsky Sims (2011) exploit approach identify “TFP news shock”, define shock () orthogonal innovation current utilization-adjusted TFP (b) best explains variation future TFP. Levchenko Pandalai-Nayar (2018) add “sentiment shock” define shock () orthogonal innovation current utilization-adjusted TFP TFP news shock, (b) best explains variation consumer sentiment. following code replicates Levchenko Pandalai-Nayar (2018). use mix zero FEVD identify TFP surprises, TFP news, sentiment shocks. adopt different approach using FEVD capture zero restrictions.\nFigure 6.2: Replication Levchenko Pandalai-Nayar (2020). FEVD zero restrictions.\n\nFigure 6.3: Replication Levchenko Pandalai-Nayar (2020). FEVD zero restrictions.\n\nFigure 6.4: Replication Levchenko Pandalai-Nayar (2020). FEVD zero restrictions.\nSentiment shocks explain 15% variance GDP, 76% TFP shocks (including 61% TFP news shocks).","code":"\nlibrary(IdSS)\nlibrary(readxl)\nlibrary(vars)\nlibrary(Matrix)\n# Declare data:\nTFP   <- levpan$tfp_lev\nGDP   <- levpan$lngdpcap\nE12   <- levpan$e12m\nCONS  <- levpan$lnconcap\nHOURS <- levpan$lnhrscap\ny <- cbind(TFP,GDP,E12,CONS,HOURS)\nnames.of.variables <- c(\"TFP\",\"GDP\",\"E12\",\"Consumption\",\"Hours\")\ncolnames(y)  <- names.of.variables\nnames.of.shocks <- c(\"Main GDP shock\")\n#define horizons for FEVM\nH1 <- 1\nH2 <- 20\nvariable <- 2 # variables for which we want to maximize FEVD\nnorm <- 15 # horizon at which the impact of the shock\n# is normalized to be positive\nres.svar.fevmax <- svar.fevmax(y,p=2,\n                               nb.shocks=1,\n                               names.of.shocks,\n                               H1,\n                               H2,\n                               variable,\n                               norm,\n                               nb.periods.IRF = 20,\n                               bootstrap.replications = 100, # This is used in\n                               #the parametric bootstrap only\n                               confidence.interval = 0.90, # expressed in pp.\n                               indic.plot = 1 # Plots are displayed if = 1.\n)\n# Compute variance decomposition:\nVariance.decomp <- variance.decomp(res.svar.fevmax$simulated.IRFs)\nvardecomp <- Variance.decomp$vardecomp\nmean(vardecomp[2,2,20,,1]) # mean contribution (across all simulated IRFs)## [1] 0.8121989\n# of 1st shock to variance of second variable, horizon 20.\nmean(vardecomp[1,2,10,,1]) # mean contribution of 1st shock ## [1] 0.4886171\n# to covariance between 1st and 2nd variable, horizon 10.\nnames.of.shocks <- c(\"TFP surprise\",\"TFP news\",\"Sentiment\")\n#define horizons for FEVM\nH1 <- array(0,c(1,3))\nH2 <- array(0,c(1,3))\nH1[1,1] <- 1\nH1[1,2] <- 1\nH1[1,3] <- 1\nH2[1,1] <- 1\nH2[1,2] <- 40\nH2[1,3] <- 2\nvariable <- c(1,1,3) # variables for which we want to maximize FEVD\nnorm <- c(1,20,2) # horizon at which the impact of the\n# shock is normalized to be positive\nres.svar.fevmax <-\n  svar.fevmax(y,p=2,\n              nb.shocks=3,\n              names.of.shocks,\n              H1,\n              H2,\n              variable,\n              norm,\n              nb.periods.IRF = 40,\n              bootstrap.replications = 100, # This is used in the\n              #parametric bootstrap only\n              confidence.interval = 0.90, # expressed in pp.\n              indic.plot = 1 # Plots are displayed if = 1.\n  )\nVariance.decomp <- variance.decomp(res.svar.fevmax$simulated.IRFs)\nvardecomp <- Variance.decomp$vardecomp\nmean(vardecomp[2,2,40,,3])# mean contribution (across all simulated IRFs)## [1] 0.1470743\n# of 3rd shock to variance of second variable, horizon 40."},{"path":"forecast-error-variance-maximization.html","id":"NarrativeHistDecomp","chapter":"6 Forecast error variance maximization","heading":"6.2 Restrictions based on narrative historical decomposition","text":"related approach, introduced Antolín-Díaz Rubio-Ramírez (2018), consists imposing , specific dates (based narrative information), particular shock important contributor unexpected movement variable particular period.11 can formalized two different ways (respectively called Type Type B Antolín-Díaz Rubio-Ramírez (2018)):Type : given shock important (least important) driver unexpected change variable periods. periods, absolute value contribution unexpected change variable larger (smaller) absolute value contribution structural shock.Type B: given shock overwhelming (negligible) driver unexpected change given variable period. periods, absolute value contribution unexpected change variable larger (smaller) sum absolute value contributions structural shocks.","code":""},{"path":"NonGaussian.html","id":"NonGaussian","chapter":"7 Identification based on non-normality of the shocks","heading":"7 Identification based on non-normality of the shocks","text":"","code":""},{"path":"NonGaussian.html","id":"intuition","chapter":"7 Identification based on non-normality of the shocks","heading":"7.1 Intuition","text":"section, show non-identification structural shocks (\\(\\eta_t\\)) specific Gaussian case. propose consistent estimation approaches SVAR context non-Gaussian shocks.seen precedes identify \\(B\\) based first second moments . Since Gaussian distribution perfectly determined first two moments, comes one achieve identification structural shocks Gaussian. , even observe infinite number ..d. \\(B \\eta_t\\), recover \\(B\\) \\(\\eta_t\\)’s Gaussian.Indeed, \\(\\eta_t \\sim \\mathcal{N}(0,Id)\\), distribution \\(\\varepsilon_t \\equiv B \\eta_t\\) \\(\\mathcal{N}(0,BB')\\). Hence \\(\\Omega = B B'\\) observed (population), orthogonal matrix \\(Q\\) (.e. \\(QQ'=Id\\)), also \\(BQ \\eta_t \\sim \\mathcal{N}(0,\\Omega)\\).illustrate, consider following bivariate Gaussian situations, \\(\\Theta_1=0\\)):\\(\\left[\\begin{array}{c}\\eta_{1,t}\\\\ \\eta_{2,t}\\end{array}\\right]\\sim \\mathcal{N}(0,Id)\\), \n\\(B = \\left[\\begin{array}{cc}\n1 & 2 \\\\\n-1 & 1\n\\end{array}\\right]\\) \n\\(Q = \\left[\\begin{array}{cc}\n\\cos(\\pi/3) & -\\sin(\\pi/3) \\\\\n\\sin(\\pi/3) & \\cos(\\pi/3)\n\\end{array}\\right]\\) (rotation).Figure 7.1 shows distributions \\(B \\eta_t\\) \\(BQ\\eta_t\\) identical. However, impulse response functions associated one impulse matrix (\\(B\\) \\(BQ\\)) different. illustrated Figure 7.2, shows IRFs associated two identical models (defined Eq. (1.6)), difference impulse matrix (\\(B\\) \\(BQ\\)).\nFigure 7.1: figure compares distributions two Gaussian bivariate vectors, \\(B \\eta_t\\) \\(BQ\\eta_t\\), \\(\\eta_{t} \\sim \\mathcal{N}(0,Id)\\) (therefore \\(\\eta_{1,t}\\) \\(\\eta_{2,t}\\) independent), \\(Q\\) orthogonal matrix.\n\nFigure 7.2: figure shows impulse response functions associated impulse matrix equal \\(B\\) (black line) \\(BQ\\) (red line) different (even \\(BB'=BQ(BQ)'\\)).\nHence, Gaussian case, external restrictions (economic hypotheses) needed identify \\(B\\) (see previous sections). restrictions may necessary structural shocks Gaussian. , identification problem specific normally-distributed \\(\\eta_t\\)’s (Rigobon (2003), Normandin Phaneuf (2004), Lanne Lütkepohl (2008)).better see can case, consider bivariate vector independent structural shocks (\\(\\eta_{1,t}\\) \\(\\eta_{2,t}\\)) , now, assume one Gaussian . Specifically, assume \\(\\eta_{2,t}\\) drawn Student distribution 5 degrees freedom:\n\\(\\eta_{1,t} \\sim \\mathcal{N}(0,1)\\), \\(\\eta_{2,t} \\sim t(5)\\),\n\\(B = \\left[\\begin{array}{cc}\n1 & 2 \\\\\n-1 & 1\n\\end{array}\\right]\\) \n\\(Q = \\left[\\begin{array}{cc}\n\\cos(\\pi/3) & -\\sin(\\pi/3) \\\\\n\\sin(\\pi/3) & \\cos(\\pi/3)\n\\end{array}\\right]\\).Figure 7.3 shows , case, \\(B \\eta_t\\) \\(BQ\\eta_t\\) distribution (spite fact , cases, \\(\\mathbb{V}ar(\\varepsilon_t)=BB'\\)). opens door identification impulse matrix (\\(BQ\\)) non-Gaussian case.\nFigure 7.3: figure compares distributions two Gaussian bivariate vectors, \\(B \\eta_t\\) \\(BQ\\eta_t\\), \\(\\eta_t{1,t} \\sim \\mathcal{N}(0,1)\\), \\(\\eta_t{2,t} \\sim t(5)\\), \\(Q\\) orthogonal matrix.\n","code":""},{"path":"NonGaussian.html","id":"independent-component-analysis-ica","chapter":"7 Identification based on non-normality of the shocks","heading":"7.2 Independent Component Analysis (ICA)","text":"exercise consists identifying non-Gaussian independent shocks linear combinations shocks well-known problem signal-processing literature, called independent component analysis (ICA). Let us denote \\(C\\) matrix \\(C = \\Omega^{-1/2}B\\), \\(\\Omega^{1/2}\\) results Cholesky decomposition \\(\\Omega = BB'\\) (implying, \\(\\Omega^{1/2}{\\Omega^{1/2}}'=\\Omega\\)). easy check \\(C\\) orthogonal matrix (\\(B = \\Omega^{1/2}C\\)).classical ICA problem follows: Find \\(C\\) \\(\\varepsilon_t = C \\eta_t\\) (\\(\\eta_t = C'\\varepsilon_t\\)) given :12We observe \\(\\varepsilon_t\\)’s,components \\(\\eta_t\\) independent,\\(CC'=Id\\) (.e., \\(C\\) orthogonal).Figure 7.4 represents bivariate distributions. black (red) lines correspond distributions \\(\\eta_t\\) (\\(C\\eta_t\\)). important note two components vector \\(C \\eta_t\\) independent (contrary \\(\\eta_t\\)).\nFigure 7.4: three plots represent bivariate distributions \\(\\eta_t\\) (black) \\(C\\eta_t\\) (red), two components \\(\\eta_t\\) independent, unit variance, \\(C\\) orthogonal. Hence, three plots, \\(\\mathbb{V}ar(C\\eta_t)=Id\\).\ncases, \\(\\mathbb{V}ar(\\varepsilon_t)=\\mathbb{V}ar(\\eta_t)=Id\\). two components \\(\\varepsilon_t\\) independent. instance, last two cases, \\(\\mathbb{E}(\\varepsilon_{2,t}|\\varepsilon_{1,t}>4)<0\\) (whereas \\(\\mathbb{E}(\\eta_{2,t}|\\eta_{1,t}>4)=0\\)). objective ICA rotate \\(\\varepsilon_t\\) retrieve independent components (\\(\\eta_t\\)).Hypothesis 7.1  Process \\(\\eta_t\\) satisfies:\\(\\eta_t\\)’s ..d. (across time) \\(\\mathbb{E}(\\eta_t) = 0\\) \\(\\mathbb{V}ar(\\eta_t) = Id.\\)components \\(\\eta_{1,t}, \\ldots, \\eta_{n,t}\\) mutually independent.\niii \n\\[\n\\varepsilon_t = C_0 \\eta_t,\n\\]\n\\(\\mathbb{V}ar(\\varepsilon_t) = Id\\) (.e., \\(C_0\\) orthogonal).Theorem 7.1  (Eriksson, Koivunen (2004)) Hypothesis 7.1 satisfied one components \\(\\eta\\) Gaussian, matrix \\(C_0\\) identifiable post multiplication \\(DP\\), \\(P\\) permutation matrix \\(D\\) diagonal matrix whose diagonal entries 1 \\(-1\\).}","code":""},{"path":"NonGaussian.html","id":"pseudo-maximum-likelihood-pml-approach","chapter":"7 Identification based on non-normality of the shocks","heading":"7.3 Pseudo-Maximum Likelihood (PML) approach","text":"Hence, non-normal structural shocks identifiable. estimate based observations \\(\\varepsilon_t\\)’s? Gouriéroux, Monfort, Renne (2017) proposed Pseudo-Maximum Likelihood (PML) approach. approach consists maximizing -called pseudo log-likelihood function, based set p.d.f. \\(g_i (\\eta_i), =1,\\ldots,n\\) (may different true p.d.f. \\(\\eta_{,t}\\)’s):\n\\[\\begin{equation}\n\\log \\mathcal{L}_T (C) = \\sum^T_{t=1} \\sum^n_{=1} \\log g_i (c'_i \\varepsilon_t),\\tag{7.1}\n\\end{equation}\\]\n\\(c_i\\) \\(^{th}\\) column matrix \\(C\\) (\\(c'_i\\) \\(^{th}\\) row \\(C^{-1}\\) since \\(C^{-1}=C'\\)).restrictions \\(C'C = Id\\) can eliminated parameterizing \\(C\\) way , whatever considered parameters, \\(C\\) orthogonal.13 Gouriéroux, Monfort, Renne (2017) propose use, , Cayley’s representation: orthogonal matrix eigenvalue equal \\(-1\\) can written \n\\[\\begin{equation}\nC() = (Id+) (Id-)^{-1},\n\\end{equation}\\]\n\\(\\) skew symmetric (antisymmetric) matrix, \\('=-\\). one--one relationship \\(\\), since:\n\\[\\begin{equation}\n= (C()+Id)^{-1} (C()-Id).\n\\end{equation}\\]Hence, PML estimator matrix \\(C\\) obtained \\(\\widehat{C_T} = C(\\hat{}_T),\\) :\n\\[\\begin{equation}\n\\hat{}_T = \\arg \\max_{a_{,j}, >j} \\sum^T_{t=1} \\sum^n_{=1} \\log g_i [c_i ()' \\varepsilon_t].\\tag{7.3}\n\\end{equation}\\]assumptions \\(g_i\\) functions (excluding Gaussian distributions), Gouriéroux, Monfort, Renne (2017) derive asymptotic properties PML estimator. Specifically, PML estimator \\(\\widehat{C_T}\\) \\(C_0\\) consistent (\\(\\mathcal{P}_0\\), set matrices obtained permutation sign change columns \\(C_0\\)) asymptotically normal, speed convergence \\(1/\\sqrt{T}\\).asymptotic variance-covariance matrix \\(vec \\sqrt{T} (\\widehat{C_T} - C_0)\\) \\(^{-1} \\left[\\begin{array}{cc} \\Gamma & 0 \\\\ 0 & 0 \\end{array} \\right] (')^{-1}\\), matrices \\(\\) \\(\\Gamma\\) detailed Gouriéroux, Monfort, Renne (2017).Note potential misspecification pseudo-distributions \\(g_i\\) effect consistency specific PML estimators.Table 7.1 reports usual p.d.f. derivatives. (latter needed compute asymptotic variance-covariance matrix \\(vec \\sqrt{T} (\\widehat{C_T} - C_0)\\).)Table 7.1:  table reports usual p.d.f. derivatives.Example 7.1  (Non-Gaussian monetary-policy shocks) apply PML-ICA approach U.S. data coerving period 1959:IV 2015:quarterly frequency (\\(T=224\\)). consider three dependent variables: inflation (\\(\\pi_t\\)), economic activity (\\(z_t\\), output gap) nominal short-term interest rate (\\(r_t\\)). Changes log oil prices added exogenous variable (\\(x_t\\)).Let us denote \\(W_t\\) set information made past values \\(y_t= [\\pi_t,z_t,r_t]\\), \\(\\{y_{t-1},y_{t-2},\\dots\\}\\), exogenous variables \\(\\{x_{t},x_{t-1},\\dots\\}\\). reduced-form VAR model reads:\n\\[\ny_t  = \\underbrace{\\mu + \\sum_{=1}^{p} \\Phi_i y_{t-} + \\Theta x_t}_{(W_t;\\theta)} + u_t\n\\]\n\\(u_t\\)’s assumed serially independent, zero mean variance-covariance matrix \\(\\Omega\\).Matrices \\(\\mu\\), \\(\\Phi_i\\), \\(\\Theta\\) \\(\\Omega\\) consistently estimated OLS. Jarque-Bera tests support hypothesis non-normality residuals.want estimate orthogonal matrix \\(C\\) \\(u_t=\\Omega^{1/2}C \\eta_t\\), \\(\\Omega^{1/2}\\) results Cholesky decomposition \\(\\Omega\\) andthe components \\(\\eta_t\\) independent, zero-mean unit variance.PML approach applied standardized VAR residuals given :\n\\[\n\\hat\\varepsilon_t = \\hat\\Omega^{-1/2}_T\\underbrace{[y_t - (W_t;\\hat\\theta_T)]}_{\\mbox{VAR residuals}}.\n\\]\nconstruction \\(\\hat\\Omega^{-1/2}_T\\), comes covariance matrix residuals \\(Id\\).pseudo density functions distinct asymmetric mixtures Gaussian distributions.(Note: always useful combine two optimization algorithms, Nelder-Mead BFGS.)obtain close results neglecting commodity prices. case, one can simply use function estim.SVAR.ICA IdSS package. Let us compare \\(C\\) matrix obtained two cases (without commodity prices):\\(C\\) estimated, remains label resulting structural shocks (components \\(\\eta_{t}\\)). Postulated shocks monetary-policy, supply, demand shocks. labelling can based following considerations:Contractionary monetary-policy shocks negative impact real activity inflation.Supply shock influences opposite signs economic activity inflation.Demand shock influences signs economic activity inflation.Let us compute IRFs associated three structural shocks. (sake comparison, first line plots shows IRFs monetary-policy shock obtained Cholesky-based approach short-term rate ordered last.)\nFigure 7.5: first row plots shows responses three endogenous variables monetary policy shock context Cholesky-idendtified SVAR (ordering: inflation, output gap, interest rate). next three rows plots show repsonses endogenous variables three structural shocks identified ICA. last one (Shock 3) close Cholesky-identified monetary policy shock.\nAccording Figure 7.5, Shock 1 supply shock, Shock 2 demand shock, Shock 3 monetary-policy shock. Note Shock 3 close one resulting Cholesky approach.","code":"\nlibrary(IdSS)\nFirst.date <- \"1959-04-01\"\nLast.date  <- \"2015-01-01\"\ndata <- US3var\ndata <- data[(data$Date>=First.date)&(data$Date<=Last.date),]\nY <- as.matrix(data[c(\"infl\",\"y.gdp.gap\",\"r\")])\nnames.var <- c(\"inflation\",\"real activity\",\"short-term rate\")\nT <- dim(Y)[1]\nn <- dim(Y)[2]\nnb.lags <- 6 # number of lags used in the VAR model\nX <- NULL\nfor(i in 1:nb.lags){\n  lagged.Y <- rbind(matrix(NaN,i,n),Y[1:(T-i),])\n  X <- cbind(X,lagged.Y)}\nX <- cbind(X,data$commo) # add exogenous variables\nPhi <- matrix(0,n,n*nb.lags);mu <- rep(0,n)\neffect.commo <- rep(0,n)\nU <- NULL # Eta is the matrix of OLS residuals\nfor(i in 1:n){\n  eq <- lm(Y[,i] ~ X)\n  Phi[i,] <- eq$coef[2:(dim(Phi)[2]+1)]\n  mu[i] <- eq$coef[1]\n  U <- cbind(U,eq$residuals)\n  effect.commo[i] <- eq$coef[length(eq$coef)]\n}\nOmega <- var(U) # Covariance matrix of the OLS residuals.\nOmeg12 <- t(chol(Omega)) # Cholesky matrix associated with Omega (lower triang.)\nEps <- U %*% t(solve(Omeg12)) # Recover associated structural shocks\ndistri <- list(\n  type=c(\"mixt.gaussian\",\"mixt.gaussian\",\"mixt.gaussian\"),\n  df=c(NaN,NaN,NaN),\n  p=c(0.5,.5,.5),mu=c(.1,.1,.1),sigma=c(.5,.7,1.3))\nAA.0 <- c(0,0,0)\nres.optim <- optim(AA.0,func.2.minimize,\n                   Y = Eps, distri = distri,\n                   gr = d.func.2.minimize,\n                   method=\"Nelder-Mead\",\n                   control=list(trace=FALSE,maxit=1000))\nAA.0 <- res.optim$par\nres.optim <- optim(AA.0,func.2.minimize,d.func.2.minimize,\n                   Y = Eps, distri = distri,\n                   method=\"BFGS\",\n                   control=list(trace=FALSE))\nAA.est <- res.optim$par\nn <- ncol(Y)\nM <- make.M(n)\nA.est <- matrix(M %*% AA.est,n,n)\nC.PML <- (diag(n) + A.est) %*% solve(diag(n) - A.est)\neta.PML <- Eps %*% C.PML # eta.PML are the ICA-estimated structural shocks\n\n# Compute asymptotic covariance matrix of C.PML:\nV <- make.Asympt.Cov.delta(eta.PML,distri,C.PML)\nparam <- c(C.PML)\nst.dev <- sqrt(diag(V))\nt.stat <- c(C.PML)/sqrt(diag(V))\ncbind(param,st.dev,t.stat) # print results of PML estimation##             param      st.dev      t.stat\n##  [1,]  0.94417705 0.040848382  23.1141845\n##  [2,] -0.32711569 0.118802653  -2.7534376\n##  [3,]  0.03905164 0.074172945   0.5264944\n##  [4,]  0.32070293 0.119270893   2.6888616\n##  [5,]  0.93977707 0.041629110  22.5749976\n##  [6,]  0.11818924 0.060821400   1.9432179\n##  [7,] -0.07536139 0.071980455  -1.0469702\n##  [8,] -0.09906759 0.062185577  -1.5930959\n##  [9,]  0.99222290 0.007785691 127.4418551\nICA.res.no.commo <- estim.SVAR.ICA(Y,distri = distri,p=6)\nround(cbind(ICA.res.no.commo$C.PML,NaN,C.PML),3)##        [,1]  [,2]   [,3] [,4]   [,5]  [,6]   [,7]\n## [1,]  0.956 0.287 -0.059  NaN  0.944 0.321 -0.075\n## [2,] -0.292 0.950 -0.108  NaN -0.327 0.940 -0.099\n## [3,]  0.025 0.121  0.992  NaN  0.039 0.118  0.992\nIRF.Chol <- array(NaN,c(n,41,n))\nIRF.ICA  <- array(NaN,c(n,41,n))\nPHI <- list();for(i in 1:nb.lags){PHI[[i]]<-array(Phi,c(3,3,nb.lags))[,,i]}\nfor(jjjj in 1:n){\n  u.shock <- rep(0,n)\n  u.shock[jjjj] <- 1\n  IRF.Chol[,,jjjj] <- \n    t(simul.VAR(c=rep(0,3),Phi=PHI,B=Omeg12,nb.sim=41,\n                y0.star=rep(0,3*nb.lags),indic.IRF = 1,u.shock = u.shock))\n  IRF.ICA[,,jjjj]  <- \n    t(simul.VAR(c=rep(0,3),Phi=PHI,B=Omeg12%*%C.PML,nb.sim=41,\n                y0.star=rep(0,3*nb.lags),indic.IRF = 1,u.shock = u.shock))\n}"},{"path":"NonGaussian.html","id":"relation-with-the-heteroskedasticity-identification","chapter":"7 Identification based on non-normality of the shocks","heading":"7.4 Relation with the Heteroskedasticity Identification","text":"cases, \\(\\varepsilon_t\\)’s heteroskedastic, \\(B\\) matrix can identified (Rigobon (2003), Lanne, Lütkepohl, Maciejowska (2010)).Consider case still \\(\\varepsilon_t = B \\eta_t\\) \\(\\eta_t\\)’s variance conditionally depends regime \\(s_t \\\\{1,\\dots,M\\}\\). :\n\\[\n\\mathbb{V}ar(\\eta_{k,t}|s_t) = \\lambda_{s_t,k} \\quad \\mbox{} k \\\\{1,\\dots,n\\}\n\\]Denoting \\(\\Lambda_i\\) diagonal matrix whose diagonal entries \\(\\lambda_{,k}\\)’s, comes :\n\\[\n\\mathbb{V}ar(\\eta_{t}|s_t) = \\Lambda_{s_t},\\quad \\mbox{}\\quad \\mathbb{V}ar(\\varepsilon_{t}|s_t) = B\\Lambda_{s_t}B'.\n\\]Without loss generality, can assumed \\(\\Lambda_1=Id\\).context, \\(B\\) identified, apart sign reversal columns \\(k \\ne j \\\\{1,\\dots,n\\}\\), regime \\(\\) s.t. \\(\\lambda_{,k} \\ne \\lambda_{,j}\\). (Prop.1 Lanne, Lütkepohl, Maciejowska (2010)).Bivariate regime case (\\(M=2\\)): \\(B\\) identified \\(\\lambda_{2,k}\\)’s different. , identification ensured “sufficient heterogeneity volatility changes” (Lütkepohl Netšunajev (2017)).regimes \\(s_t\\) exogenous serially independent, situation consistent “non-Gaussian” situation described .","code":""},{"path":"Projections.html","id":"Projections","chapter":"8 Local projection methods","heading":"8 Local projection methods","text":"","code":""},{"path":"Projections.html","id":"overview-of-the-approach","chapter":"8 Local projection methods","heading":"8.1 Overview of the approach","text":"Consider infinite MA representation \\(y_t\\) (Eq. (1.4)):\n\\[\ny_t = \\mu + \\sum_{h=0}^\\infty \\Psi_{h} \\eta_{t-h}.\n\\]\nseen Section 1.2, entries \\((,j)\\) sequence \\(\\Psi_h\\) matrices define IRF \\(\\eta_{j,t}\\) \\(y_{,t}\\).Assume observe \\(\\eta_{j,t}\\), consistent estimate \\(\\Psi_{,j,h}\\) simply obtained OLS regression \\(y_{,t+h}\\) \\(\\eta_{j,t}\\):14\n\\[\\begin{equation}\ny_{,t+h} = \\mu_i + \\Psi_{,j,h}\\eta_{j,t} + u_{,j,t+h}.\\tag{8.1}\n\\end{equation}\\]\nRunning kind regression (using instruments \\(\\eta_{j,t}\\)) core idea local projection (LP) approach proposed Jordà (2005).Now, proceed (usual) case \\(\\eta_{j,t}\\) observed? consider two situations. second requires instruments, first approach . first approach (Section 8.2) original Jordà (2005)’s approach.","code":""},{"path":"Projections.html","id":"LPIVww","chapter":"8 Local projection methods","heading":"8.2 Situation A: Without IV","text":"Assume structural shock interest (\\(\\eta_{1,t}\\), say) can consistently obtained residual regression variable \\(x_t\\) set control variables \\(w_t\\) independent \\(\\eta_{1,t}\\):\n\\[\\begin{equation}\n\\eta_{1,t} = x_t - \\mathbb{E}(x_t|w_t),\\tag{8.2}\n\\end{equation}\\]\n\\(\\mathbb{E}(x_t|w_t)\\) affine \\(w_t\\) \\(w_t\\) affine transformation \\(\\eta_{2:n,t}\\) past shocks \\(\\eta_{t-1},\\eta_{t-2},\\dots\\).Eq. (8.2) implies , conditional \\(w_t\\), additional knowledge \\(x_t\\) useful comes forecast something depends \\(\\eta_{1,t}\\). Hence, given \\(u_{,1,t+h}\\) (see Eq. (8.1)) independent \\(\\eta_{1,t}\\) (depends \\(\\eta_{t+h},\\dots,\\eta_{t+1},\\color{blue}{\\eta_{2:n,t}},\\eta_{t-1},\\eta_{t-2},\\dots\\)), comes \n\\[\\begin{equation}\n\\mathbb{E}(u_{,1,t+h}|x_t,w_t)= \\mathbb{E}(u_{,1,t+h}|w_t).\\tag{8.3}\n\\end{equation}\\]\nconditional mean independence case.Using (8.2), one can rewrite Eq. (8.1) follows:\n\\[\\begin{eqnarray*}\ny_{,t+h} &=& \\mu_i + \\Psi_{,1,h}\\eta_{1,t} + u_{,1,t+h}\\\\\n&=&  \\mu_i + \\Psi_{,1,h}x_t  \\color{blue}{-\\Psi_{,1,h}\\mathbb{E}(x_t|w_t) + u_{,1,t+h}},\n\\end{eqnarray*}\\]Given Eq. (8.3), comes , conditional \\(x_t\\) \\(w_t\\), expectation blue term function \\(w_t\\). Assuming expectation linear, standard results conditional mean independence case imply OLS estimator regression \\(y_{,t+h}\\) \\(x_t\\), controlling \\(w_t\\), provides consistent estimate \\(\\Psi_{,1,h}\\):\n\\[\\begin{equation}\ny_{,t+h} = \\alpha_i + \\Psi_{,1,h}x_t + \\beta'w_t + v_{,t+h}.\n\\end{equation}\\]instance consistent case \\([\\Delta GDP_t, \\pi_t,i_t]'\\) follows VAR(1) monetary-policy shock contemporaneously affect \\(\\Delta GDP_t\\) \\(\\pi_t\\). IRFs can estimated LP, taking \\(x_t = i_t\\) \\(w_t = [\\Delta GDP_t,\\pi_t,\\Delta GDP_{t-1}, \\pi_{t-1},i_{t-1}]'\\).approach closely relates SVAR Cholesky-based identification approach. Specifically, \\(w_t = [\\color{blue}{y_{1,t},\\dots,y_{k-1,t}}, y_{t-1}',\\dots,y_{t-p}']'\\), \\(k\\le n\\), \\(x_t = y_{k,t}\\), approach corresponds, \\(h=0\\), SVAR(\\(p\\)) Cholesky-based IRF (focusing responses \\(k^{th}\\) structural shock). However, two approaches differ \\(h>0\\), LP methodology assumes VAR dynamics \\(y_t\\).15In following lines code, employ Jordà (2005)’s approach dataset one used Section 2.3. (illustrating Christiano, Eichenbaum, Evans (1996)’s methodology.)\nFigure 8.1: Response monetary-policy shock. Identification approach Jorda (2005).\n","code":"\nlibrary(IdSS);library(vars)\ndata(\"USmonthly\")\n# Select sample period:\nFirst.date <- \"1965-01-01\";Last.date <- \"1995-06-01\"\nindic.first <- which(USmonthly$DATES==First.date)\nindic.last  <- which(USmonthly$DATES==Last.date)\nUSmonthly   <- USmonthly[indic.first:indic.last,]\nconsidered.variables <- c(\"LIP\",\"UNEMP\",\"LCPI\",\"LPCOM\",\"FFR\",\"NBR\",\"TTR\",\"M1\")\ny <- as.matrix(USmonthly[considered.variables])\nres.jorda <- make.jorda.irf(y,posit.of.shock = 5,\n                            nb.periods.IRF = 12,\n                            nb.lags.endog.var.4.control=3,\n                            indic.plot = 1, # Plots are displayed if = 1.\n                            confidence.interval = 0.90)"},{"path":"Projections.html","id":"situation-b-iv-approach","chapter":"8 Local projection methods","heading":"8.3 Situation B: IV approach","text":"","code":""},{"path":"Projections.html","id":"instruments-proxies-for-structural-shocks","chapter":"8 Local projection methods","heading":"8.3.1 Instruments (proxies for structural shocks)","text":"Consider now valid instrument \\(z_t\\) \\(\\eta_{1,t}\\) (\\(\\mathbb{E}(z_t)=0\\)). :\n\\[\\begin{equation}\n\\left\\{\n\\begin{array}{llll}\n(IV.) & \\mathbb{E}(z_t \\eta_{1,t}) &\\ne 0 & \\mbox{(relevance condition)} \\\\\n(IV.ii) & \\mathbb{E}(z_t \\eta_{j,t}) &= 0 \\quad \\mbox{} j>1 & \\mbox{(exogeneity condition).}\n\\end{array}\\right.\\tag{8.4}\n\\end{equation}\\]\ninstrument \\(z_t\\) can used identify structural shock. Eq. (8.4) implies exist \\(\\rho \\ne 0\\) mean-zero variable \\(\\xi_t\\) :\n\\[\n\\eta_{1,t} = \\rho z_t + \\xi_t,\n\\]\n\\(\\xi_t\\) correlated neither \\(z_t\\), \\(\\eta_{j,t}\\), \\(j\\ge2\\).Proof. Define \\(\\rho = \\frac{\\mathbb{E}(\\eta_{1,t}z_t)}{\\mathbb{V}ar(z_t)}\\) \\(\\xi_t = \\eta_{1,t} - \\rho z_t\\). easily seen \\(\\xi_t\\) satisfies moment restrictions given .Ramey (2016) reviews different approaches employed construct monetary policy-shocks (two main approaches presented 8.1 8.2 ). also collected time series shocks, see website. Several shocks included Ramey dataset package IdSS.Example 8.1  (Identification Monetary-Policy Shocks Based High-Frequency Data) Instruments monetary-policy shocks can extracted high-frequency market data associated interest-rate products.quotes interest-rate-related financial products sensitive monetary-policy announcements. quotes mainly depends investors’ expectations regarding future short-term rates: \\(\\mathbb{E}_t(i_{t+s})\\). Typically, agents risk-neutral, maturity-\\(h\\) interest rate approximatively given :\n\\[\ni_{t,h} \\approx \\mathbb{E}_t\\left(\\frac{1}{h}\\int_{0}^{h} i_{t+s} ds\\right) = \\frac{1}{h}\\int_{0}^{h} \\mathbb{E}_t\\left(i_{t+s}\\right) ds.\n\\]\ngeneral, changes \\(\\mathbb{E}_t(i_{t+s})\\), \\(s>0\\), can affected types shocks may trigger reaction central bank.However, MP announcement takes place \\(t\\) \\(t+\\epsilon\\), \\(\\mathbb{E}_{t+\\epsilon}(i_{t+s})-\\mathbb{E}_t(i_{t+s})\\) attributed MP shock (see Figure 8.2, Gürkaynak, Sack, Swanson (2005)). Hence, monthly time series MP shocks can obtained summing, month, changes \\(i_{t+ \\epsilon,h} - i_{t,h}\\) associated given interest rate (T-bills, futures, swaps) given maturity \\(h\\).See among others: Kuttner (2001), Cochrane Piazzesi (2002), Gürkaynak, Sack, Swanson (2005), Piazzesi Swanson (2008), Gertler Karadi (2015). time series named\nFF4_TC, ED2_TC, ED3_TC, ED4_TC, GS1, ff1_vr, ff4_vr, ed2_vr, ff1_gkgreen, ff4_gkgreen, ed2_gkgreen data frame Ramey package IdSS time series shocks based approach (see Ramey’s website details).\nFigure 8.2: Source: Gurkaynak, Sack Swanson (2005). Transaction rates Federal funds futures June 25, 2003, day regularly scheduled FOMC meeting scheduled. 2:15 p.m., FOMC announced lowering target federal funds rate 1.25% 1%, many market participants expecting 50 bp cut. shows () financial markets seem fully adjust policy action within just minutes (ii) federal funds rate surprise necessarily direction federal funds rate action .\nExample 8.2  (Identification Monetary-Policy Shocks Based Narrative Approach) Romer Romer (2004) propose two-step approach:derive series Federal Reserve intentions federal funds rate (explicit target Fed) around FOMC meetings,control Federal Reserve forecasts.gives measure intended monetary policy actions driven information future economic developments.“intentions” measured combination narrative quantitative evidence. Sources: (among others) Minutes FOMC “Blue Books”.Controls = variables spanning information Federal Reserve future developments. Data: Federal Reserve’s internal forecasts (inflation, real output unemployment), “Greenbook’s forecasts” – usually issued 6 days FOMC meeting.shock measure residual series linear regression () (b). time series Ramey$rrshock83 Ramey$rrshock83b (Ramey data frame included package IdSS) contain shocks period 1983-2007. (Ramey$rrshock83b uses long-horizon Greenbook forecasts.)create measure news future government spending, Ramey (2011) uses newspaper articles construct time series (unexpected) fiscal shocks:16Example 8.3  (Identification news future government spending) Ramey (2011)’s measure aims measure expected discounted value government spending changes due foreign political events. argues variable matter wealth effect neoclassical framework. series constructed reading periodicals order gauge public’s expectations (Business Week 2001, newspapers afterwards).According Ramey (2011), government sources used () either released timely manner (b) known underestimate costs certain actions.Figure 8.3 shows resulting time series shocks. Figure 8.4 shows IRF macro variables shock expected government spending.\nFigure 8.3: Source: Ramey (2011). Defense News: PDV Change Spending Percent GDP.\n\nFigure 8.4: Source: Ramey (2011) [Figure X paper]. Responses macro variables shock expected government spending.\ntwo main IV approaches estimate IRFs see Stock Watson (2018):SVAR-IV approach (Subsection 8.3.2),LP-IV approach, \\(y_t\\)’s DGP left unspecified (Subsection 8.3.3).LP-IV approach based set IV regressions (variable interest, one forecast horizon). SVAR-IV approach based IV regressions VAR innovations (one series VAR innovations).VAR adequately captures DGP, IV-SVAR optimal horizons. However, VAR misspecified, specification errors compounded horizon local projection method lead better results.","code":""},{"path":"Projections.html","id":"SVARIVa","chapter":"8 Local projection methods","heading":"8.3.2 Situation B.1: SVAR-IV approach","text":"Assume consistent estimates \\(\\varepsilon_t = B\\eta_t\\), estimates (\\(\\hat\\varepsilon_{t}\\)) coming estimation VAR model. , \\(\\\\{1,\\dots,n\\}\\):\n\\[\\begin{eqnarray}\n\\varepsilon_{,t} &=& b_{,1} \\eta_{1,t} + u_{,t} \\tag{8.5}\\\\\n&=& b_{,1} \\rho z_t + \\underbrace{b_{,1}\\xi_t + u_{,t}}_{\\perp z_t}. \\nonumber\n\\end{eqnarray}\\]\n(\\(u_{,t}\\) linear combination \\(\\eta_{j,t}\\)’s, \\(j\\ge2\\)).Hence, multiplicative factor (\\(\\rho\\)), (OLS) regressions \\(\\hat\\varepsilon_{,t}\\)’s \\(z_t\\) (consistent true \\(\\varepsilon_{,t}\\)’s) provide consistent estimates \\(b_{,1}\\)’s.Combined estimated VAR (\\(\\Phi_k\\) matrices), provides consistent estimates IRFs \\(\\eta_{1,t}\\) \\(y_t\\), though multiplicative factor. scale ambiguity can solved rescaling structural shock (“unit-effect normalisation”, see Stock Watson (2018)). Let us consider \\(\\tilde\\eta_{1,t}=b_{1,1}\\eta_{1,t}\\); construction, \\(\\tilde\\eta_{1,t}\\) unit contemporaneous effect \\(y_{1,t}\\). Denoting \\(\\tilde{B}_{,1}\\) contemporaneous impact \\(\\tilde\\eta_{1,t}\\) \\(^{th}\\) endogenous variable, get:\n\\[\n\\tilde{B}_{1} = \\frac{1}{b_{1,1}} {B}_{1},\n\\]\n\\(B_{1}\\) denotes \\(1^{st}\\) column \\(B\\) \\(\\tilde{B}_{1}=[1,\\tilde{B}_{2,1},\\dots,\\tilde{B}_{n,1}]'\\).Eq. (8.5) gives:\n\\[\\begin{eqnarray*}\n\\varepsilon_{1,t} &=& \\tilde\\eta_{1,t} + u_{1,t}\\\\\n\\varepsilon_{,t} &=& \\tilde{B}_{,1} \\tilde\\eta_{1,t} + u_{,t}.\n\\end{eqnarray*}\\]\nsuggests \\(\\tilde{B}_{,1}\\) can estimated regressing \\(\\varepsilon_{,t}\\) \\(\\varepsilon_{1,t}\\) (\\(\\hat\\varepsilon_{,t}\\) \\(\\hat\\varepsilon_{1,t}\\) practice), using \\(z_t\\) instrument.inference? use usual TSLS standard deviations \\(\\varepsilon_{,t}\\)’s directly observed. Bootstrap procedures can resorted . Stock Watson (2018) propose, particular, Gaussian parametric bootstrap:Assume estimated \\(\\{\\widehat{\\Phi}_1,\\dots,\\widehat{\\Phi}_p,\\widehat{B}_1\\}\\) using SVAR-IV approach based size-\\(T\\) sample. Generate \\(N\\) (\\(N\\) large) size-\\(T\\) samples following VAR:\n\\[\n\\left[\n\\begin{array}{cc}\n\\widehat{\\Phi}(L) & 0 \\\\\n0 & \\widehat{\\rho}(L)\n\\end{array}\n\\right]\n\\left[\n\\begin{array}{c}\ny_t \\\\\nz_t\n\\end{array}\n\\right] =\n\\left[\n\\begin{array}{c}\n\\varepsilon_t \\\\\ne_t\n\\end{array}\n\\right],\n\\]\n\\[\n\\mbox{} \\quad \\left[\n\\begin{array}{c}\n\\varepsilon_t \\\\\ne_t\n\\end{array}\n\\right]\\sim \\, ..d.\\,\\mathcal{N}\\left(\\left[\\begin{array}{c}0\\\\0\\end{array}\\right],\n\\left[\\begin{array}{cc}\n\\Omega & S'_{\\varepsilon,e}\\\\\nS_{\\varepsilon,e}& \\sigma^2_{e}\n\\end{array}\\right]\n\\right),\n\\]\n\\(\\widehat{\\rho}(L)\\) \\(\\sigma^2_{e}\\) result estimation AR process \\(z_t\\), \\(\\Omega\\) \\(S_{\\varepsilon,e}\\) sample covariances VAR/AR residuals.simulated sample (\\(\\tilde{y}_t\\) \\(\\tilde{z}_t\\), say), estimate \\(\\{\\widetilde{\\widehat{\\Phi}}_1,\\dots,\\widetilde{\\widehat{\\Phi}}_p,\\widetilde{\\widehat{B}}_1\\}\\) associated \\(\\widetilde{\\Psi}_{,1,h}\\). provides e.g. sequence \\(N\\) estimates \\(\\Psi_{,1,h}\\), quantiles conf. intervals can deduced.following lines code, use approach estimate response macroeconomic variables monetary policy shock. instrument FF4_TC Ramsey’s database; base Gertler Karadi (2015) approach, use 3-month fed funds futures.\nFigure 8.5: Gertler-Karadi monthly shocks, fed funds futures 3 months (resp. 6 months) black (resp. red).\n\nFigure 8.6: Reponses monetary-policy shock, SVAR-IV approach.\n","code":"\nlibrary(vars);library(IdSS)\ndata(\"USmonthly\")\nFirst.date <- \"1990-05-01\";Last.date <- \"2012-6-01\"\nindic.first <- which(USmonthly$DATES==First.date)\nindic.last  <- which(USmonthly$DATES==Last.date)\nUSmonthly   <- USmonthly[indic.first:indic.last,]\nshock.name <- c(\"FF4_TC\",\"ED2_TC\") # \"ff1_vr\", \"rrshock83b\"\nindic.shock.name <- which(names(USmonthly)%in%shock.name)\nZ <- as.matrix(USmonthly[,indic.shock.name])\npar(plt=c(.1,.95,.1,.95))\nplot(USmonthly$DATES,Z[,1],type=\"l\",xlab=\"\",ylab=\"\",lwd=2)\nlines(USmonthly$DATES,Z[,2],col=\"red\",lwd=2,pch=3,lty=2)\nconsidered.variables <- c(\"GS1\",\"LIP\",\"LCPI\",\"EBP\")\nY <- as.matrix(USmonthly[,considered.variables])\nn <- length(considered.variables)\ncolnames(Y) <- considered.variables\npar(plt=c(.15,.95,.15,.8))\nres.svar.iv <- \n  svar.iv(Y,Z,p = 4,names.of.variables=considered.variables,\n          nb.periods.IRF = 20,\n          z.AR.order=1, \n          nb.bootstrap.replications = 100, \n          confidence.interval = 0.90,\n          indic.plot=1)"},{"path":"Projections.html","id":"LPIVa","chapter":"8 Local projection methods","heading":"8.3.3 Situation B.2: LP-IV","text":"want posit VAR-type dynamics \\(y_t\\) –e.g., suspect true generating model may non-invertible VARMA model– can directly proceed IV-projection methods obtain \\(\\tilde\\Psi_{,1,h}\\equiv \\Psi_{,1,h}/b_{1,1}\\) (IRFs \\(\\tilde\\eta_{1,t}\\) \\(y_{,t}\\)).However, Assumptions (IV.) (IV.ii) (Eq. (8.4)) complemented (IV.iii):\n\\[\\begin{equation*}\n\\begin{array}{llll}\n(IV.iii) & \\mathbb{E}(z_t \\eta_{j,t+h}) &= 0 \\, \\mbox{ } h \\ne 0 & \\mbox{(lead-lag exogeneity)}\n\\end{array}\n\\end{equation*}\\](IV.), (IV.ii) (IV.iii) satisfied, \\(\\tilde\\Psi_{,1,h}\\) can estimated regressing \\(y_{,t+h}\\) \\(y_{1,t}\\), using \\(z_t\\) instrument, .e. considering TSLS estimation :\n\\[\\begin{equation}\ny_{,t+h} = \\alpha_i + \\tilde\\Psi_{,1,h}y_{1,t} + \\nu_{,t+h},\\tag{8.6}\n\\end{equation}\\]\n\\(\\nu_{,t+h}\\) correlated \\(y_{1,t}\\), \\(z_t\\).indeed:\n\\[\\begin{eqnarray*}\ny_{1,t} &=& \\alpha_1 + \\tilde\\eta_{1,t} + v_{1,t}\\\\\ny_{,t+h} &=& \\alpha_i + \\tilde\\Psi_{,1,h}\\tilde\\eta_{1,t} + v_{,t+h},\n\\end{eqnarray*}\\]\n\\(v_{,t+h}\\)’s uncorrelated \\(z_t\\) (IV.), (IV.ii) (IV.iii).Note , \\(h>0\\), \\(v_{,t+h}\\) (\\(\\nu_{,t+h}\\)) auto-correlated. Newey-West corrections therefore used compute std errors \\(\\tilde\\Psi_{,1,h}\\)’s estimates.Consider linear regression:\n\\[\n\\mathbf{Y} = \\mathbf{X}\\boldsymbol\\beta + \\boldsymbol\\varepsilon,\n\\]\n\\(\\mathbb{E}(\\boldsymbol\\varepsilon)=0\\), explicative variables \\(\\mathbf{X}\\) can correlated residuals \\(\\boldsymbol\\varepsilon\\). Moreover, \\(\\boldsymbol\\varepsilon\\)’s may feature heteroskedasticity auto-correlated. denote \\(\\mathbf{Z}\\) matrix instruments, \\(\\mathbb{E}(\\mathbf{X}'\\mathbf{Z}) \\ne 0\\) \\(\\mathbb{E}(\\boldsymbol\\varepsilon'\\mathbf{Z}) = 0\\).IV estimator \\(\\boldsymbol\\beta\\) obtained regressing \\(\\hat{\\mathbf{Y}}\\) \\(\\hat{\\mathbf{X}}\\), \\(\\hat{\\mathbf{Y}}\\) \\(\\hat{\\mathbf{X}}\\) respective residuals regressions \\(\\mathbf{Y}\\) \\(\\mathbf{X}\\) \\(\\mathbf{Z}\\).\n\\[\\begin{eqnarray*}\n\\mathbf{b}_{iv} &=& [\\mathbf{X}'\\mathbf{Z}(\\mathbf{Z}'\\mathbf{Z})^{-1}\\mathbf{Z}'\\mathbf{X}]^{-1}\\mathbf{X}'\\mathbf{Z}(\\mathbf{Z}'\\mathbf{Z})^{-1}\\mathbf{Z}'\\mathbf{Y}\\\\\n\\mathbf{b}_{iv} &=& \\boldsymbol\\beta + \\frac{1}{\\sqrt{T}}\\underbrace{T[\\mathbf{X}'\\mathbf{Z}(\\mathbf{Z}'\\mathbf{Z})^{-1}\\mathbf{Z}'\\mathbf{X}]^{-1}\\mathbf{X}'\\mathbf{Z}(\\mathbf{Z}'\\mathbf{Z})^{-1}}_{=Q(\\mathbf{X},\\mathbf{Z}) \\overset{p}{\\rightarrow} \\mathbf{Q}_{xz}}\\underbrace{\\sqrt{T}\\left(\\frac{1}{T}\\mathbf{Z}'\\boldsymbol\\varepsilon\\right)}_{\\overset{d}{\\rightarrow} \\mathcal{N}(0,S)},\n\\end{eqnarray*}\\]\n\\(\\mathbf{S}\\) long-run variance \\(\\mathbf{z}_t\\varepsilon_t\\).17 asymptotic covariance matrix \\(\\sqrt{T}\\mathbf{b}_{iv}\\) \\(\\mathbf{Q}_{xz} \\mathbf{S} \\mathbf{Q}_{xz}'\\). Therefore, covariance matrix \\(\\mathbf{b}_{iv}\\) can approximated \\(\\frac{1}{T}Q(\\mathbf{X},\\mathbf{Z})\\hat{\\mathbf{S}}Q(\\mathbf{X},\\mathbf{Z})'\\) \\(\\hat{\\mathbf{S}}\\) Newey-West estimator \\(\\mathbf{S}\\).18Assumption (IV.iii) usually restrictive \\(h>0\\) (\\(z_t\\) usually affected future shocks). contrast, may restrictive \\(h<0\\). can solved adding controls Regression (8.6). controls span space \\(\\{\\eta_{t-1},\\eta_{t-2},\\dots\\}\\).\\(z_t\\) suspected correlated past values \\(\\eta_{1,t}\\) \\(\\eta_{j,t}\\)’s, \\(j>1\\), one can add lags \\(z_t\\) controls (method e.g. advocated Ramey, 2016, p.108, considering instrument Gertler Karadi (2015)).general case, one can use lags \\(y_t\\) controls. Note , even (IV.iii) holds, adding controls may reduce variance regression error.","code":"\nres.LP.IV <- make.LPIV.irf(Y,Z,\n                           nb.periods.IRF = 20,\n                           nb.lags.Y.4.control=4,\n                           nb.lags.Z.4.control=4,\n                           indic.plot = 1, # Plots are displayed if = 1.\n                           confidence.interval = 0.90)"},{"path":"PanelVARs.html","id":"PanelVARs","chapter":"9 Panel VARs","heading":"9 Panel VARs","text":"Panel VARs structure VAR models, \nsense variables assumed endogenous \ninterdependent, cross sectional dimension \nadded representation. \\(N\\) units indexed \n\\(\\\\{1,...,N\\}\\). index \\(\\) generic indicate\ncountries, sectors, markets… panel VAR \n\\[ y_{}=c_i+\\Phi_i(L)\ny_{t-1}+\\varepsilon_{}.\\] \\(y_t\\) stacked version \n\\(y_{}\\) \\(\\varepsilon_t\\) ..d., variance-covariance\nmatrix \\(\\Omega\\). Vector \\(c_i\\) lag polynomial \\(\\Phi_i(L)\\) may depend unit. Canova Ciccarelli (2013) provide survey panel estimation methods.Contrary standard VARs, panel VARs may help study\n* similarities/differences transmission shocks;\n* Spillovers, contagion.panel VARs subject tothe curse dimensionality. Indeed, can characterized \n* Dynamic interdependence: potentially, lags \nendogenous variables units can enter model unit \\(\\).\n* Static interdependence: \\(\\varepsilon_{}\\) \ngenerally correlated across \\(\\).\n* Cross sectional heterogeneity: intercept, slope \nvariance shocks may unit-specific.","code":""},{"path":"PanelVARs.html","id":"without-dynamic-interdependence","chapter":"9 Panel VARs","heading":"9.1 Without Dynamic interdependence","text":"panel VAR, assuming dynamic interdependence, form:\n\\[ y_{}=c_i+\\Phi_i(L)\n\\color{red}{y_{-1}}+\\varepsilon_{}.\\]comparison, consider micro panel data, univariate cae (AR(1) case): \\[y_{}=c_i+{\\color{red}\\phi} y_{-1}+\\varepsilon_{}.\\] kind context, usually cross-sectional heterogeneity \\(\\phi_i=\\phi\\) \\(\\). Typically, large cross-sectional dimension \\(N\\), small time dimension \\(T\\). one uses ``Fixed-effect’’ regression:\n\\[y_{}-\\frac{1}{T}\\sum_{s=1}^Ty_{}=\\phi(y_{-1}-\\frac{1}{T}\\sum_{s=1}^Ty_{})+\\varepsilon_{}-\\frac{1}{T}\\sum_{s=1}^T\\color{red}{\\varepsilon_{}},\\]\none faces Nickell bias: lagged dependent variable, estimator biased, bias size \\(\\sim 1/T\\). One can use GMM regressions (Arellano Bond (1991)) get unbiased estimates.Macro panel data different structure, typically moderate cross-sectional dimension \\(N\\) large time dimension \\(T\\), Nickell bias negligible (\\(\\rightarrow 0\\) \\(T\\rightarrow\\infty\\)).","code":""},{"path":"PanelVARs.html","id":"mean-group-estimator","chapter":"9 Panel VARs","heading":"9.1.1 Mean Group Estimator","text":"etimate\n\\[ y_{}=c_i+\\Phi_i(L)\ny_{-1}+\\varepsilon_{},\\]\nneed take account cross-sectional heterogeneity coefficients, .e., different \\(\\Phi_i\\)’s across \\(\\)’s.\nPooled estimators (assuming cross-sectional homogeneity, .e. identical \\(\\Phi_i\\)’s across \\(\\)s) consistent (biased) underlying dynamics actually heterogeneous. contrast, Mean Group (MG) estimator, consists estimating \\(N\\) separate regressions\ncalculating coefficient means, consistent.","code":""},{"path":"PanelVARs.html","id":"shock-identification","chapter":"9 Panel VARs","heading":"9.1.2 Shock identification","text":"Shock identification can performed standard methods (zeros, signs, FEVM, etc.). make assumption \\(\\Omega\\) block-diagonal (interdependence impact), consistent assumption cross-sectional dependence.","code":""},{"path":"PanelVARs.html","id":"with-dynamic-interdependencies","chapter":"9 Panel VARs","heading":"9.2 With Dynamic Interdependencies","text":"panel VAR accommodates dynamic interdependence form:\n\\[ y_{}=c_i+\\Phi_i(L)\ny_{t-1}+\\varepsilon_{}.\\]face serious curse dimensionality : \\(NGp+1\\) coefficients estimate \nequation.solution select eligible dynamic links (See instance Negro (2011)). Another alternative use factor model. consists capturing dynamic interdependencies set unobservable factors (See Canova Ciccarelli (2004) Canova Ciccarelli (2009)). See Section 10 details FAVAR models.","code":""},{"path":"FAVAR.html","id":"FAVAR","chapter":"10 Factor-Augmented VAR","heading":"10 Factor-Augmented VAR","text":"VAR models subject curse dimensionality: \\(n\\), large, number parameters (\\(n^2\\)) explodes.case one suspects \\(y_{,t}\\)’s mainly driven small number random sources, factor structure may imposed, principal component analysis (PCA, see Appendix 10.1) can employed estimate relevant factors (Bernanke, Boivin, Eliasz (2005)).","code":""},{"path":"FAVAR.html","id":"PCAapp","chapter":"10 Factor-Augmented VAR","heading":"10.1 Principal component analysis (PCA)","text":"Principal component analysis (PCA) classical easy--use statistical method reduce dimension large datasets containing variables linearly driven relatively small number factors. approach widely used data analysis image compression.Suppose \\(T\\) observations \\(n\\)-dimensional random vector \\(x\\), denoted \\(x_{1},x_{2},\\ldots,x_{T}\\). suppose component \\(x\\) mean zero.Let denote \\(X\\) matrix given \\(\\left[\\begin{array}{cccc}\nx_{1} & x_{2} & \\ldots & x_{T}\\end{array}\\right]'\\). Denote \\(j^{th}\\) column \\(X\\) \\(X_{j}\\).want find linear combination \\(x_{}\\)’s (\\(x.u\\)), \\(\\left\\Vert u\\right\\Vert =1\\), “maximum variance.” , want solve:\n\\[\\begin{equation}\n\\begin{array}{clll}\n\\underset{u}{\\arg\\max} & u'X'Xu. \\\\\n\\mbox{s.t. } & \\left\\Vert u \\right\\Vert =1\n\\end{array}\\tag{10.1}\n\\end{equation}\\]Since \\(X'X\\) positive definite matrix, admits following decomposition:\n\\[\\begin{eqnarray*}\nX'X & = & PDP'\\\\\n& = & P\\left[\\begin{array}{ccc}\n\\lambda_{1}\\\\\n& \\ddots\\\\\n&  & \\lambda_{n}\n\\end{array}\\right]P',\n\\end{eqnarray*}\\]\n\\(P\\) orthogonal matrix whose columns eigenvectors \\(X'X\\).can order eigenvalues \\(\\lambda_{1}\\geq\\ldots\\geq\\lambda_{n}\\). (Since \\(X'X\\) positive definite, eigenvalues positive.)Since \\(P\\) orthogonal, \\(u'X'Xu=u'PDP'u=y'Dy\\) \\(\\left\\Vert y\\right\\Vert =1\\). Therefore, \\(y_{}^{2}\\leq 1\\) \\(\\leq n\\).consequence:\n\\[\ny'Dy=\\sum_{=1}^{n}y_{}^{2}\\lambda_{}\\leq\\lambda_{1}\\sum_{=1}^{n}y_{}^{2}=\\lambda_{1}.\n\\]easily seen maximum reached \\(y=\\left[1,0,\\cdots,0\\right]'\\). Therefore, maximum optimization program (Eq. (10.1)) obtained \\(u=P\\left[1,0,\\cdots,0\\right]'\\). , \\(u\\) eigenvector \\(X'X\\) associated larger eigenvalue (first column \\(P\\)).Let us denote \\(F\\) vector given matrix product \\(XP\\). columns \\(F\\), denoted \\(F_{j}\\), called factors. :\n\\[\nF'F=P'X'XP=D.\n\\]\nTherefore, particular, \\(F_{j}\\)’s orthogonal.Since \\(X=FP'\\), \\(X_{j}\\)’s linear combinations factors. Let us denote \\(\\hat{X}_{,j}\\) part \\(X_{}\\) explained factor \\(F_{j}\\), :\n\\[\\begin{eqnarray*}\n\\hat{X}_{,j} & = & p_{ij}F_{j}\\\\\nX_{} & = & \\sum_{j}\\hat{X}_{,j}=\\sum_{j}p_{ij}F_{j}.\n\\end{eqnarray*}\\]Consider share variance explained—\\(n\\) variables (\\(X_{1},\\ldots,X_{n}\\))—first factor \\(F_{1}\\):\n\\[\\begin{eqnarray*}\n\\frac{\\sum_{}\\hat{X}'_{,1}\\hat{X}_{,1}}{\\sum_{}X_{}'X_{}} & = & \\frac{\\sum_{}p_{i1}F'_{1}F_{1}p_{i1}}{tr(X'X)} = \\frac{\\sum_{}p_{i1}^{2}\\lambda_{1}}{tr(X'X)} = \\frac{\\lambda_{1}}{\\sum_{}\\lambda_{}}.\n\\end{eqnarray*}\\]Intuitively, first eigenvalue large, means first factor captures large share fluctutaions \\(n\\) \\(X_{}\\)’s.token, easily seen fraction variance \\(n\\) variables explained factor \\(j\\) given :\n\\[\\begin{eqnarray*}\n\\frac{\\sum_{}\\hat{X}'_{,j}\\hat{X}_{,j}}{\\sum_{}X_{}'X_{}} & = & \\frac{\\lambda_{j}}{\\sum_{}\\lambda_{}}.\n\\end{eqnarray*}\\]Let us illustrate PCA term structure yields. term strucutre yields (yield curve) know driven small number factors (e.g., Litterman Scheinkman (1991)). One can typically employ PCA recover factors. data used example taken Fred database (tickers: “DGS6MO”,“DGS1”, …). second plot shows factor loardings, indicate first factor level factor (loadings = black line), second factor slope factor (loadings = blue line), third factor curvature factor (loadings = red line).run PCA, one simply apply function prcomp matrix data:Let us know visualize results. first plot Figure 10.1 shows share total variance explained different principal components (PCs). second plot shows facotr loadings. two bottom plots show yields (black) fitted linear combinations first two PCs .\nFigure 10.1: PCA results. dataset contains 8 time series U.S. interest rates different maturities.\n","code":"\nlibrary(IdSS)\nUSyields <- USyields[complete.cases(USyields),]\nyds <- USyields[c(\"Y1\",\"Y2\",\"Y3\",\"Y5\",\"Y7\",\"Y10\",\"Y20\",\"Y30\")]\nPCA.yds <- prcomp(yds,center=TRUE,scale. = TRUE)\npar(mfrow=c(2,2))\npar(plt=c(.1,.95,.2,.8))\nbarplot(PCA.yds$sdev^2/sum(PCA.yds$sdev^2),\n        main=\"Share of variance expl. by PC's\")\naxis(1, at=1:dim(yds)[2], labels=colnames(PCA.yds$x))\nnb.PC <- 2\nplot(-PCA.yds$rotation[,1],type=\"l\",lwd=2,ylim=c(-1,1),\n     main=\"Factor loadings (1st 3 PCs)\",xaxt=\"n\",xlab=\"\")\naxis(1, at=1:dim(yds)[2], labels=colnames(yds))\nlines(PCA.yds$rotation[,2],type=\"l\",lwd=2,col=\"blue\")\nlines(PCA.yds$rotation[,3],type=\"l\",lwd=2,col=\"red\")\nY1.hat <- PCA.yds$x[,1:nb.PC] %*% PCA.yds$rotation[\"Y1\",1:2]\nY1.hat <- mean(USyields$Y1) + sd(USyields$Y1) * Y1.hat\nplot(USyields$date,USyields$Y1,type=\"l\",lwd=2,\n     main=\"Fit of 1-year yields (2 PCs)\",\n     ylab=\"Obs (black) / Fitted by 2PCs (dashed blue)\")\nlines(USyields$date,Y1.hat,col=\"blue\",lty=2,lwd=2)\nY10.hat <- PCA.yds$x[,1:nb.PC] %*% PCA.yds$rotation[\"Y10\",1:2]\nY10.hat <- mean(USyields$Y10) + sd(USyields$Y10) * Y10.hat\nplot(USyields$date,USyields$Y10,type=\"l\",lwd=2,\n     main=\"Fit of 10-year yields (2 PCs)\",\n     ylab=\"Obs (black) / Fitted by 2PCs (dashed blue)\")\nlines(USyields$date,Y10.hat,col=\"blue\",lty=2,lwd=2)"},{"path":"FAVAR.html","id":"favar-models","chapter":"10 Factor-Augmented VAR","heading":"10.2 FAVAR models","text":"Let us denote \\(F_t\\) \\(k\\)-dimensional vector latent factors accounting important shares variances \\(y_{,t}\\)’s (\\(K \\ll n\\)) \\(x_t\\) small \\(M\\)-dimensional subset \\(y_t\\) (\\(M \\ll n\\)). following factor structure posited:\n\\[\ny_t = \\Lambda^f F_t + \\Lambda^x x_t + e_t,\n\\]\n\\(e_t\\) “small” serially mutually ..d. error terms. \\(F_t\\) \\(x_t\\) supposed drive fluctuations \\(y_t\\)’s components.model complemented positing VAR dynamics \\([F_t',x_t']'\\):\n\\[\\begin{equation}\n\\left[\\begin{array}{c}F_t\\\\x_t\\end{array}\\right] = \\Phi(L)\\left[\\begin{array}{c}F_{t-1}\\\\ x_{t-1}\\end{array}\\right] + v_t.\\tag{10.2}\n\\end{equation}\\]Standard identification techniques structural shocks can employed Eq. (10.2): Cholesky approach can used instance last component \\(x_t\\) short-term interest rate assumed MP shock contemporaneous impact variables.identification procedure, Bernanke, Boivin, Eliasz (2005) exploit fact macro-finance variables can decomposed two sets—fast-moving slow-moving variables—former reacts contemporaneously monetary-policy shocks. Now, estimate (unobserved) factors \\(F_t\\)? Bernanke, Boivin, Eliasz (2005) note first \\(K+M\\) PCA whole dataset (\\(y_t\\)), denote \\(\\hat{C}(F_t,x_t)\\) span space \\(F_t\\) \\(x_t\\). get estimate \\(F_t\\), dependence \\(\\hat{C}(F_t,x_t)\\) \\(x_t\\) removed. done regressing, OLS, \\(\\hat{C}(F_t,x_t)\\) \\(x_t\\) \\(\\hat{C}^*(F_t)\\), latter estimate common components \\(x_t\\). proxy \\(\\hat{C}^*(F_t)\\), Bernanke, Boivin, Eliasz (2005) take principal components set slow-moving variables, comtemporaneously correlated \\(x_t\\). Vector \\(\\hat{F}_t\\) computed \\(\\hat{C}(F_t,x_t) - b_x x_t\\), \\(b_x\\) coefficients coming previous OLS regressions.Note approach implies vectorial space spanned \\((\\hat{F}_t,x_t)\\) spanned \\(\\hat{C}(F_t,x_t)\\)., employ method dataset built McCracken Ng (2016) —FRED:MD database— includes 119 time series.\nFigure 10.2: Responses monetary-policy shock. FAVAR approach Bernanke, Boivin, Eliasz (2005). FRED-MD dataset.\n","code":"\nlibrary(BVAR)# contains the fred_md dataset\nlibrary(IdSS)\nlibrary(vars)\ndata <- fred_transform(fred_md,na.rm = FALSE, type = \"fred_md\")\ndata <- data[complete.cases(data),]\ndata.values <- scale(data, center = TRUE, scale = TRUE)\ndata_scaled <- data\ndata_scaled[1:dim(data)[1],1:dim(data)[2]] <- data.values\nK <- 3\nM <- 1\nPCA <- prcomp(data_scaled) # implies that PCA$x %*% t(PCA$rotation) = data\nC.hat <- PCA$x[,1:(K+M)]\nfast_moving <- c(\"HOUST\",\"HOUSTNE\",\"HOUSTMW\",\"HOUSTS\",\"HOUSTW\",\"HOUSTS\",\"AMDMNOx\",\n                 \"FEDFUNDS\",\"CP3Mx\",\"TB3MS\",\"TB6MS\",\"GS1\",\"GS5\",\"GS10\",\n                 \"COMPAPFFx\",\"TB3SMFFM\",\"TB6SMFFM\",\"T1YFFM\",\"T5YFFM\",\"T10YFFM\",\n                 \"AAAFFM\",\"EXSZUSx\",\"EXJPUSx\",\"EXUSUKx\",\"EXCAUSx\")\ndata.slow <- data_scaled[,-which(fast_moving %in% names(data))]\nPCA.star <- prcomp(data.slow) # implies that PCA$x %*% t(PCA$rotation) = data\nC.hat.star <- PCA.star$x[,1:K]\nD <- cbind(data$FEDFUNDS,C.hat.star)\nb.x <- solve(t(D)%*%D) %*% t(D) %*% C.hat\nF.hat <- C.hat - data$FEDFUNDS %*% matrix(b.x[1,],nrow=1)\ndata_var <- data.frame(F.hat, FEDFUNDS = data$FEDFUNDS)\np <- 10\nvar <- VAR(data_var, p)\nOmega <- var(residuals(var))\nB <- t(chol(Omega))\nD <- cbind(F.hat,data$FEDFUNDS)\nloadings <- solve(t(D)%*%D) %*% t(D) %*% as.matrix(data_scaled)\nirf <- simul.VAR(c=rep(0,(K+M)*p),Phi=Acoef(var),B,nb.sim=120,\n                 y0.star=rep(0,(K+M)*p),indic.IRF = 1,\n                 u.shock = c(rep(0,K+1),1))\nirf.all <- irf %*% loadings\npar(mfrow=c(2,2))\nvariables.2.plot <- c(\"FEDFUNDS\",\"INDPRO\",\"UNRATE\",\"CPIAUCSL\")\npar(plt=c(.2,.95,.3,.95))\nfor(i in 1:length(variables.2.plot)){\n  plot(cumsum(irf.all[,which(variables.2.plot[i]==names(data))]),lwd=2,\n       type=\"l\",xlab=\"months after shock\",ylab=variables.2.plot[i])\n}"},{"path":"append.html","id":"append","chapter":"11 Appendix","heading":"11 Appendix","text":"","code":""},{"path":"append.html","id":"definitions-and-statistical-results","chapter":"11 Appendix","heading":"11.1 Definitions and statistical results","text":"Definition 11.1  (Covariance stationarity) process \\(y_t\\) covariance stationary —weakly stationary— , \\(t\\) \\(j\\),\n\\[\n\\mathbb{E}(y_t) = \\mu \\quad \\mbox{} \\quad \\mathbb{E}\\{(y_t - \\mu)(y_{t-j} - \\mu)\\} = \\gamma_j.\n\\]Definition 11.2  (Likelihood Ratio test statistics) likelihood ratio associated restriction form \\(H_0: h({\\boldsymbol\\theta})=0\\) (\\(h({\\boldsymbol\\theta})\\) \\(r\\)-dimensional vector) given :\n\\[\nLR = \\frac{\\mathcal{L}_R(\\boldsymbol\\theta;\\mathbf{y})}{\\mathcal{L}_U(\\boldsymbol\\theta;\\mathbf{y})} \\quad (\\[0,1]),\n\\]\n\\(\\mathcal{L}_R\\) (respectively \\(\\mathcal{L}_U\\)) likelihood function imposes (resp. impose) restriction. likelihood ratio test statistic given \\(-2\\log(LR)\\), :\n\\[\n\\boxed{\\xi^{LR}= 2 (\\log\\mathcal{L}_U(\\boldsymbol\\theta;\\mathbf{y})-\\log\\mathcal{L}_R(\\boldsymbol\\theta;\\mathbf{y})).}\n\\]\nregularity assumptions null hypothesis, test statistic follows chi-square distribution \\(r\\) degrees freedom (see Table 11.3).Proposition 11.1  (p.d.f. multivariate Gaussian variable) \\(Y \\sim \\mathcal{N}(\\mu,\\Omega)\\) \\(Y\\) \\(n\\)-dimensional vector, density function \\(Y\\) :\n\\[\n\\frac{1}{(2 \\pi)^{n/2}|\\Omega|^{1/2}}\\exp\\left[-\\frac{1}{2}\\left(Y-\\mu\\right)'\\Omega^{-1}\\left(Y-\\mu\\right)\\right].\n\\]","code":""},{"path":"append.html","id":"AppEstimVARMA","chapter":"11 Appendix","heading":"11.2 Estimation of VARMA models","text":"Section 1.4 discusses estimation VAR models shows standard VAR models can esitmated running OLS regressions.MA component (.e., consider VARMA model), OLS regressions yield biased estimates (even asymptotically large samples). Assume instance \\(y_t\\) follows VARMA(1,1) model:\n\\[\ny_{,t} = \\phi_i y_{t-1} + \\varepsilon_{,t},\n\\]\n\\(\\phi_i\\) \\(^{th}\\) row \\(\\Phi_1\\), \\(\\varepsilon_{,t}\\) linear combination \\(\\eta_t\\) \\(\\eta_{t-1}\\). Since \\(y_{t-1}\\) (regressor) correlated \\(\\eta_{t-1}\\), also correlated \\(\\varepsilon_{,t}\\). OLS regression \\(y_{,t}\\) \\(y_{t-1}\\) yields biased estimator \\(\\phi_i\\) (see Figure 11.1). Hence, SVARMA models consistently estimated simple OLS regressions (contrary VAR models, see next section); instrumental-variable approaches can employed estimate SVARMA models (using past values \\(y_t\\) instruments, see, e.g., Gouriéroux, Monfort, Renne (2020)).\nFigure 11.1: Illustration bias obtained estimating auto-regressive parameters ARMA process (standard) OLS.\n","code":"\nN <- 1000 # number of replications\nT <- 100 # sample length\nphi <- .8 # autoregressive parameter\nsigma <- 1\npar(mfrow=c(1,2))\nfor(theta in c(0,-0.4)){\n  all.y <- matrix(0,1,N)\n  y     <- all.y\n  eta_1 <- rnorm(N)\n  for(t in 1:(T+1)){\n    eta <- rnorm(N)\n    y <- phi * y + sigma * eta + theta * sigma * eta_1\n    all.y <- rbind(all.y,y)\n    eta_1 <- eta\n  }\n  all.y_1 <- all.y[1:T,]\n  all.y   <- all.y[2:(T+1),]\n  XX_1 <- 1/apply(all.y_1 * all.y_1,2,sum)\n  XY   <- apply(all.y_1 * all.y,2,sum)\n  phi.est.OLS <- XX_1 * XY\n  plot(density(phi.est.OLS),xlab=\"OLS estimate of phi\",ylab=\"\",\n       main=paste(\"theta = \",theta,sep=\"\"))\n  abline(v=phi,col=\"red\",lwd=2)}"},{"path":"append.html","id":"AppendixProof","chapter":"11 Appendix","heading":"11.3 Proofs","text":"Proof Proposition 1.2Proof. Using Proposition 11.1, obtain , conditionally \\(x_1\\), log-likelihood given \n\\[\\begin{eqnarray*}\n\\log\\mathcal{L}(Y_{T};\\theta) & = & -(Tn/2)\\log(2\\pi)+(T/2)\\log\\left|\\Omega^{-1}\\right|\\\\\n&  & -\\frac{1}{2}\\sum_{t=1}^{T}\\left[\\left(y_{t}-\\Pi'x_{t}\\right)'\\Omega^{-1}\\left(y_{t}-\\Pi'x_{t}\\right)\\right].\n\\end{eqnarray*}\\]\nLet’s rewrite last term log-likelihood:\n\\[\\begin{eqnarray*}\n\\sum_{t=1}^{T}\\left[\\left(y_{t}-\\Pi'x_{t}\\right)'\\Omega^{-1}\\left(y_{t}-\\Pi'x_{t}\\right)\\right] & =\\\\\n\\sum_{t=1}^{T}\\left[\\left(y_{t}-\\hat{\\Pi}'x_{t}+\\hat{\\Pi}'x_{t}-\\Pi'x_{t}\\right)'\\Omega^{-1}\\left(y_{t}-\\hat{\\Pi}'x_{t}+\\hat{\\Pi}'x_{t}-\\Pi'x_{t}\\right)\\right] & =\\\\\n\\sum_{t=1}^{T}\\left[\\left(\\hat{\\varepsilon}_{t}+(\\hat{\\Pi}-\\Pi)'x_{t}\\right)'\\Omega^{-1}\\left(\\hat{\\varepsilon}_{t}+(\\hat{\\Pi}-\\Pi)'x_{t}\\right)\\right],\n\\end{eqnarray*}\\]\n\\(j^{th}\\) element \\((n\\times1)\\) vector \\(\\hat{\\varepsilon}_{t}\\) sample residual, observation \\(t\\), OLS regression \\(y_{j,t}\\) \\(x_{t}\\). Expanding previous equation, get:\n\\[\\begin{eqnarray*}\n&&\\sum_{t=1}^{T}\\left[\\left(y_{t}-\\Pi'x_{t}\\right)'\\Omega^{-1}\\left(y_{t}-\\Pi'x_{t}\\right)\\right]  = \\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}'\\Omega^{-1}\\hat{\\varepsilon}_{t}\\\\\n&&+2\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}'\\Omega^{-1}(\\hat{\\Pi}-\\Pi)'x_{t}+\\sum_{t=1}^{T}x'_{t}(\\hat{\\Pi}-\\Pi)\\Omega^{-1}(\\hat{\\Pi}-\\Pi)'x_{t}.\n\\end{eqnarray*}\\]\nLet’s apply trace operator second term (scalar):\n\\[\\begin{eqnarray*}\n\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}'\\Omega^{-1}(\\hat{\\Pi}-\\Pi)'x_{t} & = & Tr\\left(\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}'\\Omega^{-1}(\\hat{\\Pi}-\\Pi)'x_{t}\\right)\\\\\n=  Tr\\left(\\sum_{t=1}^{T}\\Omega^{-1}(\\hat{\\Pi}-\\Pi)'x_{t}\\hat{\\varepsilon}_{t}'\\right) & = & Tr\\left(\\Omega^{-1}(\\hat{\\Pi}-\\Pi)'\\sum_{t=1}^{T}x_{t}\\hat{\\varepsilon}_{t}'\\right).\n\\end{eqnarray*}\\]\nGiven , construction (property OLS estimates), sample residuals orthogonal explanatory variables, term zero. Introducing \\(\\tilde{x}_{t}=(\\hat{\\Pi}-\\Pi)'x_{t}\\), \n\\[\\begin{eqnarray*}\n\\sum_{t=1}^{T}\\left[\\left(y_{t}-\\Pi'x_{t}\\right)'\\Omega^{-1}\\left(y_{t}-\\Pi'x_{t}\\right)\\right] =\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}'\\Omega^{-1}\\hat{\\varepsilon}_{t}+\\sum_{t=1}^{T}\\tilde{x}'_{t}\\Omega^{-1}\\tilde{x}_{t}.\n\\end{eqnarray*}\\]\nSince \\(\\Omega\\) positive definite matrix, \\(\\Omega^{-1}\\) well. Consequently, smallest value last term can take obtained \\(\\tilde{x}_{t}=0\\), .e. \\(\\Pi=\\hat{\\Pi}.\\)MLE \\(\\Omega\\) matrix \\(\\hat{\\Omega}\\) maximizes \\(\\Omega\\overset{\\ell}{\\rightarrow}L(Y_{T};\\hat{\\Pi},\\Omega)\\). :\n\\[\\begin{eqnarray*}\n\\log\\mathcal{L}(Y_{T};\\hat{\\Pi},\\Omega) & = & -(Tn/2)\\log(2\\pi)+(T/2)\\log\\left|\\Omega^{-1}\\right| -\\frac{1}{2}\\sum_{t=1}^{T}\\left[\\hat{\\varepsilon}_{t}'\\Omega^{-1}\\hat{\\varepsilon}_{t}\\right].\n\\end{eqnarray*}\\]Matrix \\(\\hat{\\Omega}\\) symmetric positive definite. easily checked (unrestricted) matrix maximizes latter expression symmetric positive definite matrix. Indeed:\n\\[\n\\frac{\\partial \\log\\mathcal{L}(Y_{T};\\hat{\\Pi},\\Omega)}{\\partial\\Omega}=\\frac{T}{2}\\Omega'-\\frac{1}{2}\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}\\hat{\\varepsilon}'_{t}\\Rightarrow\\hat{\\Omega}'=\\frac{1}{T}\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}\\hat{\\varepsilon}'_{t},\n\\]\nleads result.Proof Proposition 1.3Proof. Let us drop \\(\\) subscript. Rearranging Eq. (1.15), :\n\\[\n\\sqrt{T}(\\mathbf{b}-\\boldsymbol{\\beta}) =  (X'X/T)^{-1}\\sqrt{T}(X'\\boldsymbol\\varepsilon/T).\n\\]\nLet us consider autocovariances \\(\\mathbf{v}_t = x_t \\varepsilon_t\\), denoted \\(\\gamma^v_j\\). Using fact \\(x_t\\) linear combination past \\(\\varepsilon_t\\)s \\(\\varepsilon_t\\) white noise, get \\(\\mathbb{E}(\\varepsilon_t x_t)=0\\). Therefore\n\\[\n\\gamma^v_j = \\mathbb{E}(\\varepsilon_t\\varepsilon_{t-j}x_tx_{t-j}').\n\\]\n\\(j>0\\), \\(\\mathbb{E}(\\varepsilon_t\\varepsilon_{t-j}x_tx_{t-j}')=\\mathbb{E}(\\mathbb{E}[\\varepsilon_t\\varepsilon_{t-j}x_tx_{t-j}'|\\varepsilon_{t-j},x_t,x_{t-j}])=\\) \\(\\mathbb{E}(\\varepsilon_{t-j}x_tx_{t-j}'\\mathbb{E}[\\varepsilon_t|\\varepsilon_{t-j},x_t,x_{t-j}])=0\\). Note \\(\\mathbb{E}[\\varepsilon_t|\\varepsilon_{t-j},x_t,x_{t-j}]=0\\) \\(\\{\\varepsilon_t\\}\\) ..d. white noise sequence. \\(j=0\\), :\n\\[\n\\gamma^v_0 = \\mathbb{E}(\\varepsilon_t^2x_tx_{t}')= \\mathbb{E}(\\varepsilon_t^2) \\mathbb{E}(x_tx_{t}')=\\sigma^2\\mathbf{Q}.\n\\]\nconvergence distribution \\(\\sqrt{T}(X'\\boldsymbol\\varepsilon/T)=\\sqrt{T}\\frac{1}{T}\\sum_{t=1}^Tv_t\\) results Central Limit Theorem covariance-stationary processes, using \\(\\gamma_j^v\\) computed .","code":""},{"path":"append.html","id":"statistical-tables","chapter":"11 Appendix","heading":"11.4 Statistical Tables","text":"Table 11.1: Quantiles \\(\\mathcal{N}(0,1)\\) distribution. \\(\\) \\(b\\) respectively row column number; corresponding cell gives \\(\\mathbb{P}(0<X\\le +b)\\), \\(X \\sim \\mathcal{N}(0,1)\\).Table 11.2: Quantiles Student-\\(t\\) distribution. rows correspond different degrees freedom (\\(\\nu\\), say); columns correspond different probabilities (\\(z\\), say). cell gives \\(q\\) s.t. \\(\\mathbb{P}(-q<X<q)=z\\), \\(X \\sim t(\\nu)\\).Table 11.3: Quantiles \\(\\chi^2\\) distribution. rows correspond different degrees freedom; columns correspond different probabilities.Table 11.4: Quantiles \\(\\mathcal{F}\\) distribution. columns rows correspond different degrees freedom (resp. \\(n_1\\) \\(n_2\\)). different panels correspond different probabilities (\\(\\alpha\\)) corresponding cell gives \\(z\\) s.t. \\(\\mathbb{P}(X \\le z)=\\alpha\\), \\(X \\sim \\mathcal{F}(n_1,n_2)\\).","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
