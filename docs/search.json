[{"path":"index.html","id":"intro","chapter":"1 Before starting","heading":"1 Before starting","text":"WORK PROGRESSThe identification estimation dynamic responses structural shocks one principal goals macroeconometrics. responses correspond effect, time, exogenous intervention propagates economy, modelled system simultaneous equations.last decades, several methodologies proposed estimate responses. objective course, developed Kenza Benhima Jean-Paul Renne, provide exhaustive view methodologies provide students tools enabling implement various contexts.Codes associated course part AEC package, available GitHub. install , one need employ devtools library:Useful (R) links:Download R:\nR software: https://cran.r-project.org (basic R software)\nRStudio: https://www.rstudio.com (convenient R editor)\nDownload R:R software: https://cran.r-project.org (basic R software)RStudio: https://www.rstudio.com (convenient R editor)Tutorials:\nRstudio: https://dss.princeton.edu/training/RStudio101.pdf (Oscar Torres-Reyna)\nR: https://cran.r-project.org/doc/contrib/Paradis-rdebuts_en.pdf (Emmanuel Paradis)\ntutorial: https://jrenne.shinyapps.io/Rtuto_publiShiny/\nTutorials:Rstudio: https://dss.princeton.edu/training/RStudio101.pdf (Oscar Torres-Reyna)R: https://cran.r-project.org/doc/contrib/Paradis-rdebuts_en.pdf (Emmanuel Paradis)tutorial: https://jrenne.shinyapps.io/Rtuto_publiShiny/","code":"\nlibrary(devtools)\ninstall_github(\"jrenne/AEC\")\nlibrary(AEC)"},{"path":"TS.html","id":"TS","chapter":"2 Time Series","heading":"2 Time Series","text":"","code":""},{"path":"TS.html","id":"introduction-to-time-series","chapter":"2 Time Series","heading":"2.1 Introduction to time series","text":"time series infinite sequence random variables indexed time: \\(\\{y_t\\}_{t=-\\infty}^{+\\infty}=\\{\\dots, y_{-2},y_{-1},y_{0},y_{1},\\dots,y_t,\\dots\\}\\), \\(y_i \\\\mathbb{R}^k\\). practice, observe samples, typically: \\(\\{y_{1},\\dots,y_T\\}\\).Standard time series models built using shocks often denote \\(\\varepsilon_t\\). Typically, \\(\\mathbb{E}(\\varepsilon_t)=0\\). many models, shocks supposed ..d., exist (less restrictive) notions shocks. particular, definition many processes based whote noises:Definition 2.1  (White noise) process \\(\\{\\varepsilon_t\\}_{t \\] -\\infty,+\\infty[}\\) white noise , \\(t\\):\\(\\mathbb{E}(\\varepsilon_t)=0\\),\\(\\mathbb{E}(\\varepsilon_t^2)=\\sigma^2<\\infty\\) andfor \\(s\\ne t\\), \\(\\mathbb{E}(\\varepsilon_t \\varepsilon_s)=0\\).Another type shocks commonly used Martingale Difference Sequences:Definition 2.2  (Martingale Difference Sequence) process \\(\\{\\varepsilon_t\\}_{t = -\\infty}^{+\\infty}\\) martingale difference sequence (MDS) \\(\\mathbb{E}(|\\varepsilon_{t}|)<\\infty\\) , \\(t\\),\n\\[\n\\underbrace{\\mathbb{E}_{t-1}(\\varepsilon_{t})}_{\\mbox{Expectation conditional past}}=0.\n\\]definition, \\(y_t\\) martingale, \\(y_{t}-y_{t-1}\\) MDS.Example 2.1  (ARCH process) Autoregressive conditional heteroskedasticity (ARCH) process example shock satisfies MDS definition ..d.:\n\\[\n\\varepsilon_{t} = \\sigma_t \\times z_{t},\n\\]\n\\(z_t \\sim ..d.\\,\\mathcal{N}(0,1)\\) \\(\\sigma_t^2 = w + \\alpha \\varepsilon_{t-1}^2\\).Example 2.2  white noise process necessarily MDS. instance following process:\n\\[\n\\varepsilon_{t} = z_t + z_{t-1}z_{t-2},\n\\]\n\\(z_t \\sim ..d.\\mathcal{N}(0,1)\\).Let us now introduce lag operator. lag operator, denoted \\(L\\), defined time series space defined :\n\\[\\begin{equation}\nL: \\{y_t\\}_{t=-\\infty}^{+\\infty} \\rightarrow \\{w_t\\}_{t=-\\infty}^{+\\infty} \\quad \\mbox{} \\quad w_t = y_{t-1}.\\tag{2.1}\n\\end{equation}\\]: \\(L^2 y_t = y_{t-2}\\) , generally, \\(L^k y_t = y_{t-k}\\).Consider time series \\(y_t\\) defined \\(y_t = \\mu + \\phi y_{t-1} + \\varepsilon_t\\), \\(\\varepsilon_t\\)’s ..d. \\(\\mathcal{N}(0,\\sigma^2)\\). Using lag operator, dynamics \\(y_t\\) can expressed follows:\n\\[\n(1-\\phi L) y_t = \\mu + \\varepsilon_t.\n\\]easily checked \\(L^2 y_t = y_{t-2}\\) , generally, \\(L^k y_t = y_{t-k}\\).exists, unconditional (marginal) mean random variable \\(y_t\\) given :\n\\[\n\\mu_t := \\mathbb{E}(y_t) = \\int_{-\\infty}^{\\infty} y_t f_{Y_t}(y_t) dy_t,\n\\]\n\\(f_{Y_t}\\) unconditional (marginal) density \\(y_t\\). Similarly, exists, unconditional (marginal) variance random variable \\(y_t\\) :\n\\[\n\\mathbb{V}ar(y_t) = \\int_{-\\infty}^{\\infty} (y_t - \\mathbb{E}(y_t))^2 f_{Y_t}(y_t) dy_t.\n\\]Definition 2.3  (Autocovariance) \\(j^{th}\\) autocovariance \\(y_t\\) given :\n\\[\\begin{eqnarray*}\n\\gamma_{j,t} &:=& \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} \\dots \\int_{-\\infty}^{\\infty} [y_t - \\mathbb{E}(y_t)][y_{t-j} - \\mathbb{E}(y_{t-j})] \\times\\\\\n&& f_{Y_t,Y_{t-1},\\dots,Y_{t-j}}(y_t,y_{t-1},\\dots,y_{t-j}) dy_t dy_{t-1} \\dots dy_{t-j} \\\\\n&=& \\mathbb{E}([y_t - \\mathbb{E}(y_t)][y_{t-j} - \\mathbb{E}(y_{t-j})]),\n\\end{eqnarray*}\\]\n\\(f_{Y_t,Y_{t-1},\\dots,Y_{t-j}}(y_t,y_{t-1},\\dots,y_{t-j})\\) joint distribution \\(y_t,y_{t-1},\\dots,y_{t-j}\\).particular, \\(\\gamma_{0,t} = \\mathbb{V}ar(y_t)\\).Definition 2.4  (Covariance stationarity) process \\(y_t\\) covariance stationary —weakly stationary— , \\(t\\) \\(j\\),\n\\[\n\\mathbb{E}(y_t) = \\mu \\quad \\mbox{} \\quad \\mathbb{E}\\{(y_t - \\mu)(y_{t-j} - \\mu)\\} = \\gamma_j.\n\\]Figure 2.1 displays simulation process covariance stationary. process follows \\(y_t = 0.1t + \\varepsilon_t\\), \\(\\varepsilon_t \\sim\\,..d.\\,\\mathcal{N}(0,1)\\). Indeed, process, : \\(\\mathbb{E}(y_t)=0.1t\\), depends \\(t\\).\nFigure 2.1: Example process covariance stationary (\\(y_t = 0.1t + \\varepsilon_t\\), \\(\\varepsilon_t \\sim \\mathcal{N}(0,1)\\)).\nDefinition 2.5  (Strict stationarity) process \\(y_t\\) strictly stationary , \\(t\\) sets integers \\(J=\\{j_1,\\dots,j_n\\}\\), distribution \\((y_{t},y_{t+j_1},\\dots,y_{t+j_n})\\) depends \\(J\\) \\(t\\).following process covariance stationary strictly stationary:\n\\[\ny_t = \\mathbb{}_{\\{t<1000\\}}\\varepsilon_{1,t}+\\mathbb{}_{\\{t\\ge1000\\}}\\varepsilon_{2,t},\n\\]\n\\(\\varepsilon_{1,t} \\sim \\mathcal{N}(0,1)\\) \\(\\varepsilon_{2,t} \\sim \\sqrt{\\frac{\\nu - 2}{\\nu}} t(\\nu)\\) \\(\\nu = 4\\).\nFigure 2.2: Example process covariance stationary strictly stationary. red lines delineate 99% confidence interval standard normal distribution (\\(\\pm 2.58\\)).\nProposition 2.1  \\(y_t\\) covariance stationary, \\(\\gamma_j = \\gamma_{-j}\\).Proof. Since \\(y_t\\) covariance stationary, covariance \\(y_t\\) \\(y_{t-j}\\) (.e \\(\\gamma_j\\)) \\(y_{t+j}\\) \\(y_{t+j-j}\\) (.e. \\(\\gamma_{-j}\\)).Definition 2.6  (Auto-correlation) \\(j^{th}\\) auto-correlation covariance-stationary process :\n\\[\n\\rho_j = \\frac{\\gamma_j}{\\gamma_0}.\n\\]Consider long historical time series Swiss GDP growth, taken Jordà, Schularick, Taylor (2017) dataset.1\nFigure 2.3: Annual growth rate Swiss GDP, based Jorda-Schularick-Taylor Macrohistory Database.\n\nFigure 2.4: order \\(j\\), slope blue line , approximately, \\(\\hat{\\gamma}_j/\\widehat{\\mathbb{V}ar}(y_t)\\), hats indicate sample moments.\nDefinition 2.7  (Mean ergodicity) covariance-stationary process \\(y_t\\) ergodic mean :\n\\[\n\\mbox{plim}_{T \\rightarrow +\\infty} \\frac{1}{T}\\sum_{t=1}^T y_t = \\mathbb{E}(y_t).\n\\]Definition 2.8  (Second-moment ergodicity) covariance-stationary process \\(y_t\\) ergodic second moments , \\(j\\):\n\\[\n\\mbox{plim}_{T \\rightarrow +\\infty} \\frac{1}{T}\\sum_{t=1}^T (y_t-\\mu) (y_{t-j}-\\mu) = \\gamma_j.\n\\]noted ergodicity stationarity different properties. Typically process \\(\\{x_t\\}\\) , \\(\\forall t\\), \\(x_t \\equiv y\\), \\(y \\sim\\,\\mathcal{N}(0,1)\\) (say), \\(\\{x_t\\}\\) stationary ergodic.Theorem 2.1  (Central Limit Theorem covariance-stationary processes) process \\(y_t\\) covariance stationary series autocovariances absolutely summable (\\(\\sum_{j=-\\infty}^{+\\infty} |\\gamma_j| <\\infty\\)), :\n\\[\\begin{eqnarray}\n\\bar{y}_T \\overset{m.s.}{\\rightarrow} \\mu &=& \\mathbb{E}(y_t) \\tag{2.2}\\\\\n\\mbox{lim}_{T \\rightarrow +\\infty} T \\mathbb{E}\\left[(\\bar{y}_T - \\mu)^2\\right] &=& \\sum_{j=-\\infty}^{+\\infty} \\gamma_j \\tag{2.3}\\\\\n\\sqrt{T}(\\bar{y}_T - \\mu) &\\overset{d}{\\rightarrow}& \\mathcal{N}\\left(0,\\sum_{j=-\\infty}^{+\\infty} \\gamma_j \\right) \\tag{2.4}.\n\\end{eqnarray}\\][Mean square (m.s.) distribution (d.) convergences: see Definitions 3.18 3.16.]Proof. Proposition 3.8, Eq. (2.3) implies Eq. (2.2). Eq. (2.3), see Appendix 3.5. Eq. (2.4), see Anderson (1971), p. 429.Definition 2.9  (Long-run variance) assumptions Theorem 2.1, limit appearing Eq. (2.3) exists called long-run variance. denoted \\(S\\), .e.:\n\\[\nS = \\Sigma_{j=-\\infty}^{+\\infty} \\gamma_j  = \\mbox{lim}_{T \\rightarrow +\\infty} T \\mathbb{E}[(\\bar{y}_T - \\mu)^2].\n\\]\\(y_t\\) ergodic second moments (see Def. 2.8), natural estimator \\(S\\) :\n\\[\\begin{equation}\n\\hat\\gamma_0 + 2 \\sum_{\\nu=1}^{q} \\hat\\gamma_\\nu, \\tag{2.5}\n\\end{equation}\\]\n\\(\\hat\\gamma_\\nu = \\frac{1}{T}\\sum_{\\nu+1}^{T} (y_t - \\bar{y})(y_{t-\\nu} - \\bar{y})\\).However, small samples, Eq. (2.5) necessarily result positive definite matrix. Newey West (1987) proposed estimator defect. estimator given :\n\\[\\begin{equation}\nS^{NW}=\\hat\\gamma_0 + 2 \\sum_{\\nu=1}^{q}\\left(1-\\frac{\\nu}{q+1}\\right) \\hat\\gamma_\\nu.\\tag{2.6}\n\\end{equation}\\]Loosely speaking, Theorem 2.1 says , given sample size, higher “persistency” proicess, lower accuracy sample mean estimate population mean. illustrate, consider three processes feature marginal variance (equal one, say), different autocorrelations: 0%, 70%, 99.9%. Figure 2.5 displays simulated paths three processes. indeed appears , larger autocorrelation process, sample mean (dashed red line) population mean (red solid line).type simulations can performed using ShinyApp (use panel “AR(1)”).\nFigure 2.5: three samples simulated using following data generating process: \\(x_t = \\mu + \\rho (x_{t-1}-\\mu) + \\sqrt{1-\\rho^2}\\varepsilon_t\\), \\(\\varepsilon_t \\sim \\mathcal{N}(0,1)\\). Case : \\(\\rho = 0\\); Case B: \\(\\rho = 0.7\\); Case C: \\(\\rho = 0.999\\). three cases, \\(\\mathbb{E}(x_t)=\\mu=2\\) \\(\\mathbb{V}ar(x_t)=1\\).\n","code":""},{"path":"TS.html","id":"univariate-processes","chapter":"2 Time Series","heading":"2.2 Univariate processes","text":"","code":""},{"path":"TS.html","id":"moving-average-ma-processes","chapter":"2 Time Series","heading":"2.2.1 Moving Average (MA) processes","text":"Definition 2.10  Consider white noise process \\(\\{\\varepsilon_t\\}_{t = -\\infty}^{+\\infty}\\) (Def. 2.1). \\(y_t\\) first-order moving average process , \\(t\\):\n\\[\ny_t = \\mu + \\varepsilon_t + \\theta \\varepsilon_{t-1}.\n\\]\\(\\mathbb{E}(\\varepsilon_t^2)=\\sigma^2\\), easily obtained unconditional mean variances \\(y_t\\) :\n\\[\n\\mathbb{E}(y_t) = \\mu, \\quad \\mathbb{V}ar(y_t) = (1+\\theta^2)\\sigma^2.\n\\]first auto-covariance :\n\\[\n\\gamma_1=\\mathbb{E}\\{(y_t - \\mu)(y_{t-1} - \\mu)\\} = \\theta \\sigma^2.\n\\]Higher-order auto-covariances zero (\\(\\gamma_j=0\\) \\(j>1\\)). Therefore: MA(1) process covariance-stationary (Def. 2.4).MA(1) process, autocorrelation order \\(j\\) (see Def. 2.6) given :\n\\[\n\\rho_j =\n\\left\\{\n\\begin{array}{lll}\n1 &\\mbox{ }& j=0,\\\\\n\\theta / (1 + \\theta^2) &\\mbox{ }& j = 1\\\\\n0 &\\mbox{ }& j>1.\n\\end{array}\n\\right.\n\\]Notice process \\(y_t\\) defined :\n\\[\ny_t = \\mu + \\varepsilon_t +\\theta \\varepsilon_{t-1},\n\\]\n\\(\\mathbb{V}ar(\\varepsilon_t)=\\sigma^2\\), mean autocovariances \n\\[\ny_t = \\mu + \\varepsilon^*_t +\\frac{1}{\\theta}\\varepsilon^*_{t-1},\n\\]\n\\(\\mathbb{V}ar(\\varepsilon^*_t)=\\theta^2\\sigma^2\\). , even perfectly know mean auto-covariances process, possible identify specification one used generate data. one two specifications said fundamental, one satisfies \\(|\\theta_1|<1\\) (see Eq. (2.28)).Definition 2.11  (MA(q) process) \\(q^{th}\\) order Moving Average process defined :\n\\[\ny_t = \\mu + \\varepsilon_t + \\theta_1 \\varepsilon_{t-1} + \\dots + \\theta_q \\varepsilon_{t-q}.\n\\]\n\\(\\{\\varepsilon_t\\}_{t = -\\infty}^{+\\infty}\\) white noise process (Def. 2.1).Proposition 2.2  (Covariance-stationarity MA(q) process) Finite-order Moving Average processes covariance-stationary.Moreover, autocovariances MA(q) process (defined Def. 2.11) given :\n\\[\\begin{equation}\n\\gamma_j = \\left\\{ \\begin{array}{ll} \\sigma^2(\\theta_j\\theta_0 + \\theta_{j+1}\\theta_{1} +  \\dots + \\theta_{q}\\theta_{q-j}) &\\mbox{} \\quad j \\\\{0,\\dots,q\\} \\\\ 0 &\\mbox{} \\quad j>q, \\end{array} \\right.\\tag{2.7}\n\\end{equation}\\]\nuse notation \\(\\theta_0=1\\), \\(\\mathbb{V}ar(\\varepsilon_t)=\\sigma^2\\).Proof. unconditional expectation \\(y_t\\) depend time, since \\(\\mathbb{E}(y_t)=\\mu\\). Let’s turn autocovariances. can extend series \\(\\theta_j\\)’s setting \\(\\theta_j=0\\) \\(j>q\\). :\n\\[\\begin{eqnarray*}\n\\mathbb{E}((y_t-\\mu)(y_{t-j}-\\mu)) &=& \\mathbb{E}\\left[(\\theta_0 \\varepsilon_t +\\theta_1 \\varepsilon_{t-1} + \\dots +\\theta_j \\varepsilon_{t-j}+\\theta_{j+1} \\varepsilon_{t-j-1} + \\dots) \\right.\\times \\\\\n&&\\left. (\\theta_0 \\varepsilon_{t-j} +\\theta_1 \\varepsilon_{t-j-1} + \\dots)\\right].\n\\end{eqnarray*}\\]\nuse fact \\(\\mathbb{E}(\\varepsilon_t\\varepsilon_s)=0\\) \\(t \\ne s\\) (\\(\\{\\varepsilon_t\\}_{t = -\\infty}^{+\\infty}\\) white noise process).Figure 2.6 displays simulated paths two MA processes (MA(1) MA(4)). simulations can produced using panel “ARMA(p,q)” web interface.\nFigure 2.6: Simulation MA processes.\norder \\(q\\) MA(q) process gets infinite? notion infinite-order Moving Average process exists important time series analysis. (infinite) sequence \\(\\theta_j\\) satisfy conditions process well-defined (see Theorem 2.2 ). conditions relate “summability” \\(\\{\\theta_{}\\}_{\\\\mathbb{N}}\\) (see Definition 2.12).Definition 2.12  (Absolute square summability) sequence \\(\\{\\theta_{}\\}_{\\\\mathbb{N}}\\) absolutely summable \\(\\sum_{=0}^{\\infty}|\\theta_i| < + \\infty\\), square summable \\(\\sum_{=0}^{\\infty} \\theta_i^2 < + \\infty\\).According Prop. 3.8, absolute summability implies square summability.Theorem 2.2  (Existence condition infinite MA process) \\(\\{\\theta_{}\\}_{\\\\mathbb{N}}\\) square summable (see Def. 2.12) \\(\\{\\varepsilon_t\\}_{t = -\\infty}^{+\\infty}\\) white noise process (see Def. 2.1), \n\\[\n\\mu + \\sum_{=0}^{+\\infty} \\theta_{} \\varepsilon_{t-}\n\\]\ndefines well-behaved [covariance-stationary] process, called infinite-order MA process (MA(\\(\\infty\\))).Proof. See Appendix 3.Hamilton. “Well behaved” means \\(\\Sigma_{=0}^{T} \\theta_{t-} \\varepsilon_{t-}\\) converges mean square (Def. 3.16) random variable \\(Z_t\\). proof makes use fact :\n\\[\n\\mathbb{E}\\left[\\left(\\sum_{=N}^{M}\\theta_{} \\varepsilon_{t-}\\right)^2\\right] = \\sum_{=N}^{M}|\\theta_{}|^2 \\sigma^2,\n\\]\n, \\(\\{\\theta_{}\\}\\) square summable, \\(\\forall \\eta>0\\), \\(\\exists N\\) s.t. right-hand-side term last equation lower \\(\\eta\\) \\(M \\ge N\\) (static Cauchy criterion, Theorem 3.2). implies \\(\\Sigma_{=0}^{T} \\theta_{} \\varepsilon_{t-}\\) converges mean square (stochastic Cauchy criterion, see Theorem 3.3).Proposition 2.3  (First two moments infinite MA process) \\(\\{\\theta_{}\\}_{\\\\mathbb{N}}\\) absolutely summable, .e. \\(\\sum_{=0}^{\\infty}|\\theta_i| < + \\infty\\), \\(y_t = \\mu + \\sum_{=0}^{+\\infty} \\theta_{} \\varepsilon_{t-}\\) exists (Theorem 2.2) :\n\\[\\begin{eqnarray*}\n\\mathbb{E}(y_t) &=& \\mu\\\\\n\\gamma_0 = \\mathbb{E}([y_t-\\mu]^2) &=& \\sigma^2(\\theta_0^2 +\\theta_1^2 + \\dots)\\\\\n\\gamma_j = \\mathbb{E}([y_t-\\mu][y_{t-j}-\\mu]) &=& \\sigma^2(\\theta_0\\theta_j + \\theta_{1}\\theta_{j+1} + \\dots).\n\\end{eqnarray*}\\]Process \\(y_t\\) absolutely summable auto-covariances, implies results Theorem 2.1 (Central Limit) apply.Proof. absolute summability \\(\\{\\theta_{}\\}\\) fact \\(\\mathbb{E}(\\varepsilon^2)<\\infty\\) imply order integration summation interchangeable (see Hamilton, 1994, Footnote p. 52), proves (). (ii), see end Appendix 3.Hamilton (1994).","code":"\nlibrary(AEC)\nT <- 100;nb.sim <- 1\ny.0 <- c(0)\nc <- 1;phi <- c(0);sigma <- 1\ntheta <- c(1,1) # MA(1) specification\ny.sim <- sim.arma(c,phi,theta,sigma,T,y.0,nb.sim)\npar(mfrow=c(1,2))\npar(plt=c(.2,.9,.2,.85))\nplot(y.sim[,1],xlab=\"\",ylab=\"\",type=\"l\",lwd=2,\n     main=expression(paste(theta[0],\"=1, \",theta[1],\"=1\",sep=\"\")))\nabline(h=c)\ntheta <- c(1,1,1,1,1) # MA(4) specification\ny.sim <- sim.arma(c,phi,theta,sigma,T,y.0,nb.sim)\nplot(y.sim[,1],xlab=\"\",ylab=\"\",type=\"l\",lwd=2,\n     main=expression(paste(theta[0],\"=...=\",theta[4],\"=1\",sep=\"\")))\nabline(h=c)"},{"path":"TS.html","id":"ARsection","chapter":"2 Time Series","heading":"2.2.2 Auto-Regressive (AR) processes","text":"Definition 2.13  (First-order AR process (AR(1))) Consider white noise process \\(\\{\\varepsilon_t\\}_{t = -\\infty}^{+\\infty}\\) (see Def. 2.1). Process \\(y_t\\) AR(1) process defined following difference equation:\n\\[\ny_t = c + \\phi y_{t-1} + \\varepsilon_t.\n\\]\\(|\\phi|\\ge1\\), \\(y_t\\) stationary. Indeed, :\n\\[\ny_{t+k} = c + \\varepsilon_{t+k} + \\phi  ( c + \\varepsilon_{t+k-1})+ \\phi^2  ( c + \\varepsilon_{t+k-2})+ \\dots + \\phi^{k-1}  ( c + \\varepsilon_{t+1}) + \\phi^k y_t.\n\\]\nTherefore, conditional variance\n\\[\n\\mathbb{V}ar_t(y_{t+k}) = \\sigma^2(1 + \\phi^2 + \\phi^4 + \\dots + \\phi^{2(k-1)})\n\\]\nconverge large \\(k\\)’s. implies \\(\\mathbb{V}ar(y_{t})\\) exist.contrast, \\(|\\phi| < 1\\), one can see :\n\\[\ny_t = c + \\varepsilon_t + \\phi  ( c + \\varepsilon_{t-1})+ \\phi^2  ( c + \\varepsilon_{t-2})+ \\dots + \\phi^k  ( c + \\varepsilon_{t-k}) + \\dots\n\\]\nHence, \\(|\\phi| < 1\\), unconditional mean variance \\(y_t\\) :\n\\[\n\\mathbb{E}(y_t) = \\frac{c}{1-\\phi} =: \\mu \\quad \\mbox{} \\quad \\mathbb{V}ar(y_t) = \\frac{\\sigma^2}{1-\\phi^2}.\n\\]Let us compute \\(j^{th}\\) autocovariance AR(1) process:\n\\[\\begin{eqnarray*}\n\\mathbb{E}([y_{t} - \\mu][y_{t-j} - \\mu]) &=& \\mathbb{E}([\\varepsilon_t + \\phi  \\varepsilon_{t-1}+ \\phi^2 \\varepsilon_{t-2} + \\dots + \\color{red}{\\phi^j \\varepsilon_{t-j}} + \\color{blue}{\\phi^{j+1} \\varepsilon_{t-j-1}} \\dots]\\times \\\\\n&&[\\color{red}{\\varepsilon_{t-j}} + \\color{blue}{\\phi \\varepsilon_{t-j-1}} + \\phi^2 \\varepsilon_{t-j-2} + \\dots + \\phi^k \\varepsilon_{t-j-k} + \\dots])\\\\\n&=& \\mathbb{E}(\\color{red}{\\phi^j \\varepsilon_{t-j}^2}+\\color{blue}{\\phi^{j+2} \\varepsilon_{t-j-1}^2}+\\phi^{j+4} \\varepsilon_{t-j-2}^2+\\dots)\\\\\n&=& \\frac{\\phi^j \\sigma^2}{1 - \\phi^2}.\n\\end{eqnarray*}\\]Therefore \\(\\rho_j = \\phi^j\\).precedes, :Proposition 2.4  (Covariance-stationarity AR(1) process) AR(1) process, defined Def. 2.13, covariance-stationary iff \\(|\\phi|<1\\).Definition 2.14  (AR(p) process) Consider white noise process \\(\\{\\varepsilon_t\\}_{t = -\\infty}^{+\\infty}\\) (see Def. 2.1). Process \\(y_t\\) \\(p^{th}\\)-order autoregressive process (AR(p)) dynamics defined following difference equation (\\(\\phi_p \\ne 0\\)):\n\\[\\begin{equation}\ny_t = c + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\dots + \\phi_p y_{t-p} + \\varepsilon_t.\\tag{2.8}\n\\end{equation}\\]see, covariance-stationarity process \\(y_t\\) hinges matrix \\(F\\) defined :\n\\[\\begin{equation}\nF = \\left[\n\\begin{array}{ccccc}\n\\phi_1 & \\phi_2 & \\dots& & \\phi_p \\\\\n1 & 0 &\\dots && 0 \\\\\n0 & 1 &\\dots && 0 \\\\\n\\vdots &  & \\ddots && \\vdots \\\\\n0 & 0 &\\dots &1& 0 \\\\\n\\end{array}\n\\right].\\tag{2.9}\n\\end{equation}\\]Note matrix \\(F\\) \\(y_t\\) follows Eq. (2.8), process \\(\\mathbf{y}_t\\) follows:\n\\[\n\\mathbf{y}_t = \\mathbf{c} + F \\mathbf{y}_{t-1} + \\boldsymbol\\xi_t\n\\]\n\n\\[\n\\mathbf{c} =\n\\left[\\begin{array}{c}\nc\\\\\n0\\\\\n\\vdots\\\\\n0\n\\end{array}\\right],\n\\quad\n\\boldsymbol\\xi_t =\n\\left[\\begin{array}{c}\n\\varepsilon_t\\\\\n0\\\\\n\\vdots\\\\\n0\n\\end{array}\\right],\n\\quad\n\\mathbf{y}_t =\n\\left[\\begin{array}{c}\ny_t\\\\\ny_{t-1}\\\\\n\\vdots\\\\\ny_{t-p+1}\n\\end{array}\\right].\n\\]Proposition 2.5  (eigenvalues matrix F) eigenvalues \\(F\\) (defined Eq. (2.9)) solutions :\n\\[\\begin{equation}\n\\lambda^p - \\phi_1 \\lambda^{p-1} - \\dots - \\phi_{p-1}\\lambda - \\phi_p = 0.\\tag{2.10}\n\\end{equation}\\]Proposition 2.6  (Covariance-stationarity AR(p) process) four statements equivalent:Process \\(\\{y_t\\}\\), defined Def. 2.14, covariance-stationary.eigenvalues \\(F\\) (defined Eq. (2.9)) lie strictly within unit circle.roots Eq. (2.11) () lie strictly outside unit circle.\n\\[\\begin{equation}\n1 - \\phi_1 z - \\dots - \\phi_{p-1}z^{p-1} - \\phi_p z^p = 0.\\tag{2.11}\n\\end{equation}\\]roots Eq. (2.12) () lie strictly inside unit circle.\n\\[\\begin{equation}\n\\lambda^p - \\phi_1 \\lambda^{p-1} - \\dots - \\phi_{p-1}\\lambda - \\phi_p = 0.\\tag{2.12}\n\\end{equation}\\]Proof. consider case eigenvalues \\(F\\) distinct; Jordan decomposition can used general case. eigenvalues \\(F\\) distinct, \\(F\\) admits following spectral decomposition: \\(F = PDP^{-1}\\), \\(D\\) diagonal. Using notations introduced Eq. (2.9), :\n\\[\n\\mathbf{y}_{t} = \\mathbf{c} + F \\mathbf{y}_{t-1} + \\boldsymbol\\xi_{t}.\n\\]\nLet’s introduce \\(\\mathbf{d} = P^{-1}\\mathbf{c}\\), \\(\\mathbf{z}_t = P^{-1}\\mathbf{y}_t\\) \\(\\boldsymbol\\eta_t = P^{-1}\\boldsymbol\\xi_t\\). :\n\\[\n\\mathbf{z}_{t} = \\mathbf{d} + D \\mathbf{z}_{t-1} + \\boldsymbol\\eta_{t}.\n\\]\n\\(D\\) diagonal, different component \\(\\mathbf{z}_t\\), denoted \\(z_{,t}\\), follow AR(1) processes. (scalar) autoregressive parameters AR(1) processes diagonal entries \\(D\\) –also eigenvalues \\(F\\)– denote \\(\\lambda_i\\).Process \\(y_t\\) covariance-stationary iff \\(\\mathbf{y}_{t}\\) also covariance-stationary, case iff \\(z_{,t}\\), \\(\\[1,p]\\), covariance-stationary. Prop. 2.4, process \\(z_{,t}\\) covariance-stationary iff \\(|\\lambda_i|<1\\). proves () equivalent (ii). Prop. 2.5 proves (ii) equivalent (iv). Finally, easily seen (iii) equivalent (iv) (long \\(\\phi_p \\ne 0\\)).Using lag operator (see Eq (2.1)), \\(y_t\\) covariance-stationary AR(p) process (Def. 2.14), can write:\n\\[\ny_t = \\mu + \\psi(L)\\varepsilon_t,\n\\]\n\n\\[\\begin{equation}\n\\psi(L) = (1 - \\phi_1 L - \\dots - \\phi_p L^p)^{-1},\n\\end{equation}\\]\n\n\\[\\begin{equation}\n\\mu = \\mathbb{E}(y_t) = \\dfrac{c}{1-\\phi_1 -\\dots - \\phi_p}.\\tag{2.13}\n\\end{equation}\\]following lines codes, compute eigenvalues \\(F\\) matrices associated following processes (\\(\\varepsilon_t\\) white noise):\n\\[\\begin{eqnarray*}\nx_t &=& 0.9 x_{t-1} -0.2 x_{t-2} + \\varepsilon_t\\\\\ny_t &=& 1.1 y_{t-1} -0.3 y_{t-2} + \\varepsilon_t\\\\\nw_t &=& 1.4 w_{t-1} -0.7 w_{t-2} + \\varepsilon_t\\\\\nz_t &=& 0.9 z_{t-1} +0.2 z_{t-2} + \\varepsilon_t\n\\end{eqnarray*}\\]absolute values eigenvalues associated process \\(w_t\\) equal 0.837. Therefore, according Proposition 2.6, processes \\(x_t\\), \\(y_t\\), \\(w_t\\) covariance-stationary, \\(z_t\\) (absolute value one eigenvalues \\(F\\) matrix associated process larger 1).computation autocovariances \\(y_t\\) based -called Yule-Walker equations (Eq. (2.14)). Let’s rewrite Eq. (2.8):\n\\[\n(y_t-\\mu) = \\phi_1 (y_{t-1}-\\mu) + \\phi_2 (y_{t-2}-\\mu) + \\dots + \\phi_p (y_{t-p}-\\mu) + \\varepsilon_t.\n\\]\nMultiplying sides \\(y_{t-j}-\\mu\\) taking expectations leads (Yule-Walker) equations:\n\\[\\begin{equation}\n\\gamma_j = \\left\\{\n\\begin{array}{l}\n\\phi_1 \\gamma_{j-1}+\\phi_2 \\gamma_{j-2}+ \\dots + \\phi_p \\gamma_{j-p} \\quad \\quad j>0\\\\\n\\phi_1 \\gamma_{1}+\\phi_2 \\gamma_{2}+ \\dots + \\phi_p \\gamma_{p} + \\sigma^2 \\quad \\quad j=0.\n\\end{array}\n\\right.\\tag{2.14}\n\\end{equation}\\]\nUsing \\(\\gamma_j = \\gamma_{-j}\\) (Prop. 2.1), one can express \\((\\gamma_0,\\gamma_1,\\dots,\\gamma_{p})\\) functions \\((\\sigma^2,\\phi_1,\\dots,\\phi_p)\\).","code":"\nF <- matrix(c(.9,1,-.2,0),2,2)\nlambda_x <- eigen(F)$values\nF[1,] <- c(1.1,-.3)\nlambda_y <- eigen(F)$values\nF[1,] <- c(1.4,-.7)\nlambda_w <- eigen(F)$values\nF[1,] <- c(.9,.2)\nlambda_z <- eigen(F)$values\nrbind(lambda_x,lambda_y,lambda_w,lambda_z)##                         [,1]                  [,2]\n## lambda_x 0.500000+0.0000000i  0.4000000+0.0000000i\n## lambda_y 0.600000+0.0000000i  0.5000000+0.0000000i\n## lambda_w 0.700000+0.4582576i  0.7000000-0.4582576i\n## lambda_z 1.084429+0.0000000i -0.1844289+0.0000000i"},{"path":"TS.html","id":"ar-ma-processes","chapter":"2 Time Series","heading":"2.2.3 AR-MA processes","text":"Definition 2.15  (ARMA(p,q) process) \\(\\{y_t\\}\\) ARMA(\\(p\\),\\(q\\)) process dynamics described following equation:\n\\[\\begin{equation}\ny_t = c + \\underbrace{\\phi_1 y_{t-1} + \\dots + \\phi_p y_{t-p}}_{\\mbox{AR part}} + \\underbrace{\\varepsilon_t + \\theta_1 \\varepsilon_{t-1} + \\dots + \\theta_q \\varepsilon_{t-q}}_{\\mbox{MA part}},\\tag{2.15}\n\\end{equation}\\]\n\\(\\{\\varepsilon_t\\}_{t \\] -\\infty,+\\infty[}\\) white noise process (see Def. 2.1).Proposition 2.7  (Stationarity ARMA(p,q) process) ARMA(\\(p\\),\\(q\\)) process defined 2.15 covariance stationary iff roots \n\\[\n1 - \\phi_1 z - \\dots - \\phi_p z^p=0\n\\]\nlie strictly outside unit circle , equivalently, iff \n\\[\n\\lambda^p - \\phi_1 \\lambda^{p-1} - \\dots - \\phi_p=0\n\\]\nlie strictly within unit circle.Proof. proof Prop. 2.6 can adapted present case.can write:\n\\[\n(1 - \\phi_1 L - \\dots - \\phi_p L^p)y_t = c + (1 + \\theta_1 L + \\dots + \\theta_q L^q)\\varepsilon_t.\n\\]roots \\(1 - \\phi_1 z - \\dots - \\phi_p z^p=0\\) lie outside unit circle, :\n\\[\\begin{equation}\ny_t = \\mu + \\psi(L)\\varepsilon_t,\\tag{2.16}\n\\end{equation}\\]\n\n\\[\n\\psi(L) = \\frac{1 + \\theta_1 L + \\dots + \\theta_q L^q}{1 - \\phi_1 L - \\dots - \\phi_p L^p} \\quad \\quad \\mu = \\dfrac{c}{1-\\phi_1 -\\dots - \\phi_p}.\n\\]Eq. (2.16) Wold representation ARMA process (see Theorem 2.3 ).stationarity process depends AR specification (eigenvalues matrix \\(F\\), exactly Prop. 2.6). process stationary, weights \\(\\psi(L)\\) decay geometric rate.","code":""},{"path":"TS.html","id":"PACFapproach","chapter":"2 Time Series","heading":"2.2.4 PACF approach to identify AR/MA processes","text":"seen \\(k^{th}\\)-order auto-correlation MA(q) process null \\(k>q\\). exploited, practice, determine order MA process. Moreover, since case AR process, can used distinguish AR MA process.exists equivalent approach determine whether process can modeled AR process; based partial auto-correlations:Definition 2.16  (Partial auto-correlation) time series context, partial auto-correlation (\\(\\phi_{h,h}\\)) process \\(\\{y_t\\}\\) defined partial correlation \\(y_{t+h}\\) \\(y_t\\) given \\(y_{t+h-1},\\dots,y_{t+1}\\). (see Def. 3.5 definition partial correlation.)\\(h>p\\), regression \\(y_{t+h}\\) \\(y_{t+h-1},\\dots,y_{t+1}\\) :\n\\[\ny_{t+h} = c + \\phi_1 y_{t+h-1}+\\dots+ \\phi_p  y_{t+h-p} + \\varepsilon_{t+h}.\n\\]\nresiduals latter regressions (\\(\\varepsilon_{t+h}\\)) uncorrelated \\(y_t\\). partial autocorrelation zero \\(h>p\\).Besides, can shown \\(\\phi_{p,p}=\\phi_p\\). Hence \\(\\phi_{p,p}=\\phi_p\\) \\(\\phi_{h,h}=0\\) \\(h>p\\). can used determine order AR process. contrast (importantly) \\(y_t\\) follows MA(q) process, \\(\\phi_{k,k}\\) asymptotically approaches zero instead cutting abruptly.illustrated , functions acf pacf can conveniently used employ (P)ACF approach. (Note also use function sim.arma simulate ARMA processes.)\nFigure 2.7: ACF/PACF analysis two processes (MA process left, AR right).\n","code":"\nlibrary(AEC)\npar(mfrow=c(3,2))\npar(plt=c(.2,.9,.2,.95))\ntheta <- c(1,2,1);phi=0\ny.sim <- sim.arma(c=0,phi,theta,sigma=1,T=1000,y.0=0,nb.sim=1)\npar(mfg=c(1,1));plot(y.sim,type=\"l\",lwd=2)\npar(mfg=c(2,1));acf(y.sim)\npar(mfg=c(3,1));pacf(y.sim)\ntheta <- c(1);phi=0.9\ny.sim <- sim.arma(c=0,phi,theta,sigma=1,T=1000,y.0=0,nb.sim=1)\npar(mfg=c(1,2));plot(y.sim,type=\"l\",lwd=2)\npar(mfg=c(2,2));acf(y.sim)\npar(mfg=c(3,2));pacf(y.sim)"},{"path":"TS.html","id":"wold-decomposition","chapter":"2 Time Series","heading":"2.2.5 Wold decomposition","text":"Wold decomposition important result time series analysis:Theorem 2.3  (Wold decomposition) covariance-stationary process admits following representation:\n\\[\ny_t = \\mu + \\sum_{0}^{+\\infty} \\theta_i \\varepsilon_{t-} + \\kappa_t,\n\\]\n\\(\\theta_0 = 1\\), \\(\\sum_{=0}^{\\infty} \\theta_i^2 < +\\infty\\) (square summability, see Def. 2.12).\\(\\{\\varepsilon_t\\}\\) white noise (see Def. 2.1); \\(\\varepsilon_t\\) error made forecasting \\(y_t\\) based linear combination lagged \\(y_t\\)’s (\\(\\varepsilon_t = y_t - \\hat{\\mathbb{E}}[y_t|y_{t-1},y_{t-2},\\dots]\\)).\\(j \\ge 1\\), \\(\\kappa_t\\) correlated \\(\\varepsilon_{t-j}\\); \\(\\kappa_t\\) can perfectly forecasted based linear combination lagged \\(y_t\\)’s (.e. \\(\\kappa_t = \\hat{\\mathbb{E}}(\\kappa_t|y_{t-1},y_{t-2},\\dots)\\)). \\(\\kappa_t\\) called deterministic component \\(y_t\\).Proof. See Anderson (1971). Partial proof L. Christiano.ARMA process, Wold representation given Eq. (2.16). detailed Prop. 2.8, can computed recursively replacing lagged \\(y_t\\)’s Eq. (2.15). case, deterministic component (\\(\\kappa\\)) null.","code":""},{"path":"TS.html","id":"impulse-response-functions-irfs-in-arma-models","chapter":"2 Time Series","heading":"2.2.6 Impulse Response Functions (IRFs) in ARMA models","text":"Consider ARMA(p,q) process defined Def. 2.15, whose associated sequence white noise \\(\\{\\varepsilon_t\\}\\). Let us construct novel (counterfactual) sequence shocks \\(\\{\\tilde\\varepsilon_t^{(s)}\\}\\):\n\\[\n\\tilde\\varepsilon_t^{(s)} = \\left\\{\n\\begin{array}{lcc}\n\\varepsilon_{t} & & t \\ne s,\\\\\n\\varepsilon_{t} + \\delta && t=s.\n\\end{array}\n\\right.\n\\]\ndenote \\(\\{\\tilde{y}_t^{(s)}\\}\\) process following Eq. (2.15) \\(\\{\\varepsilon_t\\}\\) replaced \\(\\{\\tilde\\varepsilon_t^{(s)}\\}\\). time series \\(\\{\\tilde{y}_t^{(s)}\\}\\) counterfactual series \\(\\{y_t\\}\\) prevailed \\(\\varepsilon_t\\) shifted \\(\\delta\\) date \\(s\\) (change).relationship \\(\\{y_t\\}\\) \\(\\{\\tilde{y}_t^{(s)}\\}\\) defines dynamics multiplier. ltter denoted \\(\\frac{\\partial y_t}{\\partial \\varepsilon_{s}}\\) :\n\\[\n\\tilde{y}_t^{(s)} = y_t + \\frac{\\partial y_t}{\\partial \\varepsilon_{s}}\\delta.\n\\]\nsee dynamic multipliers closely related infinite MA representation (Wold decomposition, Theorem 2.3) \\(y_t\\):\n\\[\ny_t = \\mu + \\sum_{=0}^{+\\infty} \\psi_i \\varepsilon_{t-}.\n\\]\n\\(t<s\\), \\(y_t = \\tilde{y}_t^{(s)}\\) (\\(\\tilde{\\varepsilon}_{t-}= \\varepsilon_{t-}\\) \\(\\ge 0\\) \\(t<s\\)).\\(t \\ge s\\):\n\\[\n\\tilde{y}_t^{(s)} = \\mu + \\left( \\sum_{=0}^{t-s-1} \\psi_i \\varepsilon_{t-} \\right) + \\psi_{t-s}(\\varepsilon_{s}+\\delta) + \\left( \\sum_{=t-s+1}^{+\\infty} \\psi_i \\varepsilon_{t-} \\right)=y_t + \\frac{\\partial y_t}{\\partial \\varepsilon_{s}}\\delta.\n\\]\nTherefore, \\(t \\ge s\\), :\n\\[\n\\boxed{\\dfrac{\\partial y_t}{\\partial \\varepsilon_{s}}=\\psi_{t-s}.}\n\\]\n, \\(\\{y_t\\}\\)’s dynamic multiplier order \\(k\\) object \\(k^{th}\\) loading \\(\\psi_k\\) Wold decomposition \\(\\{y_t\\}\\). sequence \\(\\left\\{\\dfrac{\\partial y_{t+h}}{\\partial \\varepsilon_{t}}\\right\\}_{h \\ge 0} \\equiv \\left\\{\\psi_h\\right\\}_{h \\ge 0}\\) defines impulse response function (IRF) \\(y_t\\) shock \\(\\varepsilon_t\\).ARMA processes, computation IRFs easy:Proposition 2.8  (IRF ARMA(p,q) process) coefficients \\(\\psi_h\\), define IRF process \\(y_t\\) \\(\\varepsilon_t\\), can computed recursively follows:Set \\(\\psi_{-1}=\\dots=\\psi_{-p}=0\\).\\(h \\ge 0\\), (recursively) apply:\n\\[\n\\psi_h = \\phi_1 \\psi_{h-1} + \\dots + \\phi_p \\psi_{h-p} + \\theta_h,\n\\]\n\\(\\theta_h = 0\\) \\(h>q\\).Proof. obtained applying operator \\(\\frac{\\partial}{\\partial \\varepsilon_{t}}\\) sides Eq. (2.15):\n\\[\ny_{t+h} = c + \\phi_1 y_{t+h-1} + \\dots + \\phi_p y_{t+h-p} + \\varepsilon_{t+h} + \\theta_1 \\varepsilon_{t+h-1} + \\dots + \\theta_q \\varepsilon_{t+h-q}.\n\\]Note Proposition 2.8 constitutes simple way compute MA(\\(\\infty\\)) representation (Wold representation) ARMA process.One can use function sim.arma package AEC compute ARMA’s IRFs (argument make.IRF = 1):\nFigure 2.8: IRFs associated three processes. Process 1 (MA(2)): \\(y_t = \\varepsilon_t + \\varepsilon_{t-1} + \\varepsilon_{t-2}\\). Process 2 (ARMA(1,1)): \\(y_{t}=0.6y_{t-1} + \\varepsilon_t + 0.5\\varepsilon_{t-1}\\). Process 3 (ARMA(4,2)): \\(y_{t}=0.5y_{t-3} + 0.4y_{t-4} + \\varepsilon_t + \\varepsilon_{t-1} + \\varepsilon_{t-2}\\).\nConsider annual Swiss GDP growth JST macro-history database. Let us first determine relevant orders AR MA processes using (P)ACF approach.\nFigure 2.9: (P)ACF analysis Swiss GDP growth.\ntwo bottom plots Figure 2.9 suggest either MA(2) AR(1) used model GDP growth rate series. Figure 2.10 shows IRFs based two respective specifications.\nFigure 2.10: Dynamic response Swiss annual growth shock innovation \\(\\varepsilon_t\\) date \\(t=0\\). solid line corresponds AR(1) specification; dashed line corresponds MA(2) specification.\nkind algorithm can used compute impact increase exogenous variable \\(x_t\\) within ARMAX(p,q,r) model (see next section).","code":"\nT <- 21 # number of periods for IRF\ntheta <- c(1,1,1);phi <- c(0);c <- 0\ny.sim <- sim.arma(c,phi,theta,sigma=1,T,y.0=rep(0,length(phi)),\n                  nb.sim=1,make.IRF = 1)\npar(mfrow=c(1,3));par(plt=c(.25,.95,.2,.85))\nplot(0:(T-1),y.sim[,1],type=\"l\",lwd=2,\n     main=\"(a) Process 1\",xlab=\"Time after shock on epsilon\",\n     ylab=\"Dynamic multiplier (shock on epsilon at t=0)\",col=\"red\")\nabline(h=0)\ntheta <- c(1,.5);phi <- c(0.6)\ny.sim <- sim.arma(c,phi,theta,sigma=1,T,y.0=rep(0,length(phi)),\n                  nb.sim=1,make.IRF = 1)\nplot(0:(T-1),y.sim[,1],type=\"l\",lwd=2,\n     main=\"(b) Process 2\",xlab=\"Time after shock on epsilon\",\n     ylab=\"\",col=\"red\")\ntheta <- c(1,1,1);phi <- c(0,0,.5,.4)\ny.sim <- sim.arma(c,phi,theta,sigma=1,T,y.0=rep(0,length(phi)),\n                  nb.sim=1,make.IRF = 1)\nplot(0:(T-1),y.sim[,1],type=\"l\",lwd=2,\n     main=\"(c) Process 3\",xlab=\"Time after shock on epsilon\",\n     ylab=\"\",col=\"red\")\nlibrary(AEC)\ndata(JST)\ndata <- subset(JST,iso==\"CHE\")\npar(plt=c(.1,.95,.1,.95))\nT <- dim(data)[1]\ngrowth <- log(data$gdp[2:T]/data$gdp[1:(T-1)])\npar(mfrow=c(3,1))\npar(plt=c(.1,.95,.15,.95))\nplot(data$year[2:T],growth,type=\"l\",xlab=\"\",ylab=\"\",lwd=2)\nabline(h=0,lty=2)\nacf(growth)\npacf(growth)\n# Fit an AR process:\nres <- arima(growth,order=c(1,0,0))\nphi <- res$coef[1]\nT <- 11\ny.sim <- sim.arma(c=0,phi,theta=1,sigma=1,T,y.0=rep(0,length(phi)),\n                  nb.sim=1,make.IRF = 1)\npar(plt=c(.15,.95,.25,.95))\nplot(0:(T-1),y.sim[,1],type=\"l\",lwd=3,\n     xlab=\"Time after shock on epsilon\",\n     ylab=\"Dynamic multiplier (shock on epsilon at t=0)\",col=\"red\")\n# Fit a MA process:\nres <- arima(growth,order=c(0,0,2))\nphi <- 0;theta <- c(1,res$coef[1:2])\ny.sim <- sim.arma(c=0,phi,theta,sigma=1,T,y.0=rep(0,length(phi)),\n                  nb.sim=1,make.IRF = 1)\nlines(0:(T-1),y.sim[,1],lwd=3,col=\"red\",lty=2)\nabline(h=0)"},{"path":"TS.html","id":"ARMAIRF","chapter":"2 Time Series","heading":"2.2.7 ARMA processes with exogenous variables (ARMA-X)","text":"ARMA processes allow investigate influence exogenous variable (say \\(x_t\\)) variable interest (say \\(y_t\\)). \\(x_t\\) \\(y_t\\) reciprocal influences, Vector Autoregressive (VAR) model may used (tools studied later, Section 2.3). However, one suspects \\(x_t\\) “exogenous” influence \\(y_t\\), simple extension ARMA processes may considered. Loosely speaking, \\(x_t\\) “exogenous” influence \\(y_t\\) \\(y_t\\) affect \\(x_t\\). extension called ARMAX(p,q,r).begin , let us formalize notion exogeneity. Consider white noise sequence \\(\\{\\varepsilon_t\\}\\) (Def. 2.1).Definition 2.17  (Exogeneity) say \\(x_t\\) (strictly) exogenous \\(\\{\\varepsilon_t\\}\\) \n\\[\n\\mathbb{E}(\\varepsilon_t|\\underbrace{\\dots,x_{t+1}}_{\\mbox{future}},\\underbrace{x_t,x_{t-1},\\dots}_{\\mbox{present past}}) = 0.\n\\]Hence, \\(\\{x_t\\}\\) strictly exogenous \\(\\varepsilon_t\\), past, present future values \\(x_t\\) allow predict \\(\\varepsilon_t\\)’s.following, assume \\(\\{x_t\\}\\) covariance stationary process.Definition 2.18  (ARMAX(p,q,r) model) process \\(\\{y_t\\}\\) ARMAX(p,q,r) follows difference equation:\n\\[\\begin{eqnarray}\ny_t &=& \\underbrace{c + \\phi_1 y_{t-1} + \\dots + \\phi_p y_{t-p}}_{\\mbox{AR(p) part}} + \\underbrace{\\beta_0 x_t + \\dots + \\beta_{r} x_{t-r}}_{\\mbox{X(r) part}} + \\nonumber \\\\\n&&\\underbrace{\\varepsilon_t + \\theta_1\\varepsilon_{t-1}+\\dots +\\theta_{q}\\varepsilon_{t-q}.}_{\\mbox{MA(q) part}} \\tag{2.17}\n\\end{eqnarray}\\]\n\\(\\{\\varepsilon_t\\}\\) ..d. white noise sequence \\(\\{x_t\\}\\) exogenous \\(y_t\\).effect one-unit increase \\(x_t\\) \\(y_t\\)? address question, notion “effect” formalized. Let us introduce two related sequences values \\(\\{x\\}\\). Denote first \\(\\{\\}\\) second \\(\\{\\tilde{}^t\\}\\). , posit \\(a_s = \\tilde{}_s^t\\) \\(s \\ne t\\), \\(\\tilde{}_t^t = a_t+1\\).notations, define \\(\\frac{\\partial y_{t+h}}{\\partial x_t}\\) follows:\n\\[\\begin{equation}\n\\frac{\\partial y_{t+h}}{\\partial x_t} := \\mathbb{E}(y_{t+h}|\\{x\\} = \\{\\tilde{}^t\\}) - \\mathbb{E}(y_{t+h}|\\{x\\} = \\{\\}).\\tag{2.18}\n\\end{equation}\\]\nexogeneity assumption, easily seen \n\n\n\n\n\\[\n\\frac{\\partial y_t}{\\partial x_t} = \\beta_0.\n\\]\nNow, since\n\\[\\begin{eqnarray*}\ny_{t+1} &=& c + \\phi_1 y_{t} + \\dots + \\phi_p y_{t+1-p} + \\beta_0 x_{t+1} + \\dots + \\beta_{r} x_{t+1-r} +\\\\\n&&\\varepsilon_{t+1} + \\theta_1\\varepsilon_{t}+\\dots +\\theta_{q}\\varepsilon_{t+1-q},\n\\end{eqnarray*}\\]\nusing exogeneity assumption, obtain:\n\\[\n\\frac{\\partial y_{t+1}}{\\partial x_t} := \\phi_1 \\frac{\\partial y_{t}}{\\partial x_t} + \\beta_1 = \\phi_1\\beta_0 + \\beta_1.\n\\]\ncan applied recursively give \\(\\dfrac{\\partial y_{t+h}}{\\partial x_t}\\) \\(h \\ge 0\\):Proposition 2.9  (Dynamic multipliers ARMAX models) One can recursively compute dynamic multipliers \\(\\frac{\\partial y_{t+h}}{\\partial x_t}\\) follows:Initialization: \\(\\dfrac{\\partial y_{t+h}}{\\partial x_t}=0\\) \\(h<0\\).\\(h \\ge 0\\) assuming first \\(h-1\\) multipliers computed, :\n\\[\\begin{eqnarray}\n\\dfrac{\\partial y_{t+h}}{\\partial x_t} &=& \\phi_1 \\dfrac{\\partial y_{t+h-1}}{\\partial x_t} + \\dots + \\phi_p \\dfrac{\\partial y_{t+h-p}}{\\partial x_t} + \\beta_h,\\tag{2.18}\n\\end{eqnarray}\\]\nuse notation \\(\\beta_h=0\\) \\(h>r\\).Remark resulting dynamic multipliers obtained ARMA(p,r) model \\(\\theta_i\\)’s replaced \\(\\beta_i\\)’s (see Proposition 2.8 Section 2.2.7).stressed definition dynamic multipliers (Eq. (2.18)) reflect potential persistency shock occuring date \\(t\\) process \\(\\{x\\}\\) . Going direction necessitate model joint dynamics \\(x_t\\) (instance using VAR model , see Section 2.3).Example 2.3  (Influence number freezing days price orange juice) example based data used J. Stock Watson (2003) (Chapter 16). objective study influence number freezing days price orange juice. Let us first estimate ARMAX(0,0,12) model:Let us now use function estim.armax, package AECto estimate ARMA-X(2,0,1) model:Figure 2.11 shows IRF associated two models.\nFigure 2.11: Response changes orange juice price (percent) number freezing days. solid (respectively dashed) line corresponds ARMAX(0,0,12) (resp. ARMAX(3,0,1)) model. first model estimated OLS (see ), second MLE.\nExample 2.4  (Real effect monetary policy shock) example, make use monetary shocks identified high-frequency data (see Gertler Karadi (2015)). dataset comes Valerie Ramey’s website (see Ramey (2016)).\nFigure 2.12: blue line corresponds monetary-policy shocks identified means Gertler Karadi (2015)’s approach (high-frequency change Euro-dollar futures). black slid line year--year growth rate industrial production.\nFigure 2.13 displays resulting IRF, 95% confidence band. code used produce confidence bands (.e., compute standard deviation dynamic multipliers different horizons) based Delta method (see Eq. (??)). codes available Appendix 3.6.2.\nFigure 2.13: Response industrial-production growth monetary-policy shocks. Dashed lines correpsond \\(\\pm\\) 2-standard-deviation bands.\n","code":"\nlibrary(AEC)\nlibrary(AER)\ndata(\"FrozenJuice\")\nFJ <- as.data.frame(FrozenJuice)\ndate <- time(FrozenJuice)\nprice <- FJ$price/FJ$ppi\nT <- length(price)\nk <- 1\ndprice <- 100*log(price[(k+1):T]/price[1:(T-k)])\nfdd <- FJ$fdd[(k+1):T]\npar(mfrow=c(3,1))\npar(plt=c(.1,.95,.15,.75))\nplot(date,price,type=\"l\",xlab=\"\",ylab=\"\",\n     main=\"(a) Price of orange Juice\")\nplot(date,c(NaN,dprice),type=\"l\",xlab=\"\",ylab=\"\",\n     main=\"(b) Monthly pct Change (y)\")\nplot(date,FJ$fdd,type=\"l\",xlab=\"\",ylab=\"\",\n     main=\"(c) Number of freezing days (x)\")\nnb.lags <- 12\nFDD <- FJ$fdd[(nb.lags+1):T]\nnames.FDD <- NULL\nfor(i in 1:nb.lags){\n  FDD <- cbind(FDD,FJ$fdd[(nb.lags+1-i):(T-i)])\n  names.FDD <- c(names.FDD,paste(\" Lag \",toString(i),sep=\"\"))}\ncolnames(FDD) <- c(\" Lag 0\",names.FDD)\ndprice <- dprice[(length(dprice)-dim(FDD)[1]+1):length(dprice)]\neq <- lm(dprice~FDD)\n# Compute the Newey-West std errors:\nvar.cov.mat <- NeweyWest(eq,lag = 7, prewhite = FALSE)\nrobust_se <- sqrt(diag(var.cov.mat))\n# Stargazer output (with and without Robust SE)\nstargazer::stargazer(eq, eq, type = \"text\",\n                     column.labels=c(\"(no HAC)\",\"(HAC)\"),keep.stat=\"n\",\n                     se = list(NULL,robust_se),no.space = TRUE)## \n## =========================================\n##                  Dependent variable:     \n##              ----------------------------\n##                         dprice           \n##                 (no HAC)        (HAC)    \n##                   (1)            (2)     \n## -----------------------------------------\n## FDD Lag 0       0.496***      0.496***   \n##                 (0.058)        (0.139)   \n## FDD Lag 1       0.150***       0.150*    \n##                 (0.058)        (0.087)   \n## FDD Lag 2        0.046          0.046    \n##                 (0.057)        (0.056)   \n## FDD Lag 3        0.062          0.062    \n##                 (0.057)        (0.046)   \n## FDD Lag 4        0.024          0.024    \n##                 (0.057)        (0.030)   \n## FDD Lag 5        0.036          0.036    \n##                 (0.057)        (0.030)   \n## FDD Lag 6        0.037          0.037    \n##                 (0.057)        (0.046)   \n## FDD Lag 7        0.019          0.019    \n##                 (0.057)        (0.015)   \n## FDD Lag 8        -0.038        -0.038    \n##                 (0.057)        (0.034)   \n## FDD Lag 9        -0.006        -0.006    \n##                 (0.057)        (0.050)   \n## FDD Lag 10      -0.112*        -0.112    \n##                 (0.057)        (0.069)   \n## FDD Lag 11       -0.063        -0.063    \n##                 (0.058)        (0.052)   \n## FDD Lag 12      -0.140**       -0.140*   \n##                 (0.058)        (0.078)   \n## Constant        -0.426*        -0.426*   \n##                 (0.238)        (0.243)   \n## -----------------------------------------\n## Observations      600            600     \n## =========================================\n## Note:         *p<0.1; **p<0.05; ***p<0.01\nnb.lags <- 1\nFDD <- FJ$fdd[(nb.lags+1):T]\nnames.FDD <- NULL\nfor(i in 1:nb.lags){\n  FDD <- cbind(FDD,FJ$fdd[(nb.lags+1-i):(T-i)])\n  names.FDD <- c(names.FDD,paste(\" Lag \",toString(i),sep=\"\"))}\ncolnames(FDD) <- c(\" Lag 0\",names.FDD)\ndprice <- 100*log(price[(k+1):T]/price[1:(T-k)])\ndprice <- dprice[(length(dprice)-dim(FDD)[1]+1):length(dprice)]\nres.armax <- estim.armax(Y = dprice,p=3,q=0,X=FDD)## [1] \"==================================================\"\n## [1] \"  ESTIMATING\"\n## [1] \"==================================================\"\n## [1] \"  END OF ESTIMATION\"\n## [1] \"==================================================\"\n## [1] \"\"\n## [1] \"  RESULTS:\"\n## [1] \"  -----------------------\"\n##                 THETA     st.dev   t.ratio\n## c         -0.46556249 0.19554352 -2.380864\n## phi   t-1  0.09788977 0.04025907  2.431496\n## phi   t-2  0.05049849 0.03827488  1.319364\n## phi   t-3  0.07155170 0.03764750  1.900570\n## sigma      4.64917949 0.13300769 34.954215\n## beta  t-0  0.47015552 0.05665344  8.298800\n## beta  t-1  0.10015862 0.05972526  1.676989\n## [1] \"==================================================\"\nnb.periods <- 20\nIRF1 <- sim.arma(c=0,phi=c(0),theta=eq$coefficients[2:13],sigma=1,\n                 T=nb.periods,y.0=c(0),nb.sim=1,make.IRF=1)\nIRF2 <- sim.arma(c=0,phi=res.armax$phi,theta=res.armax$beta,sigma=1,\n                 T=nb.periods,y.0=rep(0,length(res.armax$phi)),\n                 nb.sim=1,make.IRF=1)\npar(plt=c(.15,.95,.2,.95))\nplot(IRF1,type=\"l\",lwd=2,col=\"red\",xlab=\"months after shock\",\n     ylab=\"Chge in price (percent)\")\nlines(IRF2,lwd=2,col=\"red\",lty=2)\nabline(h=0,col=\"grey\")\nlibrary(AEC)\nT <- dim(Ramey)[1]\n# Construct growth series:\nRamey$growth <- Ramey$LIP - c(rep(NaN,12),Ramey$LIP[1:(length(Ramey$LIP)-12)])\n# Prepare matrix of exogenous variables:\nvec.lags <- c(9,12,18)\nMatrix.of.Exog <- NULL\nshocks <- Ramey$ED2_TC\nfor(i in 1:length(vec.lags)){Matrix.of.Exog <-\n  cbind(Matrix.of.Exog,c(rep(NaN,vec.lags[i]),shocks[1:(T-vec.lags[i])]))}\n# Look for dates where data are available:\nindic.good.dates <- complete.cases(Matrix.of.Exog)\n# Estimate ARMAX:\np <- 1; q <- 0\nx <- estim.armax(Ramey$growth[indic.good.dates],p,q,\n                 X=Matrix.of.Exog[indic.good.dates,])## [1] \"==================================================\"\n## [1] \"  ESTIMATING\"\n## [1] \"==================================================\"\n## [1] \"  END OF ESTIMATION\"\n## [1] \"==================================================\"\n## [1] \"\"\n## [1] \"  RESULTS:\"\n## [1] \"  -----------------------\"\n##                   THETA       st.dev    t.ratio\n## c         -0.0001716198 0.0005845907 -0.2935726\n## phi   t-1  0.9825608412 0.0120458531 81.5683897\n## sigma      0.0087948724 0.0003211748 27.3834438\n## beta  t-0 -0.0193570616 0.0087331529 -2.2165032\n## beta  t-1 -0.0225707935 0.0086750938 -2.6017925\n## beta  t-2 -0.0070131593 0.0086387440 -0.8118263\n## [1] \"==================================================\"\n# Compute IRF:\nirf <- sim.arma(0,x$phi,x$beta,x$sigma,T=60,y.0=rep(0,length(x$phi)),\n                nb.sim=1,make.IRF=1,X=NaN,beta=NaN)"},{"path":"TS.html","id":"estimARMA","chapter":"2 Time Series","heading":"2.2.8 Maximum Likelihood Estimation of ARMA processes","text":"Consider general case (time series); assume observe sample \\(\\mathbf{y}=[y_1,\\dots,y_T]'\\). order implement ML techniques (see Section ??), need evaluate joint p.d.f. (“likelihood”) \\(\\mathbf{y}\\), .e., \\(\\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})\\), \\(\\boldsymbol\\theta\\) vector parameters characterizes dynamics \\(y_t\\). Maximum Likelihood (ML) estimate \\(\\boldsymbol\\theta\\) given :\n\\[\n\\boxed{\\boldsymbol\\theta_{MLE} = \\arg \\max_{\\boldsymbol\\theta} \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})  = \\arg \\max_{\\boldsymbol\\theta} \\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y}).}\n\\]time series context, process \\(y_t\\) Markovian, exists useful way rewrite likelihood \\(\\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})\\). Let us first recall definition Markovian process (see also Def. ??):Definition 2.19  (Markovian process) Process \\(y_t\\) Markovian order one \\(f_{Y_t|Y_{t-1},Y_{t-2},\\dots} = f_{Y_t|Y_{t-1}}\\). generally, Markovian order \\(k\\) \\(f_{Y_t|Y_{t-1},Y_{t-2},\\dots} = f_{Y_t|Y_{t-1},\\dots,Y_{t-k}}\\).Now, remember Bayes’ formula:\n\\[\n\\mathbb{P}(X_2=x,X_1=y) = \\mathbb{P}(X_2=x|X_1=y)\\mathbb{P}(X_1=y).\n\\]\nUsing leads following decomposition likelihood function:\n\\[\\begin{eqnarray*}\nf_{Y_T,\\dots,Y_1}(y_T,\\dots,y_1;\\boldsymbol\\theta) &=&f_{Y_T|Y_{T-1},\\dots,Y_1}(y_T,\\dots,y_1;\\boldsymbol\\theta) \\times \\\\\n&& f_{Y_{T-1},\\dots,Y_1}(y_{T-1},\\dots,y_1;\\boldsymbol\\theta).\n\\end{eqnarray*}\\]\nUsing previous expression recursively, one obtains:\n\\[\\begin{equation}\nf_{Y_T,\\dots,Y_1}(y_T,\\dots,y_1;\\boldsymbol\\theta) = f_{Y_1}(y_1;\\boldsymbol\\theta) \\prod_{t=2}^{T} f_{Y_t|Y_{t-1},\\dots,Y_1}(y_t,\\dots,y_1;\\boldsymbol\\theta).\\tag{2.19}\n\\end{equation}\\]Let us start Gaussian AR(1) process (Markovian order one):\n\\[\ny_t = c + \\phi_1 y_{t-1} + \\varepsilon_t, \\quad \\varepsilon_t \\sim\\,..d.\\, \\mathcal{N}(0,\\sigma^2).\n\\]\n\\(t>1\\):\n\\[\nf_{Y_t|Y_{t-1},\\dots,Y_1}(y_t,\\dots,y_1;\\boldsymbol\\theta) = f_{Y_t|Y_{t-1}}(y_t,y_{t-1};\\boldsymbol\\theta)\n\\]\n\n\\[\nf_{Y_t|Y_{t-1}}(y_t,y_{t-1};\\boldsymbol\\theta) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{(y_t - c - \\phi_1 y_{t-1})^2}{2\\sigma^2}\\right).\n\\]expressions can plugged Eq. (2.19). \\(f_{Y_1}(y_1;\\boldsymbol\\theta)\\)? exist two possibilities:Case 1: use marginal distribution: \\(y_1 \\sim \\mathcal{N}\\left(\\dfrac{c}{1-\\phi_1},\\dfrac{\\sigma^2}{1-\\phi_1^2}\\right)\\).Case 2: \\(y_1\\) considered deterministic. way, means first observation “sacrificed”.Gaussian AR(1) process, :Case 1: (exact) log-likelihood :\n\\[\\begin{eqnarray}\n\\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})  &=& - \\frac{T}{2} \\log(2\\pi) - T\\log(\\sigma) + \\frac{1}{2}\\log(1-\\phi_1^2)\\nonumber \\\\\n&& - \\frac{(y_1 - c/(1-\\phi_1))^2}{2\\sigma^2/(1-\\phi_1^2)} - \\sum_{t=2}^T \\left[\\frac{(y_t - c - \\phi_1 y_{t-1})^2}{2\\sigma^2} \\right].\n\\end{eqnarray}\\]\nMaximum Likelihood Estimator \\(\\boldsymbol\\theta= [c,\\phi_1,\\sigma^2]\\) obtained numerical optimization.Case 1: (exact) log-likelihood :\n\\[\\begin{eqnarray}\n\\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})  &=& - \\frac{T}{2} \\log(2\\pi) - T\\log(\\sigma) + \\frac{1}{2}\\log(1-\\phi_1^2)\\nonumber \\\\\n&& - \\frac{(y_1 - c/(1-\\phi_1))^2}{2\\sigma^2/(1-\\phi_1^2)} - \\sum_{t=2}^T \\left[\\frac{(y_t - c - \\phi_1 y_{t-1})^2}{2\\sigma^2} \\right].\n\\end{eqnarray}\\]\nMaximum Likelihood Estimator \\(\\boldsymbol\\theta= [c,\\phi_1,\\sigma^2]\\) obtained numerical optimization.Case 2: (conditional) log-likelihood :\n\\[\\begin{eqnarray}\n\\log \\mathcal{L}^*(\\boldsymbol\\theta;\\mathbf{y})  &=& - \\frac{T-1}{2} \\log(2\\pi) - (T-1)\\log(\\sigma)\\nonumber\\\\\n&& - \\sum_{t=2}^T \\left[\\frac{(y_t - c - \\phi_1 y_{t-1})^2}{2\\sigma^2} \\right].\\tag{2.20}\n\\end{eqnarray}\\]Case 2: (conditional) log-likelihood :\n\\[\\begin{eqnarray}\n\\log \\mathcal{L}^*(\\boldsymbol\\theta;\\mathbf{y})  &=& - \\frac{T-1}{2} \\log(2\\pi) - (T-1)\\log(\\sigma)\\nonumber\\\\\n&& - \\sum_{t=2}^T \\left[\\frac{(y_t - c - \\phi_1 y_{t-1})^2}{2\\sigma^2} \\right].\\tag{2.20}\n\\end{eqnarray}\\]Exact MLE conditional MLE asymptotic (.e. large-sample) distribution. Indeed, process stationary, \\(f_{Y_1}(y_1;\\boldsymbol\\theta)\\) makes relatively negligible contribution \\(\\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})\\).conditional MLE substantial advantage: Gaussian case, conditional MLE simply obtained OLS. Indeed, let us introduce notations:\n\\[\nY = \\left[\\begin{array}{c}\ny_2\\\\\n\\vdots\\\\\ny_T\n\\end{array}\\right] \\quad \\quad\nX = \\left[\\begin{array}{cc}\n1 &y_1\\\\\n\\vdots&\\vdots\\\\\n1&y_{T-1}\n\\end{array}\\right].\n\\]\nEq. (2.20) rewrites:\n\\[\\begin{eqnarray}\n\\log \\mathcal{L}^*(\\boldsymbol\\theta;\\mathbf{y})  &=& - \\frac{T-1}{2} \\log(2\\pi) - (T-1)\\log(\\sigma) \\nonumber \\\\\n&& - \\frac{1}{2\\sigma^2} (Y-X[c,\\phi_1]')'(Y-X[c,\\phi_1]'),\n\\end{eqnarray}\\]\nmaximised :\n\\[\\begin{eqnarray}\n[\\hat{c},\\hat\\phi_1]' &=& (X'X)^{-1}X'Y \\tag{2.21} \\\\\n\\hat{\\sigma^2} &=& \\frac{1}{T-1} \\sum_{t=2}^T (y_t - \\hat{c} - \\hat{\\phi_1}y_{t-1})^2 \\nonumber \\\\\n&=& \\frac{1}{T-1} Y'(- X(X'X)^{-1}X')Y. \\tag{2.22}\n\\end{eqnarray}\\]Let us turn case AR(p) process. :\n\\[\\begin{eqnarray*}\n\\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y}) &=& \\log f_{Y_p,\\dots,Y_1}(y_p,\\dots,y_1;\\boldsymbol\\theta) +\\\\\n&& \\underbrace{\\sum_{t=p+1}^{T} \\log f_{Y_t|Y_{t-1},\\dots,Y_{t-p}}(y_t,\\dots,y_{t-p};\\boldsymbol\\theta)}_{\\log \\mathcal{L}^*(\\boldsymbol\\theta;\\mathbf{y})}.\n\\end{eqnarray*}\\]\n\\(f_{Y_p,\\dots,Y_{1}}(y_p,\\dots,y_{1};\\boldsymbol\\theta)\\) marginal distribution \\(\\mathbf{y}_{1:p} := [y_p,\\dots,y_1]'\\). marginal distribution \\(\\mathbf{y}_{1:p}\\) Gaussian; therefore fully characterised mean covariance matrix:\n\\[\\begin{eqnarray*}\n\\mathbb{E}(\\mathbf{y}_{1:p})&=&\\frac{c}{1-\\phi_1-\\dots-\\phi_p} \\mathbf{1}_{p\\times 1} \\\\\n\\mathbb{V}ar(\\mathbf{y}_{1:p}) &=& \\left[\\begin{array}{cccc}\n\\gamma_0 & \\gamma_1 & \\dots & \\gamma_{p-1} \\\\\n\\gamma_1 & \\gamma_0 & \\dots & \\gamma_{p-2} \\\\\n\\vdots &  & \\ddots & \\vdots \\\\\n\\gamma_{p-1} & \\gamma_{p-2} & \\dots & \\gamma_{0} \\\\\n\\end{array}\\right],\n\\end{eqnarray*}\\]\n\\(\\gamma_i\\)’s computed using Yule-Walker equations (Eq. (2.14)). Note depend, non-linear way, model parameters. Hence, maximization exact log-likelihood necessitates numerical oprimization procedures. contrast, maximization conditional log-likelihood \\(\\log \\mathcal{L}^*(\\boldsymbol\\theta;\\mathbf{y})\\) requires OLS, using Eqs. (2.21) (2.22), :\n\\[\nY = \\left[\\begin{array}{c}\ny_{p+1}\\\\\n\\vdots\\\\\ny_T\n\\end{array}\\right] \\quad \\quad\nX = \\left[\\begin{array}{cccc}\n1 & y_p & \\dots & y_1\\\\\n\\vdots&\\vdots&&\\vdots\\\\\n1&y_{T-1}&\\dots&y_{T-p}\n\\end{array}\\right].\n\\], stationary processes, conditional exact MLE asymptotic (large-sample) distribution. small samples, OLS formula however biased. Indeed, consider regression (\\(y_t\\) follows AR(p) process):\n\\[\\begin{equation}\ny_t = \\boldsymbol\\beta'\\mathbf{x}_t + \\varepsilon_t,\\tag{2.23}\n\\end{equation}\\]\n\\(\\mathbf{x}_t = [1,y_{t-1},\\dots,y_{t-p}]'\\) \\(\\boldsymbol\\beta = [c,\\phi_1,\\dots,\\phi_p]'\\).bias results fact \\(\\mathbf{x}_t\\) correlates \\(\\varepsilon_s\\)’s \\(s<t\\). sure:\n\\[\\begin{equation}\n\\mathbf{b} = \\boldsymbol{\\beta} + (X'X)^{-1}X'\\boldsymbol\\varepsilon,\\tag{2.24}\n\\end{equation}\\]\nspecific form \\(X\\), non-zero correlation \\(\\mathbf{x}_t\\) \\(\\varepsilon_s\\) \\(s<t\\), therefore \\(\\mathbb{E}[(X'X)^{-1}X'\\boldsymbol\\varepsilon] \\ne 0\\). , asymptotically, previous expectation goes zero, :Proposition 2.10  (Large-sample porperties OLS estimator AR(p) models) Assume \\(\\{y_t\\}\\) follows AR(p) process:\n\\[\ny_t = c + \\phi_1 y_{t-1} + \\dots + \\phi_p y_{t-p} + \\varepsilon_t\n\\]\n\\(\\{\\varepsilon_{t}\\}\\) ..d. white noise process. \\(\\mathbf{b}\\) OLS estimator \\(\\boldsymbol\\beta\\) (Eq. (2.23)), :\n\\[\n\\sqrt{T}(\\mathbf{b}-\\boldsymbol{\\beta}) =  \\underbrace{\\left[\\frac{1}{T}\\sum_{t=p}^T \\mathbf{x}_t\\mathbf{x}_t' \\right]^{-1}}_{\\overset{p}{\\rightarrow} \\mathbf{Q}^{-1}}\n\\underbrace{\\sqrt{T} \\left[\\frac{1}{T}\\sum_{t=1}^T \\mathbf{x}_t\\varepsilon_t \\right]}_{\\overset{d}{\\rightarrow} \\mathcal{N}(0,\\sigma^2\\mathbf{Q})},\n\\]\n\\(\\mathbf{Q} = \\mbox{plim }\\frac{1}{T}\\sum_{t=p}^T \\mathbf{x}_t\\mathbf{x}_t'= \\mbox{plim }\\frac{1}{T}\\sum_{t=1}^T \\mathbf{x}_t\\mathbf{x}_t'\\) given :\n\\[\\begin{equation}\n\\mathbf{Q} = \\left[\n\\begin{array}{ccccc}\n1 & \\mu &\\mu & \\dots & \\mu \\\\\n\\mu & \\gamma_0 + \\mu^2 & \\gamma_1 + \\mu^2 & \\dots & \\gamma_{p-1} + \\mu^2\\\\\n\\mu & \\gamma_1 + \\mu^2 & \\gamma_0 + \\mu^2 & \\dots & \\gamma_{p-2} + \\mu^2\\\\\n\\vdots &\\vdots &\\vdots &\\dots &\\vdots \\\\\n\\mu & \\gamma_{p-1} + \\mu^2 & \\gamma_{p-2} + \\mu^2 & \\dots & \\gamma_{0} + \\mu^2\n\\end{array}\n\\right].\\tag{2.25}\n\\end{equation}\\]Proof. Rearranging Eq. (2.24), :\n\\[\n\\sqrt{T}(\\mathbf{b}-\\boldsymbol{\\beta}) =  (X'X/T)^{-1}\\sqrt{T}(X'\\boldsymbol\\varepsilon/T).\n\\]\nLet us consider autocovariances \\(\\mathbf{v}_t = \\mathbf{x}_t \\varepsilon_t\\), denoted \\(\\gamma^v_j\\). Using fact \\(\\mathbf{x}_t\\) linear combination past \\(\\varepsilon_t\\)’s \\(\\varepsilon_t\\) white noise, get \\(\\mathbb{E}(\\varepsilon_t\\mathbf{x}_t)=0\\). Therefore\n\\[\n\\gamma^v_j = \\mathbb{E}(\\varepsilon_t\\varepsilon_{t-j}\\mathbf{x}_t\\mathbf{x}_{t-j}').\n\\]\n\\(j>0\\), \n\\[\\begin{eqnarray*}\n\\mathbb{E}(\\varepsilon_t\\varepsilon_{t-j}\\mathbf{x}_t\\mathbf{x}_{t-j}')=\\mathbb{E}(\\mathbb{E}[\\varepsilon_t\\varepsilon_{t-j}\\mathbf{x}_t\\mathbf{x}_{t-j}'|\\varepsilon_{t-j},\\mathbf{x}_t,\\mathbf{x}_{t-j}])\\\\\n&=&\\mathbb{E}(\\varepsilon_{t-j}\\mathbf{x}_t\\mathbf{x}_{t-j}'\\mathbb{E}[\\varepsilon_t|\\varepsilon_{t-j},\\mathbf{x}_t,\\mathbf{x}_{t-j}])=0.\n\\end{eqnarray*}\\]\nNote , \\(j>0\\), \\(\\mathbb{E}[\\varepsilon_t|\\varepsilon_{t-j},\\mathbf{x}_t,\\mathbf{x}_{t-j}]=0\\) \\(\\{\\varepsilon_t\\}\\) ..d. white noise sequence. \\(j=0\\), :\n\\[\n\\gamma^v_0 = \\mathbb{E}(\\varepsilon_t^2\\mathbf{x}_t\\mathbf{x}_{t}')= \\mathbb{E}(\\varepsilon_t^2) \\mathbb{E}(\\mathbf{x}_t\\mathbf{x}_{t}')=\\sigma^2\\mathbf{Q}.\n\\]\nconvergence distribution \\(\\sqrt{T}(X'\\boldsymbol\\varepsilon/T)=\\sqrt{T}\\frac{1}{T}\\sum_{t=1}^Tv_t\\) results Theorem 2.1 (applied \\(\\mathbf{v}_t=\\mathbf{x}_t\\varepsilon_t\\)), using \\(\\gamma_j^v\\) computed .two cases (exact conditional log-likelihoods) can implemented asking R fit AR process means function arima. Let us instance use output gap US3var dataset (US quarterly data, covering period 1959:2 2015:1, used Gouriéroux, Monfort, Renne (2017)).two sets estimated coefficients appear close .Let us now turn Moving-Average processes. Start MA(1):\n\\[\ny_t = \\mu + \\varepsilon_t + \\theta_1 \\varepsilon_{t-1},\\quad \\varepsilon_t \\sim ..d.\\mathcal{N}(0,\\sigma^2).\n\\]\n\\(\\varepsilon_t\\)’s easily computed recursively, starting \\(\\varepsilon_t = y_t - \\mu - \\theta_1 \\varepsilon_{t-1}\\). obtain:\n\\[\n\\varepsilon_t = y_t - \\theta_1 y_{t-1} + \\theta_1^2 y_{t-2}^2 + \\dots + (-1)^{t-1} \\theta_1^{t-1} y_{1} + (-1)^t\\theta_1^{t}\\varepsilon_{0}.\n\\]\nAssume one wants recover sequence \\(\\{\\varepsilon_t\\}\\)’s based observed values \\(y_t\\) (date 1 date \\(t\\)). One can use previous expression, value used \\(\\varepsilon_0\\)? one use true value \\(\\varepsilon_0\\) 0 (say), one obtain \\(\\varepsilon_t\\), estimate (\\(\\hat\\varepsilon_t\\), say), :\n\\[\n\\hat\\varepsilon_t = \\varepsilon_t - (-1)^t\\theta_1^{t}\\varepsilon_{0}.\n\\]\nClearly, \\(|\\theta_1|<1\\), error becomes small large \\(t\\). Formally, \\(|\\theta_1|<1\\), :\n\\[\n\\hat\\varepsilon_t \\overset{p}{\\rightarrow} \\varepsilon_t.\n\\]\nHence, \\(|\\theta_1|<1\\), consistent estimate conditional log-likelihood given :\n\\[\\begin{equation}\n\\log \\hat{\\mathcal{L}}^*(\\boldsymbol\\theta;\\mathbf{y}) = -\\frac{T}{2}\\log(2\\pi) - \\frac{T}{2}\\log(\\sigma^2) - \\sum_{t=1}^T \\frac{\\hat\\varepsilon_t^2}{2\\sigma^2}.\\tag{2.26}\n\\end{equation}\\]\nLoosely speaking, \\(|\\theta_1|<1\\) \\(T\\) sufficiently large:\n\\[\n\\mbox{approximate conditional MLE $\\approx$ exact MLE.}\n\\]Note \\(\\hat{\\mathcal{L}}^*(\\boldsymbol\\theta;\\mathbf{y})\\) complicated nonlinear function \\(\\mu\\) \\(\\theta\\). maximization therefore based numerical optimization procedures.Let us consider case Gaussian MA(\\(q\\)) process:\n\\[\\begin{equation}\ny_t = \\mu + \\varepsilon_t + \\theta_1 \\varepsilon_{t-1} + \\dots + \\theta_q \\varepsilon_{t-q} , \\quad \\varepsilon_t \\sim ..d.\\mathcal{N}(0,\\sigma^2). \\tag{2.27}\n\\end{equation}\\]Let us assume process invertible MA process. , assume roots :\n\\[\\begin{equation}\n\\lambda^q + \\theta_1 \\lambda^{q-1} + \\dots + \\theta_{q-1} \\lambda + \\theta_q = 0 \\tag{2.28}\n\\end{equation}\\]\nlie strictly inside unit circle. case, polynomial form \\(\\Theta(L)=1 + \\theta_1 L + \\dots + \\theta_q L^q\\) invertible Eq. (2.27) writes:\n\\[\n\\varepsilon_t = \\Theta(L)^{-1}(y_t - \\mu),\n\\]\nimplies , knew past values \\(y_t\\), also know \\(\\varepsilon_t\\). case, can consistently estimate \\(\\varepsilon_t\\)’s recursively computing \\(\\hat\\varepsilon_t\\)’s follows (\\(t>0\\)):\n\\[\\begin{equation}\n\\hat\\varepsilon_t = y_t - \\mu - \\theta_1 \\hat\\varepsilon_{t-1} - \\dots  - \\theta_q \\hat\\varepsilon_{t-q},\\tag{2.29}\n\\end{equation}\\]\n\n\\[\\begin{equation}\n\\hat\\varepsilon_{0}=\\dots=\\hat\\varepsilon_{-q+1}=0.\\tag{2.30}\n\\end{equation}\\]context, consistent estimate conditional log-likelihood still given Eq. (2.26), using Eqs. (2.29) (2.30) recursively compute \\(\\hat\\varepsilon_t\\)’s.Note determine exact likelihood MA process. Indeed, vector \\(\\mathbf{y} = [y_1,\\dots,y_T]'\\) Gaussian-distributed vector mean \\(\\boldsymbol\\mu = [\\mu,\\dots,\\mu]'\\) variance:\n\\[\n\\boldsymbol\\Omega = \\left[\\begin{array}{ccccccc}\n\\gamma_0 & \\gamma_1&\\dots&\\gamma_q&{\\color{red}0}&{\\color{red}\\dots}&{\\color{red}0}\\\\\n\\gamma_1 & \\gamma_0&\\gamma_1&&\\ddots&{\\color{red}\\ddots}&{\\color{red}\\vdots}\\\\\n\\vdots & \\gamma_1&\\ddots&\\ddots&&\\ddots&{\\color{red}0}\\\\\n\\gamma_q &&\\ddots&&&&\\gamma_q\\\\\n{\\color{red}0} &&&\\ddots&\\ddots&\\ddots&\\vdots\\\\\n{\\color{red}\\vdots}&{\\color{red}\\ddots}&\\ddots&&\\gamma_1&\\gamma_0&\\gamma_1\\\\\n{\\color{red}0}&{\\color{red}\\dots}&{\\color{red}0}&\\gamma_q&\\dots&\\gamma_1&\\gamma_0\n\\end{array}\\right],\n\\]\n\\(\\gamma_j\\)’s given Eq. (2.7). p.d.f. \\(\\mathbf{y}\\) given (see Prop. 3.18):\n\\[\n(2\\pi)^{-T/2}|\\boldsymbol\\Omega|^{-1/2}\\exp\\left( -\\frac{1}{2} (\\mathbf{y}-\\boldsymbol\\mu)' \\boldsymbol\\Omega^{-1} (\\mathbf{y}-\\boldsymbol\\mu)\\right).\n\\]\nlarge samples, computation likelihood however becomes numerically demanding.Finally, let us consider MLE ARMA(\\(p\\),\\(q\\)) processes:\n\\[\ny_t = c + \\phi_1 y_{t-1} + \\dots + \\phi_p y_{t-p} + \\varepsilon_t + \\theta_1 \\varepsilon_{t-1} +\n\\dots + \\theta_q \\varepsilon_{t-q} , \\; \\varepsilon_t \\sim ..d.\\,\\mathcal{N}(0,\\sigma^2).\n\\]\nMA part process invertible, log-likelihood function can consistently approximated conditional counterpart (form Eq. (2.26)), using consistent estimates \\(\\hat\\varepsilon_t\\) \\(\\varepsilon_t\\). \\(\\hat\\varepsilon_t\\)’s computed recursively :\n\\[\\begin{equation}\n\\hat\\varepsilon_t = y_t - c - \\phi_1 y_{t-1} - \\dots - \\phi_p y_{t-p} - \\theta_1 \\hat\\varepsilon_{t-1} - \\dots - \\theta_q \\hat\\varepsilon_{t-q},\\tag{2.31}\n\\end{equation}\\]\ngiven initial conditions, instance:\\(\\hat\\varepsilon_0=\\dots=\\hat\\varepsilon_{-q+1}=0\\) \\(y_{0}=\\dots=y_{-p+1}=\\mathbb{E}(y_i)=\\mu\\). (Recursions Eq. (2.31) start \\(t=1\\).)\\(\\hat\\varepsilon_p=\\dots=\\hat\\varepsilon_{p-q+1}=0\\) actual values \\(y_{}\\)’s \\(\\[1,p]\\). case, first \\(p\\) observations \\(y_t\\) used. Recursions Eq. (2.31) start \\(t=p+1\\).","code":"\nlibrary(AEC)\ny <- US3var$y.gdp.gap\nar3.Case1 <- arima(y,order = c(3,0,0),method=\"ML\")\nar3.Case2 <- arima(y,order = c(3,0,0),method=\"CSS\")\nrbind(ar3.Case1$coef,ar3.Case2$coef)##           ar1         ar2        ar3  intercept\n## [1,] 1.191267 -0.08934705 -0.1781163 -0.9226007\n## [2,] 1.192003 -0.08811150 -0.1787662 -1.0341696"},{"path":"TS.html","id":"specification-choice","chapter":"2 Time Series","heading":"2.2.9 Specification choice","text":"previouss section explains fit given ARMA specification. choose appropriate specification? possibility employ (P)ACF approach (see Figure 2.7). However, previous approach leads either AR MA process (ARMA process). one wants consider various ARMA(p,q) specifications, \\(p \\\\{1,\\dots,P\\}\\) \\(q \\\\{1,\\dots,Q\\}\\), say, one can resort information criteria.general, choosing specification, one faces following dilemma:rich specification may lead “overfitting”/misspecification, implying additional estimation errors (--sample forecasts).simple specification may lead potential omission valuable information (e.g., contained older lags).lag selection approach based -called information criteria consists maximizing fit data, adding penalty “richness” model. precisely, using approach amounts minimizing loss function () negatively depends fitting errors (b) positively depends number parameters model.Definition 2.20  (Information Criteria) Akaike (AIC), Hannan-Quinn (HQ) Schwarz information (BIC) criteria form\n\\[\nc^{()}(k) = \\underbrace{\\frac{- 2 \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}_T(k);\\mathbf{y})}{T}}_{\\mbox{decreases w.r.t. $k$}} \\quad +\n\\underbrace{\n\\frac{k\\phi^{()}(T)}{T},}_{\\mbox{increases w.r.t. $k$}}\n\\]\n\\(() \\\\{AIC,HQ,BIC\\}\\) \\(\\hat{\\boldsymbol\\theta}_T(k)\\) denotes ML estimate \\(\\boldsymbol\\theta_0(k)\\), vector parameters length \\(k\\).lag suggested criterion \\(()\\) given :\n\\[\n\\boxed{\\hat{k}^{()} = \\underset{k}{\\mbox{argmin}} \\quad c^{()}(k).}\n\\]case ARMA(p,q) process, \\(k=2+p+q\\).Proposition 2.11  (Consistency criteria-based lag selection) lag selection procedure consistent (see Def. 3.8) \n\\[\n\\lim_{T \\rightarrow \\infty} \\phi(T) = \\infty \\quad \\quad \\lim_{T \\rightarrow \\infty} \\phi(T)/T = 0.\n\\]\nnotably case HQ BIC criteria.Proof. true number lags denoted \\(k_0\\). show \\(\\lim_{T \\rightarrow \\infty} \\mathbb{P}(\\hat{k}_T \\ne k_0)=0\\).Case \\(k < k_0\\): model \\(k\\) parameter misspecified, therefore:\n\\[\n\\mbox{plim}_{T \\rightarrow \\infty}  \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}_T(k);\\mathbf{y})/T < \\mbox{plim}_{T \\rightarrow \\infty}  \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}_T(k_0);\\mathbf{y})/T.\n\\]\nHence, \\(\\lim_{T \\rightarrow \\infty} \\phi(T)/T = 0\\), : \\(\\lim_{T \\rightarrow \\infty} \\mathbb{P}(c(k_0) \\ge c(k)) \\rightarrow 0\\) \n\\[\n\\lim_{T \\rightarrow \\infty} \\mathbb{P}(\\hat{k}<k_0) \\le \\lim_{T \\rightarrow \\infty} \\mathbb{P}\\left\\{c(k_0) \\ge c(k) \\mbox{ $k < k_0$}\\right\\} = 0.\n\\]Case \\(k > k_0\\): null hypothesis, likelihood ratio (LR) test statistic (see Def. ??) satisfies:\n\\[\n2 \\left(\\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}_T(k);\\mathbf{y})-\\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}_T(k_0);\\mathbf{y})\\right) \\sim \\chi^2(k-k_0).\n\\]\n\\(\\lim_{T \\rightarrow \\infty} \\phi(T) = \\infty\\), : \\(\\mbox{plim}_{T \\rightarrow \\infty} -2 \\left(\\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}_T(k);\\mathbf{y})-\\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}_T(k_0);\\mathbf{y})\\right)/\\phi(T) = 0\\). Hence \\(\\mbox{plim}_{T \\rightarrow \\infty} T[c(k_0) - c(k)]/\\phi(T) \\le -1\\) \\(\\lim_{T \\rightarrow \\infty} \\mathbb{P}(c(k_0) \\ge c(k)) \\rightarrow 0\\), implies, spirit , \\(\\lim_{T \\rightarrow \\infty} \\mathbb{P}(\\hat{k}>k_0) = 0\\).Therefore, \\(\\lim_{T \\rightarrow \\infty} \\mathbb{P}(\\hat{k}=k_0) = 1\\).Example 2.5  (Linear regression) Consider linear regression normal disturbances:\n\\[\ny_t = \\mathbf{x}_t' \\boldsymbol\\beta + \\varepsilon_t, \\quad \\varepsilon_t \\sim ..d. \\mathcal{N}(0,\\sigma^2).\n\\]\nassociated log-likelihood form Eq. (2.26). case, :\n\\[\\begin{eqnarray*}\nc^{()}(k) &=& \\frac{- 2 \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}_T(k);\\mathbf{y})}{T} + \\frac{k\\phi^{()}(T)}{T}\\\\\n&\\approx& \\log(2\\pi) + \\log(\\widehat{\\sigma^2}) + \\frac{1}{T}\\sum_{t=1}^T \\frac{\\varepsilon_t^2}{\\widehat{\\sigma^2}} + \\frac{k\\phi^{()}(T)}{T}.\n\\end{eqnarray*}\\]\nlarge \\(T\\), consistent estimation scheme, :\n\\[\n\\widehat{\\sigma^2} \\approx \\frac{1}{T}\\sum_{t=1}^T \\varepsilon_t^2 = SSR/T.\n\\]\nHence \\(\\hat{k}^{()} \\approx \\underset{k}{\\mbox{argmin}} \\quad \\log(SSR/T) + \\dfrac{k\\phi^{()}(T)}{T}\\).Example 2.6  (Swiss GDP growth) Consider long historical time series Swiss GDP growth (see Figure 2.3), taken Jordà, Schularick, Taylor (2017) dataset. Let us look best ARMA specification using AIC criteria:best specification therefore AR(1) model. , although AR(2) (say) result better fit data, fit improvement large enough compensate additional AIC cost associated additional parameter.","code":"\nlibrary(AEC);data(JST)\ndata <- subset(JST,iso==\"CHE\")\nT <- dim(data)[1]\ny <- c(NaN,log(data$gdp[2:T]/data$gdp[1:(T-1)]))\n# Use AIC criteria to look for appropriate specif:\nmax.p <- 3;max.q <- 3;\nall.AIC <- NULL\nfor(p in 0:max.p){\n  for(q in 0:max.q){\n    res <- arima(y,order=c(p,0,q))\n    if(res$aic<min(all.AIC)){best.p<-p;best.q<-q}\n    all.AIC <- c(all.AIC,res$aic)}}\nprint(c(best.p,best.q))## [1] 1 0"},{"path":"TS.html","id":"VAR","chapter":"2 Time Series","heading":"2.3 Multivariate models","text":"section presents Vector Auto-Regressive Moving-Average (SVARMA) models. models widely used macroeconomic analysis. simple easy estimate, make possible conveniently capture dynamics complex multivariate systems. VAR popularity notably due Sims (1980)’s influential work. nice survey proposed J. H. Stock Watson (2016).economics, VAR models often employed order identify structural shocks, independent primitive exogenous forces drive economic variables (Ramey (2016)). often given specific economic meaning (e.g., demand supply shocks).Working models (VAR VARMA models) often often based two steps: first step, reduced-form version model estimated; second step, structural shocks identified IRFs produced.–>","code":""},{"path":"TS.html","id":"definition-of-vars-and-svarma-models","chapter":"2 Time Series","heading":"2.3.1 Definition of VARs (and SVARMA) models","text":"Definition 2.21  ((S)VAR model) Let \\(y_{t}\\) denote \\(n \\times1\\) vector random variables. Process \\(y_{t}\\) follows \\(p^{th}\\)-order (S)VAR , \\(t\\), \n\\[\\begin{eqnarray}\n\\begin{array}{rllll}\nVAR:& y_t &=& c + \\Phi_1 y_{t-1} + \\dots + \\Phi_p y_{t-p} + \\varepsilon_t,\\\\\nSVAR:& y_t &=& c + \\Phi_1 y_{t-1} + \\dots + \\Phi_p y_{t-p} + B \\eta_t,\n\\end{array}\\tag{2.32}\n\\end{eqnarray}\\]\n\\(\\varepsilon_t = B\\eta_t\\), \\(\\{\\eta_{t}\\}\\) white noise sequence whose components mutually serially independent.first line Eq. (2.32) corresponds reduced-form VAR model (structural form second line).structural shocks (components \\(\\eta_t\\)) mutually uncorrelated, case innovations, components \\(\\varepsilon_t\\). However, boths cases, vectors \\(\\eta_t\\) \\(\\varepsilon_t\\) serially correlated (time).case univariate models, VARs can extended MA terms \\(\\eta_t\\):Definition 2.22  ((S)VARMA model) Let \\(y_{t}\\) denote \\(n \\times1\\) vector random variables. Process \\(y_{t}\\) follows VARMA model order (p,q) , \\(t\\), \n\\[\\begin{eqnarray}\n\\begin{array}{rllll}\nVARMA:& y_t &=& c + \\Phi_1 y_{t-1} + \\dots + \\Phi_p y_{t-p} + \\varepsilon_t + \\Theta_1\\varepsilon_{t-1} + \\dots + \\Theta_q ,\\\\\nSVARMA:& y_t &=& c + \\Phi_1 y_{t-1} + \\dots + \\Phi_p y_{t-p} + B_0 \\eta_t+ B_1 \\eta_{t-1} + \\dots +  B_q \\eta_{t-q},\n\\end{array}\\tag{2.33}\n\\end{eqnarray}\\]\n\\(\\varepsilon_t = B_0\\eta_t\\) (\\(B_j = \\Theta_j B_0\\), \\(j \\ge 0\\)), \\(\\{\\eta_{t}\\}\\) white noise sequence whose components mutually serially independent.","code":""},{"path":"TS.html","id":"IRFSVARMA","chapter":"2 Time Series","heading":"2.3.2 IRFs in SVARMA","text":"One main objectives macro-econometrics derive IRFs, represent dynamic effects structural shocks (components \\(\\eta_t\\)) though system variables \\(y_t\\).Formally, IRF difference conditional expectations:\n\\[\n\\boxed{\\Psi_{,j,h} = \\mathbb{E}(y_{,t+h}|\\eta_{j,t}=1) - \\mathbb{E}(y_{,t+h})}\n\\]\n(effect \\(y_{,t+h}\\) one-unit shock \\(\\eta_{j,t}\\)).dynamics process \\(y_t\\) can described VARMA model, \\(y_t\\) covariance stationary (see Def. 2.4), \\(y_t\\) admits following infinite MA representation (MA(\\(\\infty\\))):\n\\[\\begin{equation}\ny_t = \\mu + \\sum_{h=0}^\\infty \\Psi_{h} \\eta_{t-h}.\\tag{2.34}\n\\end{equation}\\]\nalso Wold decomposition process \\(\\{y_t\\}\\) (see Theorem 2.3).Estimating IRFs amounts estimating \\(\\Psi_{h}\\)’s. general, exist three main approaches :Calibrate solve (purely structural) Dynamic Stochastic General Equilibrium (DSGE) model first order (linearization). solution takes form Eq. (2.34).Directly estimate \\(\\Psi_{h}\\) based projection approaches (see Section 2.3.11).Approximate infinite MA representation estimating parsimonious type model, e.g. VAR(MA) models (see Section 2.3.4). (Structural) VARMA representation obtained, Eq. (2.34) easily deduced. , one can use recursive algorithm univariate processes (see Prop. 2.8).Typically, consider AR(2) case. first steps algorithm mentioned last bullet point follows:\n\\[\\begin{eqnarray*}\ny_t &=& \\Phi_1 {\\color{blue}y_{t-1}} + \\Phi_2 y_{t-2} + B \\eta_t  \\\\\n&=& \\Phi_1 \\color{blue}{(\\Phi_1 y_{t-2} + \\Phi_2 y_{t-3} + B \\eta_{t-1})} + \\Phi_2 y_{t-2} + B \\eta_t  \\\\\n&=& B \\eta_t + \\Phi_1 B \\eta_{t-1} + (\\Phi_2 + \\Phi_1^2) \\color{red}{y_{t-2}} + \\Phi_1\\Phi_2 y_{t-3}  \\\\\n&=& B \\eta_t + \\Phi_1 B \\eta_{t-1} + (\\Phi_2 + \\Phi_1^2) \\color{red}{(\\Phi_1 y_{t-3} + \\Phi_2 y_{t-4} + B \\eta_{t-2})} + \\Phi_1\\Phi_2 y_{t-3} \\\\\n&=& \\underbrace{B}_{=\\Psi_0} \\eta_t + \\underbrace{\\Phi_1 B}_{=\\Psi_1} \\eta_{t-1} + \\underbrace{(\\Phi_2 + \\Phi_1^2)B}_{=\\Psi_2} \\eta_{t-2} + f(y_{t-3},y_{t-4}).\n\\end{eqnarray*}\\]particular, \\(B = \\Psi_0\\). Matrix \\(B\\) indeed captures contemporaneous impact \\(\\eta_t\\) \\(y_t\\). matrix \\(B\\) sometimes called impulse matrix.Example 2.7  (IRFs SVARMA model) Consider following VARMA(1,1) model:\n\\[\\begin{eqnarray}\n\\quad y_t &=&\n\\underbrace{\\left[\\begin{array}{cc}\n0.5 & 0.3 \\\\\n-0.4 & 0.7\n\\end{array}\\right]}_{\\Phi_1}\ny_{t-1} +  \n\\underbrace{\\left[\\begin{array}{cc}\n1 & 2 \\\\\n-1 & 1\n\\end{array}\\right]}_{B}\\eta_t + \\underbrace{\\left[\\begin{array}{cc}\n2 & 0 \\\\\n1 & 0.5\n\\end{array}\\right]}_{\\Theta_1} \\underbrace{\\left[\\begin{array}{cc}\n1 & 2 \\\\\n-1 & 1\n\\end{array}\\right]}_{B}\\eta_{t-1}.\\tag{2.35}\n\\end{eqnarray}\\]can use function simul.VARMA package AEC produce IRFs (using indic.IRF=1 list arguments):\nFigure 2.14: Impulse response functions\n","code":"\nlibrary(AEC)\ndistri <- list(type=c(\"gaussian\",\"gaussian\"),df=c(4,4))\nn <- length(distri$type) # dimension of y_t\nnb.sim <- 30\neps <- simul.distri(distri,nb.sim)\nPhi <- array(NaN,c(n,n,1))\nPhi[,,1] <- matrix(c(.5,-.4,.3,.7),2,2)\np <- dim(Phi)[3]\nTheta <- array(NaN,c(n,n,1))\nTheta[,,1] <- -matrix(c(2,1,0,.5),2,2)\nq <- dim(Theta)[3]\nMu <- rep(0,n)\nC <- matrix(c(1,-1,2,1),2,2)\nModel <- list(\n  Mu = Mu,Phi = Phi,Theta = Theta,C = C,distri = distri)\nY0 <- rep(0,n)\neta0 <- c(1,0)\nres.sim.1 <- simul.VARMA(Model,nb.sim,Y0,eta0,indic.IRF=1)\neta0 <- c(0,1)\nres.sim.2 <- simul.VARMA(Model,nb.sim,Y0,eta0,indic.IRF=1)\npar(plt=c(.15,.95,.25,.8))\npar(mfrow=c(2,2))\nplot(res.sim.1$Y[1,],las=1,\n     type=\"l\",lwd=3,xlab=\"\",ylab=\"\",\n     main=expression(paste(\"Response of \",y[1,\"*,*\",t],\n                           \" to a one-unit increase in \",eta[1],sep=\"\")))\nabline(h=0,col=\"grey\",lty=3)\nplot(res.sim.2$Y[1,],las=1,\n     type=\"l\",lwd=3,xlab=\"\",ylab=\"\",\n     main=expression(paste(\"Response of \",y[1,\"*,*\",t],\n                           \" to a one-unit increase in \",eta[2],sep=\"\")))\nabline(h=0,col=\"grey\",lty=3)\nplot(res.sim.1$Y[2,],las=1,\n     type=\"l\",lwd=3,xlab=\"\",ylab=\"\",\n     main=expression(paste(\"Response of \",y[2,\"*,*\",t],\n                           \" to a one-unit increase in \",eta[1],sep=\"\")))\nabline(h=0,col=\"grey\",lty=3)\nplot(res.sim.2$Y[2,],las=1,\n     type=\"l\",lwd=3,xlab=\"\",ylab=\"\",\n     main=expression(paste(\"Response of \",y[2,\"*,*\",t],\n                           \" to a one-unit increase in \",eta[2],sep=\"\")))\nabline(h=0,col=\"grey\",lty=3)"},{"path":"TS.html","id":"covariance-stationary-varma-models","chapter":"2 Time Series","heading":"2.3.3 Covariance-stationary VARMA models","text":"Let’s come back infinite MA case (Eq. (2.34)):\n\\[\ny_t = \\mu + \\sum_{h=0}^\\infty \\Psi_{h} \\eta_{t-h}.\n\\]\n\\(y_t\\) covariance-stationary (ergodic mean), case \n\\[\\begin{equation}\n\\sum_{=0}^\\infty \\|\\Psi_i\\| < \\infty,\\tag{2.36}\n\\end{equation}\\]\n\\(\\|\\|\\) denotes norm matrix \\(\\) (e.g. \\(\\|\\|=\\sqrt{tr(AA')}\\)). notably implies \\(y_t\\) stationary (ergodic mean), \\(\\|\\Psi_h\\|\\rightarrow 0\\) \\(h\\) gets large.satisfied \\(\\Phi_k\\)’s \\(\\Theta_k\\)’s VARMA-based process (Eq. (??)) stationary? conditions similar univariate case (see Prop. 2.6). Let us introduce following notations:\n\\[\\begin{eqnarray}\ny_t &=& c + \\underbrace{\\Phi_1 y_{t-1} + \\dots +\\Phi_p y_{t-p}}_{\\color{blue}{\\mbox{AR component}}} +  \\tag{2.37}\\\\\n&&\\underbrace{B \\eta_t+ \\Theta_1 B \\eta_{t-1}+ \\dots+ \\Theta_q B \\eta_{t-q}}_{\\color{red}{\\mbox{MA component}}} \\nonumber\\\\\n&\\Leftrightarrow& \\underbrace{(- \\Phi_1 L - \\dots - \\Phi_p L^p)}_{= \\color{blue}{\\Phi(L)}}y_t = c +  \\underbrace{ \\color{red}{(- \\Theta_1 L - \\ldots - \\Theta_q L^q)}}_{=\\color{red}{\\Theta(L)}} B \\eta_{t}. \\nonumber\n\\end{eqnarray}\\]Process \\(y_t\\) stationary iff roots \\(\\det(\\Phi(z))=0\\) strictly outside unit circle , equivalently, iff eigenvalues \n\\[\\begin{equation}\n\\Phi = \\left[\\begin{array}{cccc}\n\\Phi_{1} & \\Phi_{2} & \\cdots & \\Phi_{p}\\\\\n& 0 & \\cdots & 0\\\\\n0 & \\ddots & 0 & 0\\\\\n0 & 0 & & 0\\end{array}\\right]\\tag{2.38}\n\\end{equation}\\]\nlie strictly within unit circle. Hence, case univariate processes, covariance-stationarity VARMA model depends specification AR part.Let’s derive first two unconditional moments (covariance-stationary) VARMA process.Based Eq. (2.37), \\(\\mathbb{E}(\\Phi(L)y_t)=c\\), gives \\(\\Phi(1)\\mathbb{E}(y_t)=c\\), ::\n\\[\n\\mathbb{E}(y_t) = (- \\Phi_1 - \\dots - \\Phi_p)^{-1}c.\n\\]\nautocovariances \\(y_t\\) can deduced infinite MA representation (Eq. (2.34)). :\n\\[\n\\gamma_j \\equiv \\mathbb{C}ov(y_t,y_{t-j}) = \\sum_{=j}^\\infty \\Psi_i \\Psi_{-j}'.\n\\]\n(Note infinite sum exists soon Eq. (2.36) satisfied.)Conditional means autocovariances can also deduced Eq. (2.34). \\(0 \\le h\\) \\(0 \\le h_1 \\le h_2\\):\n\\[\\begin{eqnarray*}\n\\mathbb{E}_t(y_{t+h}) &=& \\mu + \\sum_{k=0}^\\infty \\Psi_{k+h} \\eta_{t-k} \\\\\n\\mathbb{C}ov_t(y_{t+1+h_1},y_{t+1+h_2}) &=& \\sum_{k=0}^{h_1} \\Psi_{k}\\Psi_{k+h_2-h_1}'.\n\\end{eqnarray*}\\]previous formula implies particular forecasting error \\(y_{t+h} - \\mathbb{E}_t(y_{t+h})\\) variance equal :\n\\[\n\\mathbb{V}ar_t(y_{t+h}) = \\sum_{k=1}^{h} \\Psi_{k}\\Psi_{k}'.\n\\]\n\\(\\eta_t\\) mutually serially independent (therefore uncorrelated), :\n\\[\n\\mathbb{V}ar(\\Psi_k \\eta_{t-k}) = \\mathbb{V}ar\\left(\\sum_{=1}^n \\psi_{k,} \\eta_{,t-k}\\right)  = \\sum_{=1}^n \\psi_{k,}\\psi_{k,}',\n\\]\n\\(\\psi_{k,}\\) denotes \\(^{th}\\) column \\(\\Psi_k\\).suggests following decomposition variance forecast error (called variance decomposition):\n\\[\n\\mathbb{V}ar_t(y_{t+h}) = \\sum_{=1}^n \\underbrace{\\sum_{k=1}^{h}  \\psi_{k,}\\psi_{k,}'}_{\\mbox{Contribution $\\eta_{,t}$}}.\n\\]Let us now turn estimation VAR(MA) models.MA component, OLS regressions yield biased estimates (even asymptotically large samples).Assume \\(y_t\\) follows VARMA(1,1) model. :\n\\[\ny_{,t} = \\phi_i y_{t-1} + \\varepsilon_{,t},\n\\]\n\\(\\phi_i\\) \\(^{th}\\) row \\(\\Phi_1\\), \\(\\varepsilon_{,t}\\) linear combination \\(\\eta_t\\) \\(\\eta_{t-1}\\).Since \\(y_{t-1}\\) (regressor) correlated \\(\\eta_{t-1}\\), also correlated \\(\\varepsilon_{,t}\\).OLS regression \\(y_{,t}\\) \\(y_{t-1}\\) yields biased estimator \\(\\phi_i\\). Hence, SVARMA models consistently estimated simple OLS regressions (contrary VAR models, see next section); instrumental-variable approaches can employed estimate SVARMA models.","code":""},{"path":"TS.html","id":"estimVAR","chapter":"2 Time Series","heading":"2.3.4 VAR estimation","text":"section discusses estimation VAR models. (estimation SVARMA models challenging, see, e.g., Gouriéroux, Monfort, Renne (2020).) Eq. (2.32) can written:\n\\[\ny_{t}=c+\\Phi(L)y_{t-1}+\\varepsilon_{t},\n\\]\n\\(\\Phi(L) = \\Phi_1 + \\Phi_2 L + \\dots + \\Phi_p L^{p-1}\\).Consequently:\n\\[\ny_{t}\\mid y_{t-1},y_{t-2},\\ldots,y_{-p+1}\\sim \\mathcal{N}(c+\\Phi_{1}y_{t-1}+\\ldots\\Phi_{p}y_{t-p},\\Omega).\n\\]Using Hamilton (1994)’s notations, denote \\(\\Pi\\) matrix \\(\\left[\\begin{array}{ccccc} c & \\Phi_{1} & \\Phi_{2} & \\ldots & \\Phi_{p}\\end{array}\\right]'\\) \\(x_{t}\\) vector \\(\\left[\\begin{array}{ccccc} 1 & y'_{t-1} & y'_{t-2} & \\ldots & y'_{t-p}\\end{array}\\right]'\\), :\n\\[\\begin{equation}\ny_{t}= \\Pi'x_{t} + \\varepsilon_{t}. \\tag{2.39}\n\\end{equation}\\]\nprevious representation convenient discuss estimation VAR model, parameters gathered two matrices : \\(\\Pi\\) \\(\\Omega\\).Let us start case shocks Gaussian.Proposition 2.12  (MLE Gaussian VAR) \\(y_t\\) follows VAR(p) (see Definition 2.21), \\(\\varepsilon_t \\sim \\,..d.\\,\\mathcal{N}(0,\\Omega)\\), ML estimate \\(\\Pi\\), denoted \\(\\hat{\\Pi}\\) (see Eq. (2.39)), given \n\\[\\begin{equation}\n\\hat{\\Pi}=\\left[\\sum_{t=1}^{T}x_{t}x'_{t}\\right]^{-1}\\left[\\sum_{t=1}^{T}y_{t}'x_{t}\\right]= (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{y},\\tag{2.40}\n\\end{equation}\\]\n\\(\\mathbf{X}\\) \\(T \\times (np)\\) matrix whose \\(t^{th}\\) row \\(x_t\\) \\(\\mathbf{y}\\) \\(T \\times n\\) matrix whose \\(t^{th}\\) row \\(y_{t}'\\)., \\(^{th}\\) column \\(\\hat{\\Pi}\\) (\\(b_i\\), say) OLS estimate \\(\\beta_i\\), :\n\\[\\begin{equation}\ny_{,t} = \\beta_i'x_t + \\varepsilon_{,t},\\tag{2.41}\n\\end{equation}\\]\n(.e., \\(\\beta_i' = [c_i,\\phi_{,1}',\\dots,\\phi_{,p}']'\\)).ML estimate \\(\\Omega\\), denoted \\(\\hat{\\Omega}\\), coincides sample covariance matrix \\(n\\) series OLS residuals Eq. (2.41), .e.:\n\\[\\begin{equation}\n\\hat{\\Omega} = \\frac{1}{T} \\sum_{=1}^T \\hat{\\varepsilon}_t\\hat{\\varepsilon}_t',\\quad\\mbox{} \\hat{\\varepsilon}_t= y_t - \\hat{\\Pi}'x_t.\n\\end{equation}\\]asymptotic distributions estimators ones resulting standard OLS formula.Proof. See Appendix 3.5.stated Proposition 2.13, shocks Gaussian, OLS regressions still provide consistent estimates model parameters. However, since \\(x_t\\) correlates \\(\\varepsilon_s\\) \\(s<t\\), OLS estimator \\(\\mathbf{b}_i\\) \\(\\boldsymbol\\beta_i\\) biased small sample. (also case ML estimator.)Indeed, denoting \\(\\boldsymbol\\varepsilon_i\\) \\(T \\times 1\\) vector \\(\\varepsilon_{,t}\\)’s, using notations \\(b_i\\) \\(\\beta_i\\) introduced Proposition 2.12, :\n\\[\\begin{equation}\n\\mathbf{b}_i = \\beta_i + (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\boldsymbol\\varepsilon_i.\\tag{2.24}\n\\end{equation}\\]\nnon-zero correlation \\(x_t\\) \\(\\varepsilon_{,s}\\) \\(s<t\\) , therefore, \\(\\mathbb{E}[(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\boldsymbol\\varepsilon_i] \\ne 0\\).However, \\(y_t\\) covariance stationary, \\(\\frac{1}{n}\\mathbf{X}'\\mathbf{X}\\) converges positive definite matrix \\(\\mathbf{Q}\\), \\(\\frac{1}{n}X'\\boldsymbol\\varepsilon_i\\) converges 0. Hence \\(\\mathbf{b}_i \\overset{p}{\\rightarrow} \\beta_i\\). precisely:Proposition 2.13  (Asymptotic distribution OLS estimate $\\beta_i$) \\(y_t\\) follows VAR model, defined Definition 2.21, :\n\\[\n\\sqrt{T}(\\mathbf{b}_i-\\beta_i) =  \\underbrace{\\left[\\frac{1}{T}\\sum_{t=p}^T x_t x_t' \\right]^{-1}}_{\\overset{p}{\\rightarrow} \\mathbf{Q}^{-1}}\n\\underbrace{\\sqrt{T} \\left[\\frac{1}{T}\\sum_{t=1}^T x_t\\varepsilon_{,t} \\right]}_{\\overset{d}{\\rightarrow} \\mathcal{N}(0,\\sigma_i^2\\mathbf{Q})},\n\\]\n\\(\\sigma_i = \\mathbb{V}ar(\\varepsilon_{,t})\\) \\(\\mathbf{Q} = \\mbox{plim }\\frac{1}{T}\\sum_{t=p}^T x_t x_t'\\) given :\n\\[\\begin{equation}\n\\mathbf{Q} = \\left[\n\\begin{array}{ccccc}\n1 & \\mu' &\\mu' & \\dots & \\mu' \\\\\n\\mu & \\gamma_0 + \\mu\\mu' & \\gamma_1 + \\mu\\mu' & \\dots & \\gamma_{p-1} + \\mu\\mu'\\\\\n\\mu & \\gamma_1 + \\mu\\mu' & \\gamma_0 + \\mu\\mu' & \\dots & \\gamma_{p-2} + \\mu\\mu'\\\\\n\\vdots &\\vdots &\\vdots &\\dots &\\vdots \\\\\n\\mu & \\gamma_{p-1} + \\mu\\mu' & \\gamma_{p-2} + \\mu\\mu' & \\dots & \\gamma_{0} + \\mu\\mu'\n\\end{array}\n\\right].\\tag{2.25}\n\\end{equation}\\]Proof. See Appendix 3.5.following proposition extends previous proposition includes covariances different \\(\\beta_i\\)’s well asymptotic distribution ML estimates \\(\\Omega\\).Proposition 2.14  (Asymptotic distribution OLS estimates) \\(y_t\\) follows VAR model, defined Definition 2.21, :\n\\[\\begin{equation}\n\\sqrt{T}\\left[\n\\begin{array}{c}\nvec(\\hat\\Pi - \\Pi)\\\\\nvec(\\hat\\Omega - \\Omega)\n\\end{array}\n\\right]\n\\sim \\mathcal{N}\\left(0,\n\\left[\n\\begin{array}{cc}\n\\Omega \\otimes \\mathbf{Q}^{-1} & 0\\\\\n0 & \\Sigma_{22}\n\\end{array}\n\\right]\\right),\\tag{2.42}\n\\end{equation}\\]\ncomponent \\(\\Sigma_{22}\\) corresponding covariance \\(\\hat\\sigma_{,j}\\) \\(\\hat\\sigma_{k,l}\\) (\\(,j,l,m \\\\{1,\\dots,n\\}^4\\)) equal \\(\\sigma_{,l}\\sigma_{j,m}+\\sigma_{,m}\\sigma_{j,l}\\).Proof. See Hamilton (1994), Appendix Chapter 11.Naturally, practice, \\(\\Omega\\) replaced \\(\\hat{\\Omega}\\), \\(\\mathbf{Q}\\) replaced \\(\\hat{\\mathbf{Q}} = \\frac{1}{T}\\sum_{t=p}^T x_t x_t'\\) \\(\\Sigma\\) matrix whose components form \\(\\hat\\sigma_{,l}\\hat\\sigma_{j,m}+\\hat\\sigma_{,m}\\hat\\sigma_{j,l}\\), \\(\\hat\\sigma_{,l}\\)’s components \\(\\hat\\Omega\\).simplicity VAR framework tractability MLE open way convenient econometric testing. Let’s illustrate likelihood ratio test (see Def. ??). maximum value achieved MLE \n\\[\n\\log\\mathcal{L}(Y_{T};\\hat{\\Pi},\\hat{\\Omega}) = -\\frac{Tn}{2}\\log(2\\pi)+\\frac{T}{2}\\log\\left|\\hat{\\Omega}^{-1}\\right| -\\frac{1}{2}\\sum_{t=1}^{T}\\left[\\hat{\\varepsilon}_{t}'\\hat{\\Omega}^{-1}\\hat{\\varepsilon}_{t}\\right].\n\\]\nlast term :\n\\[\\begin{eqnarray*}\n\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}'\\hat{\\Omega}^{-1}\\hat{\\varepsilon}_{t} &=& \\mbox{Tr}\\left[\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}'\\hat{\\Omega}^{-1}\\hat{\\varepsilon}_{t}\\right] = \\mbox{Tr}\\left[\\sum_{t=1}^{T}\\hat{\\Omega}^{-1}\\hat{\\varepsilon}_{t}\\hat{\\varepsilon}_{t}'\\right]\\\\\n&=&\\mbox{Tr}\\left[\\hat{\\Omega}^{-1}\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}\\hat{\\varepsilon}_{t}'\\right] = \\mbox{Tr}\\left[\\hat{\\Omega}^{-1}\\left(T\\hat{\\Omega}\\right)\\right]=Tn.\n\\end{eqnarray*}\\]\nTherefore, optimized log-likelihood simply obtained :\n\\[\\begin{equation}\n\\log\\mathcal{L}(Y_{T};\\hat{\\Pi},\\hat{\\Omega})=-(Tn/2)\\log(2\\pi)+(T/2)\\log\\left|\\hat{\\Omega}^{-1}\\right|-Tn/2.\\tag{2.43}\n\\end{equation}\\]Assume want test null hypothesis set variables follows VAR(\\(p_{0}\\)) alternative\nspecification \\(p_{1}\\) (\\(>p_{0}\\)).Let us denote \\(\\hat{L}_{0}\\) \\(\\hat{L}_{1}\\) maximum log-likelihoods obtained \\(p_{0}\\) \\(p_{1}\\) lags, respectively.null hypothesis (\\(H_0\\): \\(p=p_0\\)), :\n\\[\\begin{eqnarray*}\n2\\left(\\hat{L}_{1}-\\hat{L}_{0}\\right)&=&T\\left(\\log\\left|\\hat{\\Omega}_{1}^{-1}\\right|-\\log\\left|\\hat{\\Omega}_{0}^{-1}\\right|\\right)  \\sim \\chi^2(n^{2}(p_{1}-p_{0})).\n\\end{eqnarray*}\\]precedes can used help determine appropriate number lags use specification. VAR, using many lags consumes numerous degrees freedom: \\(p\\) lags, \\(n\\) equations VAR contains \\(n\\times p\\) coefficients plus intercept term. Adding lags improve -sample fit, likely result -parameterization affect --sample prediction performance.select appropriate lag length, selection criteria can used (see Definition 2.20). context VAR models, using Eq. (2.43), :\n\\[\\begin{eqnarray*}\nAIC & = & cst + \\log\\left|\\hat{\\Omega}\\right|+\\frac{2}{T}N\\\\\nBIC & = & cst + \\log\\left|\\hat{\\Omega}\\right|+\\frac{\\log T}{T}N,\n\\end{eqnarray*}\\]\n\\(N=p \\times n^{2}\\).","code":""},{"path":"TS.html","id":"BlockGranger","chapter":"2 Time Series","heading":"2.3.5 Block exogeneity and Granger causality","text":"Block exogeneityLet’s decompose \\(y_t\\) two subvectors \\(y^{(1)}_{t}\\) (\\(n_1 \\times 1\\)) \\(y^{(2)}_{t}\\) (\\(n_2 \\times 1\\)), \\(y_t' = [{y^{(1)}_{t}}',{y^{(2)}_{t}}']\\) (therefore \\(n=n_1 +n_2\\)), :\n\\[\n\\left[\n\\begin{array}{c}\ny^{(1)}_{t}\\\\\ny^{(2)}_{t}\n\\end{array}\n\\right] = \\left[\n\\begin{array}{cc}\n\\Phi^{(1,1)} & \\Phi^{(1,2)}\\\\\n\\Phi^{(2,1)} & \\Phi^{(2,2)}\n\\end{array}\n\\right]\n\\left[\n\\begin{array}{c}\ny^{(1)}_{t-1}\\\\\ny^{(2)}_{t-1}\n\\end{array}\n\\right] + \\varepsilon_t.\n\\]\nUsing, e.g., likelihood ratio test (see Def. ??), one can easily test block exogeneity \\(y_t^{(2)}\\) (say). null assumption can expressed \\(\\Phi^{(2,1)}=0\\).Granger CausalityGranger (1969) developed method explore causal relationships among variables. approach consists determining whether past values \\(y_{1,t}\\) can help explain current \\(y_{2,t}\\) (beyond information already included past values \\(y_{2,t}\\)).Formally, let us denote three information sets:\n\\[\\begin{eqnarray*}\n\\mathcal{}_{1,t} & = & \\left\\{ y_{1,t},y_{1,t-1},\\ldots\\right\\} \\\\\n\\mathcal{}_{2,t} & = & \\left\\{ y_{2,t},y_{2,t-1},\\ldots\\right\\} \\\\\n\\mathcal{}_{t} & = & \\left\\{ y_{1,t},y_{1,t-1},\\ldots y_{2,t},y_{2,t-1},\\ldots\\right\\}.\n\\end{eqnarray*}\\]\nsay \\(y_{1,t}\\) Granger-causes \\(y_{2,t}\\) \n\\[\n\\mathbb{E}\\left[y_{2,t}\\mid \\mathcal{}_{2,t-1}\\right]\\neq \\mathbb{E}\\left[y_{2,t}\\mid \\mathcal{}_{t-1}\\right].\n\\]get intuition behind testing procedure, consider following\nbivariate VAR(\\(p\\)) process:\n\\[\\begin{eqnarray*}\ny_{1,t} & = & c_1+\\Sigma_{=1}^{p}\\Phi_i^{(11)}y_{1,t-}+\\Sigma_{=1}^{p}\\Phi_i^{(12)}y_{2,t-}+\\varepsilon_{1,t}\\\\\ny_{2,t} & = & c_2+\\Sigma_{=1}^{p}\\Phi_i^{(21)}y_{1,t-}+\\Sigma_{=1}^{p}\\Phi_i^{(22)}y_{2,t-}+\\varepsilon_{2,t},\n\\end{eqnarray*}\\]\n\\(\\Phi_k^{(ij)}\\) denotes element \\((,j)\\) \\(\\Phi_k\\)., \\(y_{1,t}\\) said Granger-cause \\(y_{2,t}\\) \n\\[\n\\Phi_1^{(21)}=\\Phi_2^{(21)}=\\ldots=\\Phi_p^{(21)}=0.\n\\]\nTherefore hypothesis testing \n\\[\n\\begin{cases}\nH_{0}: & \\Phi_1^{(21)}=\\Phi_2^{(21)}=\\ldots=\\Phi_p^{(21)}=0\\\\\nH_{1}: & \\Phi_1^{(21)}\\neq0\\mbox{ }\\Phi_2^{(21)}\\neq0\\mbox{ }\\ldots\\Phi_p^{(21)}\\neq0.\\end{cases}\n\\]\nLoosely speaking, reject \\(H_{0}\\) coefficients lagged \\(y_{1,t}\\)’s statistically significant. Formally, can tested using \\(F\\)-test asymptotic chi-square test. \\(F\\)-statistic \n\\[\nF=\\frac{(RSS-USS)/p}{USS/(T-2p-1)},\n\\]\nRSS Restricted sum squared residuals USS Unrestricted sum squared residuals. \\(H_{0}\\), \\(F\\)-statistic distributed \\(\\mathcal{F}(p,T-2p-1)\\). (\\(pF\\underset{T \\rightarrow \\infty}{\\rightarrow}\\chi^{2}(p)\\).)","code":""},{"path":"TS.html","id":"identification-problem-and-standard-identification-techniques","chapter":"2 Time Series","heading":"2.3.6 Identification problem and standard identification techniques","text":"Section 2.3.4, seen estimate \\(\\mathbb{V}ar(\\varepsilon_t) =\\Omega\\) \\(\\Phi_k\\) matrices context VAR model. IRFs functions \\(B\\) \\(\\Phi_k\\)’s, \\(\\Omega\\) \\(\\Phi_k\\)’s (see Section 2.3.2). \\(\\Omega = BB'\\), sufficient recover \\(B\\).Indeed, seen system equations whose unknowns \\(b_{,j}\\)’s (components \\(B\\)), system \\(\\Omega = BB'\\) contains \\(n(n+1)/2\\) linearly independent equations. instance, \\(n=2\\):\n\\[\\begin{eqnarray*}\n&&\\left[\n\\begin{array}{cc}\n\\omega_{11} & \\omega_{12} \\\\\n\\omega_{12} & \\omega_{22}\n\\end{array}\n\\right] = \\left[\n\\begin{array}{cc}\nb_{11} & b_{12} \\\\\nb_{21} & b_{22}\n\\end{array}\n\\right]\\left[\n\\begin{array}{cc}\nb_{11} & b_{21} \\\\\nb_{12} & b_{22}\n\\end{array}\n\\right]\\\\\n&\\Leftrightarrow&\\left[\n\\begin{array}{cc}\n\\omega_{11} & \\omega_{12} \\\\\n\\omega_{12} & \\omega_{22}\n\\end{array}\n\\right] = \\left[\n\\begin{array}{cc}\nb_{11}^2+b_{12}^2 & \\color{red}{b_{11}b_{21}+b_{12}b_{22}} \\\\\n\\color{red}{b_{11}b_{21}+b_{12}b_{22}} & b_{22}^2 + b_{21}^2\n\\end{array}\n\\right].\n\\end{eqnarray*}\\]3 linearly independent equations 4 unknowns. Therefore, \\(B\\) identified based second-order moments. Additional restrictions required identify \\(B\\). section covers two standard identification schemes: short-run long-run restrictions:short-run restriction (SRR) prevents structural shock affecting endogenous variable contemporaneously.Easy implement: appropriate entries \\(B\\) set 0.Particular case: Cholesky, recursive approach.Examples: Bernanke (1986), Sims (1986), Galí (1992), Ruibio-Ramírez, Waggoner, Zha (2010).long-run restriction (LRR) prevents structural shock cumulative impact one endogenous variables.Additional computations required implement . One needs compute cumulative effect one structural shocks \\(u_{t}\\) one endogenous variable.Examples: Blanchard Quah (1989), Faust Leeper (1997), Galí (1999), Erceg, Guerrieri, Gust (2005), Christiano, Eichenbaum, Vigfusson (2007).two approaches can combined (see, e.g., Gerlach Smets (1995)).Let us consider simple example motivate short-run restrictions. Consider following stylized macro model:\n\\[\\begin{equation}\n\\begin{array}{clll}\ng_{t}&=& \\bar{g}-\\lambda(i_{t-1}-\\mathbb{E}_{t-1}\\pi_{t})+ \\underbrace{{\\color{blue}\\sigma_d \\eta_{d,t}}}_{\\mbox{demand shock}}& (\\mbox{curve})\\\\\n\\Delta \\pi_{t} & = & \\beta (g_{t} - \\bar{g})+ \\underbrace{{\\color{blue}\\sigma_{\\pi} \\eta_{\\pi,t}}}_{\\mbox{cost push shock}} & (\\mbox{Phillips curve})\\\\\ni_{t} & = & \\rho i_{t-1} + \\left[ \\gamma_\\pi \\mathbb{E}_{t}\\pi_{t+1}  + \\gamma_g (g_{t} - \\bar{g}) \\right]\\\\\n&& \\qquad \\qquad+\\underbrace{{\\color{blue}\\sigma_{mp} \\eta_{mp,t}}}_{\\mbox{Mon. Pol. shock}} & (\\mbox{Taylor rule}),\n\\end{array}\\tag{2.44}\n\\end{equation}\\]\n:\n\\[\\begin{equation}\n\\eta_t =\n\\left[\n\\begin{array}{c}\n\\eta_{\\pi,t}\\\\\n\\eta_{d,t}\\\\\n\\eta_{mp,t}\n\\end{array}\n\\right]\n\\sim ..d.\\,\\mathcal{N}(0,).\\tag{2.45}\n\\end{equation}\\]Vector \\(\\eta_t\\) assumed vector structural shocks, mutually serially independent. date \\(t\\):\\(g_t\\) contemporaneously affected \\(\\eta_{d,t}\\) ;\\(\\pi_t\\) contemporaneously affected \\(\\eta_{\\pi,t}\\) \\(\\eta_{d,t}\\);\\(i_t\\) contemporaneously affected \\(\\eta_{mp,t}\\), \\(\\eta_{\\pi,t}\\) \\(\\eta_{d,t}\\).System (2.44) rewritten form:\n\\[\\begin{equation}\n\\left[\\begin{array}{c}\nd_t\\\\\n\\pi_t\\\\\ni_t\n\\end{array}\\right]\n= \\Phi(L)\n\\left[\\begin{array}{c}\nd_{t-1}\\\\\n\\pi_{t-1}\\\\\ni_{t-1} +\n\\end{array}\\right] +\\underbrace{\\underbrace{\n\\left[\n\\begin{array}{ccc}\n0 & \\bullet & 0 \\\\\n\\bullet & \\bullet & 0 \\\\\n\\bullet & \\bullet & \\bullet\n\\end{array}\n\\right]}_{=B} \\eta_t}_{=\\varepsilon_t}\\tag{2.46}\n\\end{equation}\\]reduced-form model. representation suggests three additional restrictions entries \\(B\\); latter matrix therefore identified (signs columns) soon \\(\\Omega = BB'\\) known.particular cases well-known matrix decomposition \\(\\Omega=\\mathbb{V}ar(\\varepsilon_t)\\) can used easily estimate specific SVAR.Consider following context:first shock (say, \\(\\eta_{n_1,t}\\)) can affect instantaneously\n(.e., date \\(t\\)) one endogenous variable (say, \\(y_{n_1,t}\\));second shock (say, \\(\\eta_{n_2,t}\\)) can affect instantaneously\n(.e., date \\(t\\)) two endogenous variables, \\(y_{n_1,t}\\) () \\(y_{n_2,t}\\);\\(\\dots\\)implies (1) column \\(n_1\\) \\(B\\) 1 non-zero entry (\\(n_1^{th}\\) entry), (2) column \\(n_2\\) \\(B\\) 2 non-zero entries (\\(n_1^{th}\\) \\(n_2^{th}\\) ones), etc. Without loss generality, can set \\(n_1=n\\), \\(n_2=n-1\\), etc. context, matrix \\(B\\) lower triangular.Cholesky decomposition \\(\\Omega_{\\varepsilon}\\) provides appropriate estimate \\(B\\), since matrix decomposition yields lower triangular matrix satisfying:\n\\[\n\\Omega_\\varepsilon = BB'.\n\\]instance, Dedola Lippi (2005) estimate 5 structural VAR models US, UK, Germany, France Italy analyse monetary-policy transmission mechanisms. estimate SVAR(5) models period 1975-1997. shock-identification scheme based Cholesky decompositions, ordering endogenous variables : industrial production, consumer price index, commodity price index, short-term rate, monetary aggregate effective exchange rate (except US). ordering implies monetary policy reacts shocks affecting first three variables latter react monetary policy shocks one-period lag .Importantly, Cholesky approach can useful one interested one specific structural shock. case, e.g., Christiano, Eichenbaum, Evans (1996). identification based following relationship \\(\\varepsilon_t\\) \\(\\eta_t\\):\n\\[\n\\left[\\begin{array}{c}\n\\boldsymbol\\varepsilon_{S,t}\\\\\n\\varepsilon_{r,t}\\\\\n\\boldsymbol\\varepsilon_{F,t}\n\\end{array}\\right] =\n\\left[\\begin{array}{ccc}\nB_{SS} & 0 & 0 \\\\\nB_{rS} & B_{rr} & 0 \\\\\nB_{FS} & B_{Fr} & B_{FF}\n\\end{array}\\right]\n\\left[\\begin{array}{c}\n\\boldsymbol\\eta_{S,t}\\\\\n\\eta_{r,t}\\\\\n\\boldsymbol\\eta_{F,t}\n\\end{array}\\right],\n\\]\n\\(S\\), \\(r\\) \\(F\\) respectively correspond slow-moving variables, policy variable (short-term rate) fast-moving variables. \\(\\eta_{r,t}\\) scalar, \\(\\boldsymbol\\eta_{S,t}\\) \\(\\boldsymbol\\eta_{F,t}\\) may vectors. space spanned \\(\\boldsymbol\\varepsilon_{S,t}\\) spanned \\(\\boldsymbol\\eta_{S,t}\\). result, \\(\\varepsilon_{r,t}\\) linear combination \\(\\eta_{r,t}\\) \\(\\boldsymbol\\eta_{S,t}\\) (\\(\\perp\\)), comes \\(B_{rr}\\eta_{r,t}\\)’s (population) residuals regression \\(\\varepsilon_{r,t}\\) \\(\\boldsymbol\\varepsilon_{S,t}\\). \\(\\mathbb{V}ar(\\eta_{r,t})=1\\), \\(B_{rr}\\) given square root variance \\(B_{rr}\\eta_{r,t}\\). \\(B_{F,r}\\) finally obtained regressing components \\(\\boldsymbol\\varepsilon_{F,t}\\) estimates \\(\\eta_{r,t}\\).equivalent approach consists computing Cholesky decomposition \\(BB'\\) contemporaneous impacts monetary policy shock (\\(n\\) endogenous variables) components column \\(B\\) corresponding policy variable.\nFigure 2.15: Response monetary-policy shock. Identification approach Christiano, Eichenbaum Evans (1996). Confidence intervals obtained boostrapping estimated VAR model (see inference section).\nLet us now turn Long-run restrictions. restriction concerns long-run influence shock endogenous variable. Let us consider instance structural shock assumed “long-run influence” GDP. express ? long-run change GDP can expressed \\(GDP_{t+h} - GDP_t\\), \\(h\\) large. Note :\n\\[\nGDP_{t+h} - GDP_t = \\Delta GDP_{t+h} +\\Delta GDP_{t+h-1} + \\dots + \\Delta GDP_{t+1}.\n\\]\nHence, fact given structural shock (\\(\\eta_{,t}\\), say) long-run influence GDP means \n\\[\n\\lim_{h\\rightarrow\\infty}\\frac{\\partial GDP_{t+h}}{\\partial \\eta_{,t}} = \\lim_{h\\rightarrow\\infty} \\frac{\\partial}{\\partial \\eta_{,t}}\\left(\\sum_{k=1}^h \\Delta  GDP_{t+k}\\right)= 0.\n\\]can easily formulated function \\(B\\) matrices \\(\\Phi_i\\) \\(y_t\\) (including \\(\\Delta GDP_t\\)) follows VAR process.Without loss generality, consider VAR(1) case. Indeed, one can always write VAR(\\(p\\)) VAR(1). see , stack last \\(p\\) values vector \\(y_t\\) vector \\(y_{t}^{*}=[y_t',\\dots,y_{t-p+1}']'\\); Eq. (2.32) can rewritten companion form:\n\\[\\begin{equation}\ny_{t}^{*} =\n\\underbrace{\\left[\\begin{array}{c}\nc\\\\\n0\\\\\n\\vdots\\\\\n0\\end{array}\\right]}_{=c^*}+\n\\underbrace{\\left[\\begin{array}{cccc}\n\\Phi_{1} & \\Phi_{2} & \\cdots & \\Phi_{p}\\\\\n& 0 & \\cdots & 0\\\\\n0 & \\ddots & 0 & 0\\\\\n0 & 0 & & 0\\end{array}\\right]}_{=\\Phi}\ny_{t-1}^{*}+\n\\underbrace{\\left[\\begin{array}{c}\n\\varepsilon_{t}\\\\\n0\\\\\n\\vdots\\\\\n0\\end{array}\\right]}_{\\varepsilon_t^*},\\tag{2.47}\n\\end{equation}\\]\nmatrices \\(\\Phi\\) \\(\\Omega^* = \\mathbb{V}ar(\\varepsilon_t^*)\\) dimension \\(np \\times np\\); \\(\\Omega^*\\) filled zeros, except \\(n\\times n\\) upper-left block equal \\(\\Omega = \\mathbb{V}ar(\\varepsilon_t)\\). (Matrix \\(\\Phi\\) introduced Eq. (2.38).)Focusing VAR(1) case:\n\\[\\begin{eqnarray*}\ny_{t} &=& c+\\Phi y_{t-1}+\\varepsilon_{t}\\\\\n& = & c+\\varepsilon_{t}+\\Phi(c+\\varepsilon_{t-1})+\\ldots+\\Phi^{k}(c+\\varepsilon_{t-k})+\\ldots \\\\\n& = & \\mu +\\varepsilon_{t}+\\Phi\\varepsilon_{t-1}+\\ldots+\\Phi^{k}\\varepsilon_{t-k}+\\ldots \\\\\n& = & \\mu +B\\eta_{t}+\\Phi B\\eta_{t-1}+\\ldots+\\Phi^{k}B\\eta_{t-k}+\\ldots,\n\\end{eqnarray*}\\]sequence shocks \\(\\{\\eta_t\\}\\) determines sequence \\(\\{y_t\\}\\). \\(\\{\\eta_t\\}\\) replaced \\(\\{\\tilde{\\eta}_t\\}\\), \\(\\tilde{\\eta}_t=\\eta_t\\) \\(t \\ne s\\) \\(\\tilde{\\eta}_s=\\eta_s + \\gamma\\)? Assume \\(\\{\\tilde{y}_t\\}\\) associated “perturbated” sequence. \\(\\tilde{y}_t = y_t\\) \\(t<s\\). \\(t \\ge s\\), Wold decomposition \\(\\{\\tilde{y}_t\\}\\) implies:\n\\[\n\\tilde{y}_t = y_t + \\Phi^{t-s} B \\gamma.\n\\]\nTherefore, cumulative impact \\(\\gamma\\) \\(\\tilde{y}_t\\) (\\(t \\ge s\\)):\n\\[\\begin{eqnarray}\n(\\tilde{y}_t - y_t) +  (\\tilde{y}_{t-1} - y_{t-1}) + \\dots +  (\\tilde{y}_s - y_s) &=& \\nonumber \\\\\n(Id + \\Phi + \\Phi^2 + \\dots + \\Phi^{t-s}) B \\gamma.&& \\tag{2.48}\n\\end{eqnarray}\\]Consider shock \\(\\eta_{1,t}\\), magnitude \\(1\\). shock corresponds \\(\\gamma = [1,0,\\dots,0]'\\). Given Eq. (2.48), long-run cumulative effect shock endogenous variables given :\n\\[\n\\underbrace{(Id+\\Phi+\\ldots+\\Phi^{k}+\\ldots)}_{=(Id - \\Phi)^{-1}}B\\left[\\begin{array}{c}\n1\\\\\n0\\\\\n\\vdots\\\\\n0\\end{array}\\right],\n\\]\nfirst column \\(\\Theta \\equiv (Id - \\Phi)^{-1}B\\).context, consider following long-run restriction: “\\(j^{th}\\) structural shock cumulative impact \\(^{th}\\) endogenous variable”. equivalent \n\\[\n\\Theta_{ij}=0,\n\\]\n\\(\\Theta_{ij}\\) element \\((,j)\\) \\(\\Theta\\).Blanchard Quah (1989) implemented long-run restrictions small-scale VAR. Two variables considered: GDP unemployment. Consequently, VAR affected two types shocks. Specifically, authors want identify supply shocks (can permanent effect output) demand shocks (permanent effect output).2Blanchard Quah (1989)’s dataset quarterly, spanning period 1950:2 1987:4. VAR features 8 lags. data use:Estimate reduced-form VAR(8) model:Now, let us define loss function (loss) equal zero () \\(BB'=\\Omega\\) (b) element (1,1) \\(\\Theta B\\) equal zero:(Note: one can use type approach, based loss function, mix short- long-run restrictions.)Figure 2.16 displays resulting IRFs. Note , GDP, cumulate GDP growth IRF, response GDP level.\nFigure 2.16: IRF GDP unemployment demand supply shocks.\n","code":"\nlibrary(AEC)\nlibrary(vars)\ndata(\"USmonthly\")\n# Select sample period:\nFirst.date <- \"1965-01-01\";Last.date <- \"1995-06-01\"\nindic.first <- which(USmonthly$DATES==First.date)\nindic.last  <- which(USmonthly$DATES==Last.date)\nUSmonthly   <- USmonthly[indic.first:indic.last,]\nconsidered.variables <- c(\"LIP\",\"UNEMP\",\"LCPI\",\"LPCOM\",\"FFR\",\"NBR\",\"TTR\",\"M1\")\ny <- as.matrix(USmonthly[considered.variables])\nres.svar.ordering <- svar.ordering(y,p=3,\n                                   posit.of.shock = 5,\n                                   nb.periods.IRF = 20,\n                                   nb.bootstrap.replications = 100,\n                                   confidence.interval = 0.90, # expressed in pp.\n                                   indic.plot = 1 # Plots are displayed if = 1.\n)\nlibrary(AEC)\ndata(BQ)\npar(mfrow=c(1,2))\nplot(BQ$Date,BQ$Dgdp,type=\"l\",main=\"GDP quarterly growth rate\",\n     xlab=\"\",ylab=\"\",lwd=2)\nplot(BQ$Date,BQ$unemp,type=\"l\",ylim=c(-3,6),main=\"Unemployment rate (gap)\",\n     xlab=\"\",ylab=\"\",lwd=2)\nlibrary(vars)\ny <- BQ[,2:3]\nest.VAR <- VAR(y,p=8)\nOmega <- var(residuals(est.VAR))\n# Compute (Id - Phi)^{-1}:\nPhi <- Acoef(est.VAR)\nPHI <- make.PHI(Phi)\nsum.PHI.k <- solve(diag(dim(PHI)[1]) - PHI)[1:2,1:2]\nloss <- function(param){\n  B <- matrix(param,2,2)\n  X <- Omega - B %*% t(B)\n  Theta <- sum.PHI.k[1:2,1:2] %*% B\n  loss <- 10000 * ( X[1,1]^2 + X[2,1]^2 + X[2,2]^2 + Theta[1,1]^2 )\n  return(loss)\n}\nres.opt <- optim(c(1,0,0,1),loss,method=\"BFGS\",hessian=FALSE)\nprint(res.opt$par)## [1]  0.8570358 -0.2396345  0.1541395  0.1921221\nB.hat <- matrix(res.opt$par,2,2)\nprint(cbind(Omega,B.hat %*% t(B.hat)))##             Dgdp       unemp                       \n## Dgdp   0.7582704 -0.17576173  0.7582694 -0.17576173\n## unemp -0.1757617  0.09433658 -0.1757617  0.09433558\nnb.sim <- 40\npar(mfrow=c(2,2));par(plt=c(.15,.95,.15,.8))\nY <- simul.VAR(c=matrix(0,2,1),Phi,B.hat,nb.sim,y0.star=rep(0,2*8),\n               indic.IRF = 1,u.shock = c(1,0))\nplot(cumsum(Y[,1]),type=\"l\",lwd=2,xlab=\"\",ylab=\"\",main=\"Demand shock on GDP\")\nplot(Y[,2],type=\"l\",lwd=2,xlab=\"\",ylab=\"\",main=\"Demand shock on UNEMP\")\nY <- simul.VAR(c=matrix(0,2,1),Phi,B.hat,nb.sim,y0.star=rep(0,2*8),\n               indic.IRF = 1,u.shock = c(0,1))\nplot(cumsum(Y[,1]),type=\"l\",lwd=2,xlab=\"\",ylab=\"\",main=\"Supply shock on GDP\")\nplot(Y[,2],type=\"l\",lwd=2,xlab=\"\",ylab=\"\",main=\"Supply shock on UNEMP\")"},{"path":"TS.html","id":"Signs","chapter":"2 Time Series","heading":"2.3.7 Sign restrictions","text":"identifiy structural shocks, need find matrix \\(B\\) satisfies \\(\\Omega = BB'\\) (\\(\\Omega = \\mathbb{V}ar(\\varepsilon_t)\\)) restrictions. Indeed, explained , \\(\\Omega = BB'\\) sufficient identify \\(B\\) since, take orthogonal matrix \\(Q\\) (see Def. 2.23), \\(\\mathcal{P}=BQ\\) also satisfies \\(\\Omega = \\mathcal{P}\\mathcal{P}'\\).Definition 2.23  (Orthogonal matrix) orthogonal matrix \\(Q\\) matrix \\(QQ' = ,\\) .e., columns (rows) \\(Q\\) \northogonal unit vectors:\n\\[q_i'q_j=0\\text{ }\\neq j\\text{ }q_i'q_j=1\\text{ }= j,\\]\n\\(q_i\\) \\(^{th}\\) column \\(Q\\).idea behind sign-restriction approach “draw” random matrices \\(\\mathcal{P}\\) satisfy \\(\\Omega = \\mathcal{P}\\mathcal{P}'\\), constitute set admissible matrices, keeping set simulated \\(\\mathcal{P}\\) matrices satisfy predefined sign-based restriction. example restriction “one year, contractionary monetary-policy shocks negative impact inflation”.suggested , \\(B\\) matrix satisfies \\(\\Omega = BB'\\) (instance, \\(B\\) can based Cholesky decomposition \\(\\Omega\\)), also \\(\\Omega = \\mathcal{P}\\mathcal{P}'\\) soon \\(\\mathcal{P}=BQ\\), \\(Q\\) orthogonal matrix. Therefore, draw \\(\\mathcal{P}\\) matrices, suffices draw set orthogonal matrices.fix ideas, consider dimension 2. case, orthogonal matrices rotation matrices, set orthogonal matrices can parameterized angle \\(x\\), :\n\\[\nQ_x=\\begin{pmatrix}\\cos(x)&\\cos\\left(x+\\frac{\\pi}{2}\\right)\\\\\n\\sin(x)&\\sin\\left(x+\\frac{\\pi}{2}\\right)\\end{pmatrix}=\\begin{pmatrix}\\cos(x)&-\\sin(x)\\\\\n\\sin(x)&\\cos(x)\\end{pmatrix}.\n\\]\n(angle-\\(x\\) counter-clockwise rotation.) Hence, case, drawing \\(x\\) randomly \\([0,2\\pi]\\), draw randomly set \\(2\\times2\\) rotation matrices. high-dimensional VAR, lose simple geometrical representation, though. always possible parametrize rotation matrix (high-dimentional VARs).proceed, ? Arias, Rubio-Ramírez, Waggoner (2018) provide procedure. approach based -called \\(QR\\) decomposition: square matrix \\(X\\) may decomposed \\(X=QR\\) \\(Q\\) orthogonal matrix \\(R\\) upper diagonal matrix. mind, propose two-step approach:Draw random matrix \\(X\\) drawing element independent standard normal distribution.Let \\(X = QR\\) \\(QR\\) decomposition \\(X\\) diagonal \\(R\\) normalized \npositive. random matrix \\(Q\\) orthogonal draw uniform distribution set orthogonal matrices.Equipped procedure, sign-restriction based following algorithm:Draw random orthogonal matrix \\(Q\\) (using step . ii. described ).Compute \\(B = PQ\\) \\(P\\) Cholesky decomposition reduced form residuals \\(\\Omega_{\\varepsilon}\\).Compute impulse response associated \\(B\\) \\(y_{t,t+k}=\\Phi^kB\\) cumulated response \\(\\bar y_{t,t+k}=\\sum_{j=0}^{k}\\Phi^jB\\).sign restrictions satisfied?Yes. Store impulse response set admissible response.. Discard impulse response.Perform \\(N\\) replications report median impulse response (“confidence” intervals).Note: take account uncertainty \\(B\\) \\(\\Phi\\), can draw \\(B\\) \\(\\Phi\\) Steps 2 3 using inference method (see Section 2.3.12).sign-restriction approach method advantage relatively agnostic. Moreover, fairly flexible, one can impose sign restrictions variable, horizon. prominent example Uhlig (2005). Using US monthly data 1965.2003.XII, employs sign restrictions estimate effect monetary policy shocks.According conventional wisdom, monetary contractions :3Raise federal funds rate,Lower prices,Decrease non-borrowed reserves,Reduce real output.restricitons considered Uhlig (2005) follows: expansionary monetary policy shock leads :Increases pricesIncrease nonborrowed reservesDecreases federal funds rateWhat output? Since response interest, leave un-restricted.\nFigure 2.17: IRF associated monetary policy shock; sign-restriction approach.\nstressed sign restriction approach lead unique IRF, set admissible IRFs. Also, say approach set-identified, point-identified.alternative approach -called penalty-function approach (PFA, Uhlig (2005), present Danne (2015)’s package). approach relies penalty function:\n\\[\n\\begin{array}{llll}f(x)&=&x&\\text{ }x\\le0\\\\\n&&100.x&\\text{ }x>0\\end{array}\n\\]\npenalizes positive responses rewards negative responses.Let \\(\\psi_k^j(q)\\) impulse response variable \\(j\\). \\(\\psi_k^j(q)\\)’s elements \\(\\psi_k(q)=\\Psi_kq\\).Let \\(\\sigma_j\\) standard deviation variable \\(j\\). Let \\(\\iota_{j,k}=1\\) restrict response variable \\(j\\) \\(k^th\\) horizon negative, \\(\\iota_{j,k}=-1\\) restrict positive, \\(\\iota_{j,k}=0\\) restriction. total penalty given \\[\n\\mathbf{P}(q)=\\sum_{j=1}^m\\sum_{k=0}^Kf\\left(\\iota_{j,k}\\frac{\\psi_k^j(q)}{\\sigma_j}\\right).\n\\]looking solution \n\\[\\begin{array}{ll}&\\min_q \\mathbf{P}(q)\\\\\n&\\\\\n\\text{s.t. }&q'q=1.\\end{array}\\]problem solved numerically.","code":"\nlibrary(AEC);library(vars);library(Matrix)\ndata(\"USmonthly\")\nFirst.date <- \"1965-01-01\"\nLast.date <- \"1995-06-01\"\nindic.first <- which(USmonthly$DATES==First.date)\nindic.last  <- which(USmonthly$DATES==Last.date)\nUSmonthly   <- USmonthly[indic.first:indic.last,]\nconsidered.variables<-c(\"LIP\",\"UNEMP\",\"LCPI\",\"LPCOM\",\"FFR\",\"NBR\",\"TTR\",\"M1\")\nn <- length(considered.variables)\ny <- as.matrix(USmonthly[considered.variables])\nsign.restrictions <- list()\nhorizon <- list()\n#Define sign restrictions and horizon for restrictions\nfor(i in 1:n){\n  sign.restrictions[[i]] <- matrix(0,n,n)\n  horizon[[i]] <- 1\n}\nsign.restrictions[[1]][1,3] <- 1\nsign.restrictions[[1]][2,5] <- -1\nsign.restrictions[[1]][3,6] <- 1\nhorizon[[1]] <- 1:5\nres.svar.signs <- \n  svar.signs(y,p=3,\n             nb.shocks = 1, #number of identified shocks\n             nb.periods.IRF = 20,\n             bootstrap.replications = 1, # = 0 if no bootstrap\n             confidence.interval = 0.80, # expressed in pp.\n             indic.plot = 1, # Plots are displayed if = 1.\n             nb.draws = 10000, # number of draws\n             sign.restrictions,\n             horizon,\n             recursive =1 #  =0 <- draw Q directly, =1 <- draw q recursively\n  )"},{"path":"TS.html","id":"forecast-error-variance-maximization","chapter":"2 Time Series","heading":"2.3.8 Forecast error variance maximization","text":"approach presented section exploits derivations Uhlig (2004). Barsky Sims (2011) exploit approach identify TFP news shock, define shock () orthogonal innovation current utilization-adjusted TFP (b) best explains variation future TFP.Consider process \\(\\{y_t\\}\\) admits infinite MA representation Eq. (2.34). Let \\(Q\\) orthogonal matrix, alternative decomposition :\n\\[\\begin{eqnarray}\ny_t&=&\\sum_{h=0}^{+\\infty}\\Psi_h\\underbrace{\\eta_{t-h}}_{Q\\tilde \\eta_{t-h}} = \\sum_{h=0}^{+\\infty}\\underbrace{\\Psi_hQ}_{\\tilde\\Psi_h}\\tilde\n\\eta_{t-h} = \\sum_{h=0}^{+\\infty}\\tilde\\Psi_h\\tilde \\eta_{t-h},\n\\end{eqnarray}\\]\n\\(\\tilde \\eta_{t-h}=Q'\\eta_{t-h}\\) white-noise shocks associated new MA representation. (also satisfy \\(\\mathbb{V}ar(\\tilde\\eta_t)=Id\\).)\\(h\\)-step ahead prediction error \\(y_{t+h}\\), given data including \\(t-1\\) given \n\\[\ne_{t+h}(h)=y_{t+h}-\\mathbb{E}_{t-1}(y_{t+h})=\\sum_{j=0}^h\\tilde \\Psi_h\\tilde \\eta_{t+h-j}.\n\\]variance-covariance matrix \\(e_{t+h}(h)\\) \n\\[\n\\Omega(h)=\\sum_{j=0}^h\\tilde \\Psi_j\\tilde \\Psi_j'=\\sum_{j=0}^h \\Psi_j \\Psi_j'.\n\\]can decompose \\(\\Omega(h)\\) contribution shock \\(l\\) (\\(l^{th}\\) component \\(\\tilde{\\eta}_t\\)):\n\\[\n\\Omega^{(h)}=\\sum_{l=1}^n\\Omega_l^{(h)}(Q)\n\\]\n\n\\[\n\\Omega_l^{(h)}(Q) =\\sum_{j=0}^h(\\Psi_jq_l)(\\Psi_jq_l)',\n\\]\n\\(q_l\\) \\(l^{th}\\) column \\(Q\\).decomposition can used objective finding impulse vector \\(b\\) s.t. explains much possible sum \\(h\\)-step ahead prediction error variance variable \\(\\), say, prediction horizons \\(h \\[\\underline{h} , \\overline{h}]\\).Formally, task explain much possible variance\n\\[\n\\sigma^2(\\underline{h},\\overline{h},q_1)=\\sum_{h=\\underline{h}}^{\\overline{h}} \\sum_{j=0}^h\\left[(\\Psi_jq_1)(\\Psi_jq_1)'\\right]_{,}\n\\]\nsingle impulse vector \\(q_1\\).Denote \\(E_{ii}\\) matrix filled zeros, except (\\(,\\)) entry, set 1. :\n\\[\\begin{eqnarray*}\n\\sigma^2(\\underline{h},\\overline{h},q_1)&=&\\sum_{h=\\underline{h}}^{\\overline{h}} \\sum_{j=0}^h\\left[(\\Psi_jq_1)(\\Psi_jq_1)'\\right]_{,}=\\sum_{h=\\underline{h}}^{\\overline{h}} \\sum_{j=0}^h Tr\\left[E_{ii}(\\Psi_jq_1)(\\Psi_jq_1)'\\right]\\\\\n&=&\\sum_{h=\\underline{h}}^{\\overline{h}} \\sum_{j=0}^h Tr\\left[q_1'\\Psi_j'E_{ii}\\Psi_j q_1\\right]\\\\\n&=& q_1'Sq_1,\n\\end{eqnarray*}\\]\n\n\\[\\begin{eqnarray*}\n\\begin{array}{lll}S&=&\\sum_{h=\\underline{h}}^{\\overline{h}}\\sum_{j=0}^{h}\\Psi_j'E_{ii}\\Psi_j\\\\\n&=&\\sum_{j=0}^{\\overline{h}}(\\overline{h}+1-max(\\underline{h},j))\\Psi_j'E_{ii}\\Psi_j\\\\\n&=&\\sum_{j=0}^{\\overline{h}}(\\overline{h}+1-max(\\underline{h},j))\\Psi_{j,}'\\Psi_{j,}\\\\\n\\end{array}\n\\end{eqnarray*}\\]\n\\(\\Psi_{j,}\\) denotes row \\(\\) \\(\\Psi_{j}\\), .e., response variable \\(\\) horizon \\(j\\) (\\(Q=Id\\)).maximization problem subject side constraint \\(q_1'q_1=1\\) can written Lagrangian: \\[\nL=q_1'Sq_1-\\lambda(q_1'q_1-1),\n\\]\nfirst-order condition \\(Sq_1=\\lambda q_1\\) (side constraint \\(q_1'q_1=1\\)). equation, see solution \\(q_1\\) eigenvector \\(S\\), one associated eigenvalue \\(\\lambda\\). also see \\(\\sigma^2(\\underline{h},\\overline{h},q_1)=\\lambda\\). Thus, maximize variance, need find eigenvector \\(S\\) associated maximal eigenvalue \\(\\lambda\\). defines first principal component (see Section 3.1). , \\(S\\) admits following spectral decomposition:\n\\[\nS = \\mathcal{P}D\\mathcal{P}',\n\\]\n\\(D\\) diagonal matrix whose entries (ordered) eigenvalues: \\(\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_n \\ge 0\\), \\(\\sigma^2(\\underline{h},\\overline{h},q_1)\\) maximized \\(q_1 = p_1\\), \\(p_1\\) first column \\(\\mathcal{P}\\).","code":""},{"path":"TS.html","id":"NonGaussian","chapter":"2 Time Series","heading":"2.3.9 Identification based on non-normality of the shocks","text":"section, show non-identification structural shocks (\\(\\eta_t\\)) specific Gaussian case. propose consistent estimation approaches SVAR context non-Gaussian shocks.seen precedes identify \\(B\\) based first second moments . Since Gaussian distribution perfectly determined first two moments, comes one achieve identification structural shocks Gaussian. , even observe infinite number ..d. \\(B \\eta_t\\), recover \\(B\\) \\(\\eta_t\\)’s Gaussian.Indeed, \\(\\eta_t \\sim \\mathcal{N}(0,Id)\\), distribution \\(\\varepsilon_t \\equiv B \\eta_t\\) \\(\\mathcal{N}(0,BB')\\). Hence \\(\\Omega = B B'\\) observed (population), orthogonal matrix \\(Q\\) (.e. \\(QQ'=Id\\)), also \\(BQ \\eta_t \\sim \\mathcal{N}(0,\\Omega)\\).illustrate, consider following bivariate Gaussian situations, \\(\\Theta_1=0\\)):\\(\\left[\\begin{array}{c}\\eta_{1,t}\\\\ \\eta_{2,t}\\end{array}\\right]\\sim \\mathcal{N}(0,Id)\\), \n\\(B = \\left[\\begin{array}{cc} 1 & 2 \\\\ -1 & 1 \\end{array}\\right]\\) \n\\(Q = \\left[\\begin{array}{cc} \\cos(\\pi/3) & -\\sin(\\pi/3) \\\\ \\sin(\\pi/3) & \\cos(\\pi/3) \\end{array}\\right]\\) (rotation).Figure 2.18 shows distributions \\(B \\eta_t\\) \\(BQ\\eta_t\\) identical. However, impulse response functions associated one impulse matrix (\\(B\\) \\(BQ\\)) different. illustrated Figure 2.19, shows IRFs associated two identical models (defined Eq. (2.35)), difference impulse matrix (\\(B\\) \\(BQ\\)).\nFigure 2.18: figure compares distributions two Gaussian bivariate vectors, \\(B \\eta_t\\) \\(BQ\\eta_t\\), \\(\\eta_{t} \\sim \\mathcal{N}(0,Id)\\) (therefore \\(\\eta_{1,t}\\) \\(\\eta_{2,t}\\) independent), \\(Q\\) orthogonal matrix.\n\nFigure 2.19: figure shows impulse response functions associated impulse matrix equal \\(B\\) (black line) \\(BQ\\) (red line) different (even \\(BB'=BQ(BQ)'\\)).\nHence, Gaussian case, external restrictions (economic hypotheses) needed identify \\(B\\) (see previous sections). restrictions may necessary structural shocks Gaussian. , identification problem specific normally-distributed \\(\\eta_t\\)’s (Rigobon (2003), Normandin Phaneuf (2004), Lanne Lütkepohl (2008)).better see can case, consider bivariate vector independent structural shocks (\\(\\eta_{1,t}\\) \\(\\eta_{2,t}\\)) , now, assume one Gaussian . Specifically, assume \\(\\eta_{2,t}\\) drawn Student distribution 5 degrees freedom:\n\\(\\eta_{1,t} \\sim \\mathcal{N}(0,1)\\), \\(\\eta_{2,t} \\sim t(5)\\),\n\\(B = \\left[\\begin{array}{cc} 1 & 2 \\\\ -1 & 1 \\end{array}\\right]\\) \n\\(Q = \\left[\\begin{array}{cc} \\cos(\\pi/3) & -\\sin(\\pi/3) \\\\ \\sin(\\pi/3) & \\cos(\\pi/3) \\end{array}\\right]\\).Figure 2.20 shows , case, \\(B \\eta_t\\) \\(BQ\\eta_t\\) distribution (spite fact , cases, \\(\\mathbb{V}ar(\\varepsilon_t)=BB'\\)). opens door identification impulse matrix (\\(BQ\\)) non-Gaussian case.\nFigure 2.20: figure compares distributions two Gaussian bivariate vectors, \\(B \\eta_t\\) \\(BQ\\eta_t\\), \\(\\eta_t{1,t} \\sim \\mathcal{N}(0,1)\\), \\(\\eta_t{2,t} \\sim t(5)\\), \\(Q\\) orthogonal matrix.\nexercise consists identifying non-Gaussian independent shocks linear combinations shocks well-known problem signal-processingliterature, called independent component analysis (ICA). Without loss generality, can assume \\(BB' = Id\\) (.e. \\(B\\) orthogonal). (case, .e. \\(\\mathbb{V}ar(\\varepsilon_t)=\\Omega \\ne Id\\), one can pre-multiply data \\(\\Omega^{-1/2}\\).) classical ICA problem follows: Find \\(B\\) \\(\\varepsilon_t = B \\eta_t\\) ($_t= B’ _t $) given thatWe observe \\(\\varepsilon_t\\)’s,components \\(\\eta_t\\) independent,\\(BB'=Id\\) (.e., \\(B\\) orthogonal).Figure 2.21 represents bivariate distributions. black (red) lines correspond distributions \\(\\eta_t\\) (\\(B\\eta_t\\)). important note two components vector \\(B \\eta_t\\) independent (contrary \\(\\eta_t\\)).\nFigure 2.21: three plots represent bivariate distributions \\(\\eta_t\\) (black) \\(B\\eta_t\\) (red), two components \\(\\eta_t\\) independent, unit variance, \\(B\\) orthogonal. Hence, three plots, \\(\\mathbb{V}ar(B\\eta_t)=Id\\).\ncases, \\(\\mathbb{V}ar(\\varepsilon_t)=\\mathbb{V}ar(\\eta_t)=Id\\). two components \\(\\varepsilon_t\\) independent. instance: \\(\\mathbb{E}(\\varepsilon_{2,t}|\\varepsilon_{1,t}>4)<0\\) (whereas \\(\\mathbb{E}(\\eta_{2,t}|\\eta_{1,t}>4)=0\\)). objctive ICA rotate \\(\\varepsilon_t\\) retrieve independent components (\\(\\eta_t\\)).Hypothesis 2.1  Process \\(\\eta_t\\) satisfies:\\(\\eta_t\\)’s ..d. (across time) \\(\\mathbb{E}(\\eta_t) = 0\\) \\(\\mathbb{V}ar(\\eta_t) = Id.\\)components \\(\\eta_{1,t}, \\ldots, \\eta_{n,t}\\) mutually independent.\niii \n\\[\n\\varepsilon_t = B_0 \\eta_t,\n\\]\n\\(\\mathbb{V}ar(\\varepsilon_t) = Id\\) (.e. \\(B_0\\) orthogonal).Theorem 2.4  (Eriksson, Koivunen (2004)) Hypothesis 2.1 satisfied one components \\(\\eta\\) Gaussian, matrix \\(B_0\\) identifiable post multiplication \\(DP\\), \\(P\\) permutation matrix \\(D\\) diagonal matrix whose diagonal entries 1 \\(-1\\).}Hence, structural shocks identifiable. estimate based observations \\(\\varepsilon_t\\)’s? Gouriéroux, Monfort, Renne (2017) proposed Pseudo-Maximum Likelihood (PML) approach. approach consists maximizing -called pseudo log-likelihood function, based set p.d.f. \\(g_i (\\eta_i), =1,\\ldots,n\\) (may different true p.d.f. \\(\\eta_{,t}\\)’s):\n\\[\\begin{equation}\n\\log \\mathcal{L}_T (B) = \\sum^T_{t=1} \\sum^n_{=1} \\log g_i (b'_i Y_t),\\tag{2.49}\n\\end{equation}\\]\n\\(b_i\\) \\(^{th}\\) column matrix \\(B\\) (\\(b'_i\\) \\(^{th}\\) row \\(B^{-1}\\) since \\(B^{-1}=B'\\)).restrictions \\(B'B = Id\\) can eliminated parameterizing \\(B\\) way , whatever consider parameters, \\(B\\) orthogonal. Gouriéroux, Monfort, Renne (2017) propose use, , Cayley’s representation: orthogonal matrix eigenvalue equal \\(-1\\) can written \n\\[\\begin{equation}\nB() = (Id+) (Id-)^{-1},\n\\end{equation}\\]\n\\(\\) skew symmetric (antisymmetric) matrix, \\('=-\\). one--one relationship \\(\\), since:\n\\[\\begin{equation}\n= (B()+Id)^{-1} (B()-Id).\n\\end{equation}\\]Hence, PML estimator matrix \\(B\\) obtained \\(\\widehat{B_T} = B(\\hat{}_T),\\) :\n\\[\\begin{equation}\n\\hat{}_T = \\arg \\max_{a_{,j}, >j} \\sum^T_{t=1} \\sum^n_{=1} \\log g_i [b_i ()' \\varepsilon_t].\\tag{2.51}\n\\end{equation}\\]Table 2.1:  table reports usual p.d.f. derivatives.assumptions \\(g_i\\) functions (excluding Gaussian distributions), Gouriéroux, Monfort, Renne (2017) derive asymptotic properties PML estimator. Specifically, PML estimator \\(\\widehat{B_T}\\) \\(B_0\\) consistent (\\(\\mathcal{P}_0\\), set matrices obtained permutation sign change columns \\(B_0\\)) asymptotically normal, speed convergence \\(1/\\sqrt{T}\\).asymptotic variance-covariance matrix \\(vec \\sqrt{T} (\\widehat{B_T} - B_0)\\) \\(^{-1} \\left[\\begin{array}{cc} \\Gamma & 0 \\\\ 0 & 0 \\end{array} \\right] (')^{-1}\\), matrices \\(\\) \\(\\Gamma\\) detailed Gouriéroux, Monfort, Renne (2017).Note potential misspecification pseudo-distributions \\(g_i\\) effect consistency specific PML estimators.Example 2.8  (Non-Gaussian monetary-policy shocks) apply PML-ICA approach U.S. data coerving period 1959:IV 2015:quarterly frequency (\\(T=224\\)). consider three dependent variables: inflation (\\(\\pi_t\\)), economic activity (\\(z_t\\), output gap) nominal short-term interest rate (\\(r_t\\)). Changes log oil prices added exogenous variable (\\(x_t\\)).Let us denote \\(W_t\\) set information made past values \\(y_t= [\\pi_t,z_t,r_t]\\), \\(\\{y_{t-1},y_{t-2},\\dots\\}\\), exogenous variables \\(\\{x_{t},x_{t-1},\\dots\\}\\). reduced-form VAR model reads:\n\\[\ny_t  = \\underbrace{\\mu + \\sum_{=1}^{p} \\Phi_i y_{t-} + \\Theta x_t}_{(W_t;\\theta)} + u_t\n\\]\n\\(u_t\\)’s assumed serially independent, zero mean variance-covariance matrix \\(\\Sigma\\).Matrices \\(\\mu\\), \\(\\Phi_i\\), \\(\\Theta\\) \\(\\Sigma\\) consistently estimated OLS. Jarque-Bera tests support hypothesis non-normality residuals.want estimate orthogonal matrix \\(B\\) \\(u_t=SB \\eta_t\\), \\(S\\) results Cholesky decomposition \\(\\Sigma\\) andthe components \\(\\eta_t\\) independent, zero-mean unit variance.PML approach applied standardized VAR residuals given :\n\\[\n\\hat\\varepsilon_t = \\hat{S}_T^{-1}\\underbrace{[y_t - (W_t;\\hat\\theta_T)]}_{\\mbox{VAR residuals}}.\n\\]\nconstruction \\(\\hat{S}_T^{-1}\\), comes covariance matrix residuals \\(Id\\).pseudo density functions distinct asymmetric mixtures Gaussian distributions.(Note: always useful combine two optimization algorithms, Nelder-Mead BFGS.)obtain close results neglecting commodity prices. case, one can simply use function estim.SVAR.ICA AEC package. Let us compare \\(C\\) matrix obtained two cases (without commodity prices):\\(B\\) estimated, remains label resulting structural shocks (components \\(\\eta_{t}\\)). Postulated shocks monetary-policy, supply, demand shocks. labelling can based following considerations:Contractionary monetary-policy shocks negative impact real activity inflation.Supply shock influences opposite signs economic activity inflation.Demand shock influences signs economic activity inflation.Let us compute IRFs associated three structural shocks. (sake comparison, first line plots shows IRFs monetary-policy shock obtained Cholesky-based approach short-term rate ordered last.)\nFigure 2.22: first row plots shows responses three endogenous variables monetary policy shock context Cholesky-idendtified SVAR (ordering: inflation, output gap, interest rate). next three rows plots show repsonses endogenous variables three structural shocks identified ICA. last one (Shock 3) close Cholesky-identified monetary policy shock.\nAccording Figure 2.22, Shock 1 supply shock, Shock 2 demand shock, Shock 3 monetary-policy shock. Note Shock 3 close one resulting Cholesky approach.Relation Heteroskedasticity IdentificationIn cases, \\(\\varepsilon_t\\)’s heteroskedastic, \\(B\\) matrix can identified (Rigobon (2003), Lanne, Lütkepohl, Maciejowska (2010)).Consider case still \\(\\varepsilon_t = B \\eta_t\\) \\(\\eta_t\\)’s variance conditionally depends regime \\(s_t \\\\{1,\\dots,M\\}\\). :\n\\[\n\\mathbb{V}ar(\\eta_{k,t}|s_t) = \\lambda_{s_t,k} \\quad \\mbox{} k \\\\{1,\\dots,n\\}\n\\]Denoting \\(\\Lambda_i\\) diagonal matrix whose diagonal entries \\(\\lambda_{,k}\\)’s, comes :\n\\[\n\\mathbb{V}ar(\\eta_{t}|s_t) = \\Lambda_{s_t},\\quad \\mbox{}\\quad \\mathbb{V}ar(\\varepsilon_{t}|s_t) = B\\Lambda_{s_t}B'.\n\\]Without loss generality, can assumed \\(\\Lambda_1=Id\\).context, \\(B\\) identified, apart sign reversal columns \\(k \\ne j \\\\{1,\\dots,n\\}\\), regime \\(\\) s.t. \\(\\lambda_{,k} \\ne \\lambda_{,j}\\). (Prop.1 @Lanne, Lütkepohl, Maciejowska (2010)).Bivariate regime case (\\(M=2\\)): \\(B\\) identified \\(\\lambda_{2,k}\\)’s different. , identification ensured “sufficient heterogeneity volatility changes” (Lütkepohl Netšunajev (2017)).regimes \\(s_t\\) exogenous serially independent, situation consistent “non-Gaussian” situation described .","code":"\nlibrary(AEC)\nFirst.date <- \"1959-04-01\"\nLast.date  <- \"2015-01-01\"\ndata <- US3var\ndata <- data[(data$Date>=First.date)&(data$Date<=Last.date),]\nY <- as.matrix(data[c(\"infl\",\"y.gdp.gap\",\"r\")])\nnames.var <- c(\"inflation\",\"real activity\",\"short-term rate\")\nT <- dim(Y)[1]\nn <- dim(Y)[2]\nnb.lags <- 6 # number of lags used in the VAR model\nX <- NULL\nfor(i in 1:nb.lags){\n  lagged.Y <- rbind(matrix(NaN,i,n),Y[1:(T-i),])\n  X <- cbind(X,lagged.Y)}\nX <- cbind(X,data$commo) # add exogenous variables\nPhi <- matrix(0,n,n*nb.lags);mu <- rep(0,n)\neffect.commo <- rep(0,n)\nU <- NULL # Eta is the matrix of OLS residuals\nfor(i in 1:n){\n  eq <- lm(Y[,i] ~ X)\n  Phi[i,] <- eq$coef[2:(dim(Phi)[2]+1)]\n  mu[i] <- eq$coef[1]\n  U <- cbind(U,eq$residuals)\n  effect.commo[i] <- eq$coef[length(eq$coef)]\n}\nOmega <- var(U) # Covariance matrix of the OLS residuals.\nB <- t(chol(Omega)) # Cholesky matrix associated with Omega (lower triang.)\nEps <- U %*% t(solve(B)) # Recover associated structural shocks\ndistri <- list(\n  type=c(\"mixt.gaussian\",\"mixt.gaussian\",\"mixt.gaussian\"),\n  df=c(NaN,NaN,NaN),\n  p=c(0.5,.5,.5),mu=c(.1,.1,.1),sigma=c(.5,.7,1.3))\nAA.0 <- c(0,0,0)\nres.optim <- optim(AA.0,func.2.minimize,\n                   Y = Eps, distri = distri,\n                   gr = d.func.2.minimize,\n                   method=\"Nelder-Mead\",\n                   control=list(trace=FALSE,maxit=1000))\nAA.0 <- res.optim$par\nres.optim <- optim(AA.0,func.2.minimize,d.func.2.minimize,\n                   Y = Eps, distri = distri,\n                   method=\"BFGS\",\n                   control=list(trace=FALSE))\nAA.est <- res.optim$par\nn <- ncol(Y)\nM <- make.M(n)\nA.est <- matrix(M %*% AA.est,n,n)\nC.PML <- (diag(n) + A.est) %*% solve(diag(n) - A.est)\n\neta.PML <- Eps %*% C.PML # eta.PML are the ICA-estimated structural shocks\n\nA <- make.A.matrix(eta.PML,distri,C.PML)\nOmega <- make.Omega(eta.PML,distri)\n# Compute asymptotic covariance matrix of C.PML:\nV <- make.Asympt.Cov.delta(eta.PML,distri,C.PML)\nparam <- c(C.PML)\nst.dev <- sqrt(diag(V))\nt.stat <- c(C.PML)/sqrt(diag(V))\ncbind(param,st.dev,t.stat) # print results of PML estimation##             param      st.dev      t.stat\n##  [1,]  0.94417705 0.040848382  23.1141845\n##  [2,] -0.32711569 0.118802653  -2.7534376\n##  [3,]  0.03905164 0.074172945   0.5264944\n##  [4,]  0.32070293 0.119270893   2.6888616\n##  [5,]  0.93977707 0.041629110  22.5749976\n##  [6,]  0.11818924 0.060821400   1.9432179\n##  [7,] -0.07536139 0.071980455  -1.0469702\n##  [8,] -0.09906759 0.062185577  -1.5930959\n##  [9,]  0.99222290 0.007785691 127.4418551\nICA.res.no.commo <- estim.SVAR.ICA(Y,distri = distri,p=6)\nround(cbind(ICA.res.no.commo$C.PML,NaN,C.PML),3)##        [,1]  [,2]   [,3] [,4]   [,5]  [,6]   [,7]\n## [1,]  0.956 0.287 -0.059  NaN  0.944 0.321 -0.075\n## [2,] -0.292 0.950 -0.108  NaN -0.327 0.940 -0.099\n## [3,]  0.025 0.121  0.992  NaN  0.039 0.118  0.992\nIRF.Chol <- array(NaN,c(n,41,n))\nIRF.ICA  <- array(NaN,c(n,41,n))\nPHI <- list();for(i in 1:nb.lags){PHI[[i]]<-array(Phi,c(3,3,nb.lags))[,,i]}\nfor(jjjj in 1:n){\n  u.shock <- rep(0,n)\n  u.shock[jjjj] <- 1\n  IRF.Chol[,,jjjj] <- \n    t(simul.VAR(c=rep(0,3),Phi=PHI,B=B,nb.sim=41,\n                y0.star=rep(0,3*nb.lags),indic.IRF = 1,u.shock = u.shock))\n  IRF.ICA[,,jjjj]  <- \n    t(simul.VAR(c=rep(0,3),Phi=PHI,B=B%*%C.PML,nb.sim=41,\n                y0.star=rep(0,3*nb.lags),indic.IRF = 1,u.shock = u.shock))\n}"},{"path":"TS.html","id":"factor-augmented-var-favar","chapter":"2 Time Series","heading":"2.3.10 Factor-Augmented VAR (FAVAR)","text":"VAR models subject curse dimensionality: \\(n\\), large, number parameters (\\(n^2\\)) explodes.case one suspects \\(y_{,t}\\)’s mainly driven small number random sources, factor structure may imposed, principal component analysis (PCA, see Appendix 3.1) can employed estimate relevant factors (Bernanke, Boivin, Eliasz (2005)).Let us denote \\(F_t\\) \\(k\\)-dimensional vector latent factors accounting important shares variances \\(y_{,t}\\)’s (\\(K \\ll n\\)) \\(x_t\\) small \\(M\\)-dimensional subset \\(y_t\\) (\\(M \\ll n\\)). following factor structure posited:\n\\[\ny_t = \\Lambda^f F_t + \\Lambda^x x_t + e_t,\n\\]\n\\(e_t\\) “small” serially mutually ..d. error terms. \\(F_t\\) \\(x_t\\) supposed drive fluctuations \\(y_t\\)’s components.model complemented positing VAR dynamics \\([F_t',x_t']'\\):\n\\[\\begin{equation}\n\\left[\\begin{array}{c}F_t\\\\x_t\\end{array}\\right] = \\Phi(L)\\left[\\begin{array}{c}F_{t-1}\\\\ x_{t-1}\\end{array}\\right] + v_t.\\tag{2.52}\n\\end{equation}\\]Standard identification techniques structural shocks can employed Eq. (2.52): Cholesky approach can used instance last component \\(x_t\\) short-term interest rate assumed MP shock contemporaneous impact macro-variables (\\(x_t\\)).identification procedure, Bernanke, Boivin, Eliasz (2005) exploit fact macro-finance variables can decomposed two sets —fast-moving slow-moving variables— former reacts contemporaneously monetary-policy shocks. Now, estimate (unobserved) factors \\(F_t\\)? Bernanke, Boivin, Eliasz (2005) note first \\(K+M\\) PCA whole dataset (\\(y_t\\)), denote \\(\\hat{C}(F_t,x_t)\\) span space \\(F_t\\) \\(x_t)\\). get estimate \\(F_t\\), dependence \\(\\hat{C}(F_t,x_t)\\) \\(x_t)\\) removed. done regressing, OLS, \\(\\hat{C}(F_t,x_t)\\) \\(x_t)\\) \\(\\hat{C}^*(F_t)\\), latter estimate common components \\(x_t\\). proxy \\(\\hat{C}^*(F_t)\\), Bernanke, Boivin, Eliasz (2005) take principal components set slow-moving variables, comtemporaneously correlated \\(x_t\\). Vector \\(\\hat{F}_t\\) computed \\(\\hat{C}(F_t,x_t) - b_x x_t\\), \\(b_x\\) coefficients coming previous OLS regressions.Note approach implies vectorial space spanned \\((\\hat{F}_t,x_t)\\) spanned \\(\\hat{C}(F_t,x_t)\\)., employ method dataset built McCracken Ng (2016) —FRED:MD database— includes 119 time series.\nFigure 2.23: Responses monetary-policy shock. FAVAR approach Bernanke, Boivin, Eliasz (2005). FRED-MD dataset.\n","code":"\nlibrary(BVAR)# contains the fred_md dataset\nlibrary(vars)\ndata <- fred_transform(fred_md,na.rm = FALSE, type = \"fred_md\")\nFirst.date <- \"1959-02-01\"\nLast.date <- \"2020-01-01\"\ndata <- data[(rownames(data)>First.date)&(rownames(data)<Last.date),]\nvariables.with.na <- which(is.na(apply(data,2,sum)))\ndata <- data[,-variables.with.na]\ndata.values <- scale(data, center = TRUE, scale = TRUE)\ndata_scaled <- data\ndata_scaled[1:dim(data)[1],1:dim(data)[2]] <- data.values\nK <- 3\nM <- 1\nPCA <- prcomp(data_scaled) # implies that PCA$x %*% t(PCA$rotation) = data\nC.hat <- PCA$x[,1:(K+M)]\nfast_moving <- c(\"HOUST\",\"HOUSTNE\",\"HOUSTMW\",\"HOUSTS\",\"HOUSTW\",\"HOUSTS\",\"AMDMNOx\",\n                 \"FEDFUNDS\",\"CP3Mx\",\"TB3MS\",\"TB6MS\",\"GS1\",\"GS5\",\"GS10\",\n                 \"COMPAPFFx\",\"TB3SMFFM\",\"TB6SMFFM\",\"T1YFFM\",\"T5YFFM\",\"T10YFFM\",\n                 \"AAAFFM\",\"EXSZUSx\",\"EXJPUSx\",\"EXUSUKx\",\"EXCAUSx\")\ndata.slow <- data_scaled[,-which(fast_moving %in% names(data))]\nPCA.star <- prcomp(data.slow) # implies that PCA$x %*% t(PCA$rotation) = data\nC.hat.star <- PCA.star$x[,1:K]\nD <- cbind(data$FEDFUNDS,C.hat.star)\nb.x <- solve(t(D)%*%D) %*% t(D) %*% C.hat\nF.hat <- C.hat - data$FEDFUNDS %*% matrix(b.x[1,],nrow=1)\ndata_var <- data.frame(F.hat, FEDFUNDS = data$FEDFUNDS)\np <- 10\nvar <- VAR(data_var, p)\nOmega <- var(residuals(var))\nB <- t(chol(Omega))\nD <- cbind(F.hat,data$FEDFUNDS)\nloadings <- solve(t(D)%*%D) %*% t(D) %*% as.matrix(data_scaled)\nirf <- simul.VAR(c=rep(0,(K+M)*p),Phi=Acoef(var),B,nb.sim=120,\n                 y0.star=rep(0,(K+M)*p),indic.IRF = 1,\n                 u.shock = c(rep(0,K+1),1))\nirf.all <- irf %*% loadings\npar(mfrow=c(2,2))\nvariables.2.plot <- c(\"FEDFUNDS\",\"INDPRO\",\"UNRATE\",\"CPIAUCSL\")\npar(plt=c(.2,.95,.3,.95))\nfor(i in 1:length(variables.2.plot)){\n  plot(cumsum(irf.all[,which(variables.2.plot[i]==names(data))]),lwd=2,\n       type=\"l\",xlab=\"months after shock\",ylab=variables.2.plot[i])\n}"},{"path":"TS.html","id":"Projections","chapter":"2 Time Series","heading":"2.3.11 Projection Methods","text":"Consider infinite MA representation \\(y_t\\) (Eq. (2.34)):\n\\[\ny_t = \\mu + \\sum_{h=0}^\\infty \\Psi_{h} \\eta_{t-h}.\n\\]\nseen Section 2.3.2, entries \\((,j)\\) sequence \\(\\Psi_h\\) matrices define IRF \\(\\eta_{j,t}\\) \\(y_{,t}\\).Assume observe \\(\\eta_{j,t}\\), consistent estimate \\(\\Psi_{,j,h}\\) simply obtained OLS regression \\(y_{,t+h}\\) \\(\\eta_{j,t}\\):\n\\[\\begin{equation}\ny_{,t+h} = \\mu_i + \\Psi_{,j,h}\\eta_{j,t} + u_{,j,t+h}.\\tag{2.53}\n\\end{equation}\\]\nresiduals \\(u_{,j,t+h}\\) autocorrelated (\\(h>0\\)), estimates covariance OLS estimators \\(\\Psi_{,j,h}\\) based robust estimators (e.g. Newey-West, see Eq. (??)). core idea local projection approach proposed Jordà (2005).Now, proceed (usual) case \\(\\eta_{j,t}\\) observed? consider two situations.Situation : Without IVThis corresponds original Jordà (2005)’s approach.Assume structural shock interest (\\(\\eta_{1,t}\\), say) can consistently obtained residual regression variable \\(x_t\\) set control variables \\(w_t\\) independent \\(\\eta_{1,t}\\):\n\\[\\begin{equation}\n\\eta_{1,t} = x_t - \\mathbb{E}(x_t|w_t),\\tag{2.54}\n\\end{equation}\\]\n\\(\\mathbb{E}(x_t|w_t)\\) affine \\(w_t\\) \\(w_t\\) affine transformation \\(\\eta_{2:n,t}\\) past shocks \\(\\eta_{t-1},\\eta_{t-2},\\dots\\).Eq. (2.54) implies , conditional \\(w_t\\), additional knowledge \\(x_t\\) useful comes forecast something depends \\(\\eta_{1,t}\\). Hence, given \\(u_{,1,t+h}\\) (see Eq. (2.53)) independent \\(\\eta_{1,t}\\) (depends \\(\\eta_{t+h},\\dots,\\eta_{t+1},\\color{blue}{\\eta_{2:n,t}},\\eta_{t-1},\\eta_{t-2},\\dots\\)), comes \n\\[\n\\mathbb{E}(u_{,1,t+h}|x_t,w_t)= \\mathbb{E}(u_{,1,t+h}|w_t).\n\\]\nconditional mean independence case.Let’s rewrite Eq. (2.53) follows:\n\\[\\begin{eqnarray*}\ny_{,t+h} &=& \\mu_i + \\Psi_{,1,h}\\eta_{1,t} + u_{,1,t+h}\\\\\n&=&  \\mu_i + \\Psi_{,1,h}x_t  \\color{blue}{-\\Psi_{,1,h}\\mathbb{E}(x_t|w_t) + u_{,1,t+h}},\n\\end{eqnarray*}\\]precedes implies expectation blue term, conditional \\(x_t\\) \\(w_t\\), linear \\(w_t\\). Standard results conditional mean independence case imply regression \\(y_{,t+h}\\) \\(x_t\\), controlling \\(w_t\\), provides consistent estimate \\(\\Psi_{,1,h}\\):\n\\[\\begin{equation}\ny_{,t+h} = \\alpha_i + \\Psi_{,1,h}x_t + \\beta'w_t + v_{,t+h}.\n\\end{equation}\\]instance consistent case \\([\\Delta GDP_t, \\pi_t,i_t]'\\) follows VAR(1) monetary-policy shock contemporaneously affect \\(\\Delta GDP_t\\) \\(\\pi_t\\).IRFs can estimated LP, taking \\(x_t = i_t\\) \\(w_t = [\\Delta GDP_t,\\pi_t,\\Delta GDP_{t-1}, \\pi_{t-1},i_{t-1}]'\\).approach closely relates SVAR Cholesky-based identification approach. Specifically, \\(w_t = [{\\color{blue}y_{1,t},\\dots,y_{k-1,t}}, y_{t-1}',\\dots,y_{t-p}']'\\), \\(k\\le n\\), \\(x_t = y_{k,t}\\), approach corresponds, \\(h=0\\), SVAR(\\(p\\)) Cholesky-based IRF (focusing responses \\(k^{th}\\) structural shock). However, two approaches differ \\(h>0\\), LP methodology assumes VAR dynamics \\(y_t\\).4Situation B: IV approachConsider now valid instrument \\(z_t\\) \\(\\eta_{1,t}\\) (\\(\\mathbb{E}(z_t)=0\\)). :\n\\[\\begin{equation}\n\\left\\{\n\\begin{array}{llll}\n(IV.) & \\mathbb{E}(z_t \\eta_{1,t}) &\\ne 0 & \\mbox{(relevance condition)} \\\\\n(IV.ii) & \\mathbb{E}(z_t \\eta_{j,t}) &= 0 \\quad \\mbox{} j>1 & \\mbox{(exogeneity condition)}\n\\end{array}\\right.\\tag{2.55}\n\\end{equation}\\]\ninstrument \\(z_t\\) can used identify structural shock. Eq. (2.55) implies exist \\(\\rho \\ne 0\\) mean-zero variable \\(\\xi_t\\) :\n\\[\n\\eta_{1,t} = \\rho z_t + \\xi_t,\n\\]\n\\(\\xi_t\\) correlated neither \\(z_t\\), \\(\\eta_{j,t}\\), \\(j\\ge2\\).Proof. Define \\(\\rho = \\frac{\\mathbb{E}(\\eta_{1,t}z_t)}{\\mathbb{V}ar(z_t)}\\) \\(\\xi_t = \\eta_{1,t} - \\rho z_t\\). easily seen \\(\\xi_t\\) satisfies moment restrictions given .Ramey (2016) reviews different approaches employed construct monetary policy-shocks (two main approaches presented 2.9 2.10 ). also collected time series shocks, see website.Example 2.9  (Identification Monetary-Policy Shocks Based High-Frequency Data) Instruments monetary-policy shocks can extracted high-frequency market data associated interest-rate products.quotes interest-rate-related financial products sensitive monetary-policy announcements. quotes mainly depends investors’ expectations regarding future short-term rates: \\(\\mathbb{E}_t(i_{t+s})\\). Typically, agents risk-neutral, maturity-\\(h\\) interest rate approximatively given :\n\\[\ni_{t,h} \\approx \\mathbb{E}_t\\left(\\frac{1}{h}\\int_{0}^{h} i_{t+s} ds\\right) = \\frac{1}{h}\\int_{0}^{h} \\mathbb{E}_t\\left(i_{t+s}\\right) ds.\n\\]\ngeneral, changes \\(\\mathbb{E}_t(i_{t+s})\\), \\(s>0\\), can affected types shocks may trigger reaction central bank.However, MP announcement takes place \\(t\\) \\(t+\\epsilon\\), \\(\\mathbb{E}_{t+\\epsilon}(i_{t+s})-\\mathbb{E}_t(i_{t+s})\\) attributed MP shock (see Figure 2.24, Gürkaynak, Sack, Swanson (2005)). Hence, monthly time series MP shocks can obtained summing, month, changes \\(i_{t+ \\epsilon,h} - i_{t,h}\\) associated given interest rate (T-bills, futures, swaps) given maturity \\(h\\).See among others: Kuttner (2001), Cochrane Piazzesi (2002),Gürkaynak, Sack, Swanson (2005), Piazzesi Swanson (2008), Gertler Karadi (2015).\nFigure 2.24: Source: Gurkaynak, Sack Swanson (2005). Transaction rates Federal funds futures June 25, 2003, day regularly scheduled FOMC meeting scheduled. 2:15 p.m., FOMC announced lowering target federal funds rate 1.25% 1%, many market participants expecting 50 bp cut. shows () financial markets seem fully adjust policy action within just minutes (ii) federal funds rate surprise necessarily direction federal funds rate action .\nExample 2.10  (Identification Monetary-Policy Shocks Based Narrative Approach) Romer Romer (2004) propose two-step approach:derive series Federal Reserve intentions federal funds rate (explicit target Fed) around FOMC meetings,control Federal Reserve forecasts.gives measure intended monetary policy actions driven information future economic developments.\n. “intentions” measured combination narrative quantitative evidence. Sources: (among others) Minutes FOMC “Blue Books”.\nb. Controls = variables spanning information Federal Reserve future developments. Data: Federal Reserve’s internal forecasts (inflation, real output unemployment), “Greenbook’s forecasts” – usually issued 6 days FOMC meeting.shock measure residual series linear regression () (b).two main IV approaches estimate IRFs see James H. Stock Watson (2018):LP-IV approach, \\(y_t\\)’s DGP left unspecified,SVAR-IV approach.LP-IV approach based set IV regressions (variable interest, one forecast horizon). SVAR-IV approach based IV regressions VAR innovations (one series VAR innovations).VAR adequately captures DGP, IV-SVAR optimal horizons. However, VAR misspecified, specification errors compounded horizon local projection method lead better results.Situation B.1: SVAR-IV approachAssume consistent estimates \\(\\varepsilon_t = B\\eta_t\\), estimates (\\(\\hat\\varepsilon_{t}\\)) coming estimation VAR model. , \\(\\\\{1,\\dots,n\\}\\):\n\\[\\begin{eqnarray}\n\\varepsilon_{,t} &=& b_{,1} \\eta_{1,t} + u_{,t} (\\#eq:eps_rho)\\\\\n&=& b_{,1} \\rho z_t + \\underbrace{b_{,1}\\xi_t + u_{,t}}_{\\perp z_t}. \\nonumber\n\\end{eqnarray}\\]\n(\\(u_{,t}\\) linear combination \\(\\eta_{j,t}\\)’s, \\(j\\ge2\\)).Hence, multiplicative factor (\\(\\rho\\)), (OLS) regressions \\(\\hat\\varepsilon_{,t}\\)’s \\(z_t\\) provide consistent estimates \\(b_{,1}\\)’s.Combined estimated VAR (\\(\\Phi_k\\) matrices), provides consistent estimates IRFs \\(\\eta_{1,t}\\) \\(y_t\\), though multiplicative factor. scale ambiguity can solved rescaling structural shock (“unit-effect normalisation”, see James H. Stock Watson (2018)). Let us consider \\(\\tilde\\eta_{1,t}=b_{1,1}\\eta_{1,t}\\); construction, \\(\\tilde\\eta_{1,t}\\) one-unit contemporaneous effect \\(y_{1,t}\\). Denoting \\(\\tilde{B}_{,1}\\) contemporaneous impact \\(\\tilde\\eta_{1,t}\\), get:\n\\[\n\\tilde{B}_{1} = \\frac{1}{b_{1,1}} {B}_{1},\n\\]\n\\(B_{1}\\) denotes \\(1^{st}\\) column \\(B\\) \\(\\tilde{B}_{1}=[1,\\tilde{B}_{2,1},\\dots,\\tilde{B}_{n,1}]'\\).Eq. @ref(eq:eps_rho) gives:\n\\[\\begin{eqnarray*}\n\\varepsilon_{1,t} &=& \\tilde\\eta_{1,t} + u_{1,t}\\\\\n\\varepsilon_{,t} &=& \\tilde{B}_{,1} \\tilde\\eta_{1,t} + u_{,t}.\n\\end{eqnarray*}\\]\nsuggests \\(\\tilde{B}_{,1}\\) can estimated regressing \\(\\varepsilon_{,t}\\) \\(\\varepsilon_{1,t}\\), using \\(z_t\\) instrument.inference? use usual TSLS standard deviations \\(\\varepsilon_{,t}\\)’s directly observed. Bootstrap procedures can resorted . James H. Stock Watson (2018) propose, particular, Gaussian parametric bootstrap:Assume estimated \\(\\{\\widehat{\\Phi}_1,\\dots,\\widehat{\\Phi}_p,\\widehat{B}_1\\}\\) using SVAR-IV approach based size-\\(T\\) sample. Generate \\(N\\) (\\(N\\) large) size-\\(T\\) samples following VAR:\n\\[\n\\left[\n\\begin{array}{cc}\n\\widehat{\\Phi}(L) & 0 \\\\\n0 & \\widehat{\\rho}(L)\n\\end{array}\n\\right]\n\\left[\n\\begin{array}{c}\ny_t \\\\\nz_t\n\\end{array}\n\\right] =\n\\left[\n\\begin{array}{c}\n\\varepsilon_t \\\\\ne_t\n\\end{array}\n\\right],\n\\]\n\\[\n\\mbox{} \\quad \\left[\n\\begin{array}{c}\n\\varepsilon_t \\\\\ne_t\n\\end{array}\n\\right]\\sim \\, ..d.\\,\\mathcal{N}\\left(\\left[\\begin{array}{c}0\\\\0\\end{array}\\right],\n\\left[\\begin{array}{cc}\n\\Omega & S'_{\\varepsilon,e}\\\\\nS_{\\varepsilon,e}& \\sigma^2_{e}\n\\end{array}\\right]\n\\right),\n\\]\n\\(\\widehat{\\rho}(L)\\) \\(\\sigma^2_{e}\\) result estimation AR process \\(z_t\\), \\(\\Omega\\) \\(S_{\\varepsilon,e}\\) sample covariances VAR/AR residuals.simulated sample (\\(\\tilde{y}_t\\) \\(\\tilde{z}_t\\), say), estimate \\(\\{\\widetilde{\\widehat{\\Phi}}_1,\\dots,\\widetilde{\\widehat{\\Phi}}_p,\\widetilde{\\widehat{B}}_1\\}\\) associated \\(\\widetilde{\\Psi}_{,1,h}\\). provides e.g. sequence \\(N\\) estimates \\(\\Psi_{,1,h}\\), quantiles conf. intervals can deduced.\nFigure 2.25: Gertler-Karadi monthly shocks, fed funds futures 3 months.\n\nFigure 2.26: Reponses monetary-policy shock, SVAR-IV approach.\nSituation B.2: LP-IVIf want posit VAR-type dynamics \\(y_t\\) –e.g. suspect true generating model may non-invertible VARMA model– can directly proceed IV-projection methods obtain \\(\\tilde\\Psi_{,1,h}\\equiv \\Psi_{,1,h}/b_{1,1}\\) (IRFs \\(\\tilde\\eta_{1,t}\\) \\(y_{,t}\\)).However, Assumptions (IV.) (IV.ii) (Eq. (2.55)) complemented (IV.iii):\n\\[\\begin{equation*}\n\\begin{array}{llll}\n(IV.iii) & \\mathbb{E}(z_t \\eta_{j,t+h}) &= 0 \\, \\mbox{ } h \\ne 0 & \\mbox{(lead-lag exogeneity)}\n\\end{array}\n\\end{equation*}\\](IV.), (IV.ii) (IV.iii) satisfied, \\(\\tilde\\Psi_{,1,h}\\) can estimated regressing \\(y_{,t+h}\\) \\(y_{1,t}\\), using \\(z_t\\) instrument, .e. considering TSLS estimation :\n\\[\\begin{equation}\ny_{,t+h} = \\alpha_i + \\tilde\\Psi_{,1,h}y_{1,t} + \\nu_{,t+h},\\tag{2.56}\n\\end{equation}\\]\n\\(\\nu_{,t+h}\\) correlated \\(y_{1,t}\\), \\(z_t\\).indeed:\n\\[\\begin{eqnarray*}\ny_{1,t} &=& \\alpha_1 + \\tilde\\eta_{1,t} + v_{1,t}\\\\\ny_{,t+h} &=& \\alpha_i + \\tilde\\Psi_{,1,h}\\tilde\\eta_{1,t} + v_{,t+h},\n\\end{eqnarray*}\\]\n\\(v_{,t+h}\\)’s uncorrelated \\(z_t\\) (IV.), (IV.ii) (IV.iii).Note , \\(h>0\\), \\(v_{,t+h}\\) (\\(\\nu_{,t+h}\\)) auto-correlated. Newey-West corrections therefore used compute std errors \\(\\tilde\\Psi_{,1,h}\\)’s estimates.Consider linear regression:\n\\[\n\\mathbf{Y} = \\mathbf{X}\\boldsymbol\\beta + \\boldsymbol\\varepsilon,\n\\]\n\\(\\mathbb{E}(\\boldsymbol\\varepsilon)=0\\), explicative variables \\(\\mathbf{X}\\) supposed correlated residuals \\(\\boldsymbol\\varepsilon\\).Moreover, \\(\\boldsymbol\\varepsilon\\) supposed possibly heteroskedastic auto-correlated.consider instruments \\(\\mathbf{Z}\\), \\(\\mathbb{E}(\\mathbf{X}'\\mathbf{Z}) \\ne 0\\) \\(\\mathbb{E}(\\boldsymbol\\varepsilon'\\mathbf{Z}) = 0\\).IV estimator \\(\\boldsymbol\\beta\\) obtained regressing \\(\\hat{\\mathbf{Y}}\\) \\(\\hat{\\mathbf{X}}\\), \\(\\hat{\\mathbf{Y}}\\) \\(\\hat{\\mathbf{X}}\\) respective residuals regressions \\(\\mathbf{Y}\\) \\(\\mathbf{X}\\) \\(\\mathbf{Z}\\).\n\\[\\begin{eqnarray*}\n\\mathbf{b}_{iv} &=& [\\mathbf{X}'\\mathbf{Z}(\\mathbf{Z}'\\mathbf{Z})^{-1}\\mathbf{Z}'\\mathbf{X}]^{-1}\\mathbf{X}'\\mathbf{Z}(\\mathbf{Z}'\\mathbf{Z})^{-1}\\mathbf{Z}'\\mathbf{Y}\\\\\n\\mathbf{b}_{iv} &=& \\boldsymbol\\beta + \\frac{1}{\\sqrt{T}}\\underbrace{T[\\mathbf{X}'\\mathbf{Z}(\\mathbf{Z}'\\mathbf{Z})^{-1}\\mathbf{Z}'\\mathbf{X}]^{-1}\\mathbf{X}'\\mathbf{Z}(\\mathbf{Z}'\\mathbf{Z})^{-1}}_{=Q(\\mathbf{X},\\mathbf{Z}) \\overset{p}{\\rightarrow} \\mathbf{Q}_{xz}}\\underbrace{\\sqrt{T}\\left(\\frac{1}{T}\\mathbf{Z}'\\boldsymbol\\varepsilon\\right)}_{\\overset{d}{\\rightarrow} \\mathcal{N}(0,S)},\n\\end{eqnarray*}\\]\n\\(\\mathbf{S}\\) long-run variance \\(\\mathbf{z}_t\\varepsilon_t\\) (see next slide).asymptotic covariance matrix \\(\\sqrt{T}\\mathbf{b}_{iv}\\) \\(\\mathbf{Q}_{xz} \\mathbf{S} \\mathbf{Q}_{xz}'\\).covariance matrix \\(\\mathbf{b}_{iv}\\) can approximated \\(\\frac{1}{T}Q(\\mathbf{X},\\mathbf{Z})\\hat{\\mathbf{S}}Q(\\mathbf{X},\\mathbf{Z})'\\) \\(\\hat{\\mathbf{S}}\\) Newey-West estimator \\(\\mathbf{S}\\) (see Eq. (??))(IV.iii) usually restrictive \\(h>0\\) (\\(z_t\\) usually affected future shocks). contrast, may restrictive \\(h<0\\). can solved adding controls Regression (2.56). controls span space \\(\\{\\eta_{t-1},\\eta_{t-2},\\dots\\}\\).\\(z_t\\) suspected correlated past values \\(\\eta_{1,t}\\) \\(\\eta_{j,t}\\)’s, \\(j>1\\), one can add lags \\(z_t\\) controls (method e.g. advocated Ramey, 2016, p.108, considering instrument Gertler Karadi (2015)).general case, one can use lags \\(y_t\\) controls. Note , even (IV.iii) holds, adding controls may reduce variance regression error.noted James H. Stock Watson (2018), relevant variance long-run variance instrument-times-error term. also recommend (p.926) using leads lags \\(z_t\\) improve efficiency.","code":"\n# Load vars package:\nlibrary(vars)\nlibrary(AEC)\ndata(\"USmonthly\")\nFirst.date <- \"1990-05-01\"\nLast.date <- \"2012-6-01\"\nindic.first <- which(USmonthly$DATES==First.date)\nindic.last  <- which(USmonthly$DATES==Last.date)\nUSmonthly   <- USmonthly[indic.first:indic.last,]\nshock.name <- \"FF4_TC\" #\"FF4_TC\", \"ED2_TC\", \"ff1_vr\", \"rrshock83b\"\nindic.shock.name <- which(names(USmonthly)==shock.name)\nZ <- matrix(USmonthly[,indic.shock.name],ncol=1)\npar(plt=c(.1,.95,.1,.95))\nplot(USmonthly$DATES,Z,type=\"l\",xlab=\"\",ylab=\"\",lwd=2)\nconsidered.variables <- c(\"GS1\",\"LIP\",\"LCPI\",\"EBP\")\ny <- as.matrix(USmonthly[,considered.variables])\nn <- length(considered.variables)\ncolnames(y) <- considered.variables\npar(plt=c(.15,.95,.15,.8))\nres.svar.iv <- \n  svar.iv(y,Z,p = 4,names.of.variables=considered.variables,\n          nb.periods.IRF = 20,\n          z.AR.order=1, \n          nb.bootstrap.replications = 100, \n          confidence.interval = 0.90,\n          indic.plot=1)\nres.LP.IV <- make.LPIV.irf(y,Z,\n                           nb.periods.IRF = 20,\n                           nb.lags.Y.4.control=4,\n                           nb.lags.Z.4.control=4,\n                           indic.plot = 1, # Plots are displayed if = 1.\n                           confidence.interval = 0.90)"},{"path":"TS.html","id":"Inference","chapter":"2 Time Series","heading":"2.3.12 Inference","text":"Consider following SVAR model:\n\\[y_t = \\Phi_1 y_{t-1} + \\dots + \\Phi_p y_{t-p} + \\varepsilon_t\\]\n\\(\\varepsilon_t=B\\eta_t\\), \\(\\Omega_\\varepsilon=BB'\\).corresponding infinite MA representation (Eq. (2.34), Wold theorem, Theorem 2.3) :\n\\[\ny_t = \\sum_{h=0}^\\infty\\Psi_h \\eta_{t-h},\n\\]\n\\(\\Psi_0=B\\) \\(h=1,2,\\dots\\):\n\\[\n\\Psi_h = \\sum_{j=1}^h\\Psi_{h-j}\\Phi_j,\n\\]\n\\(\\Phi_j=0\\) \\(j>p\\) (see Prop. 2.8 recursive computation \\(\\Psi_j\\)’s).Inference VAR coefficients \\(\\{\\Phi_j\\}_{j=1,...,p}\\) straightforward (standard OLS inference). inference complicated regarding IRF. Indeed, shown previous equation, (infinite) MA coefficients \\(\\{\\Psi_j\\}_{j=1,...}\\) non-linear functions \\(\\{\\Phi_j\\}_{j=1,...,p}\\) \\(\\Omega_\\varepsilon\\). issue pertain small sample bias: typically, persistent process, auto-regressive parameters known downward biased.main inference methods following:Monte Carlo method (Hamilton (1994))Asymptotic normal approximation (Lütkepohl (1990)), Delta methodBootstrap method (Kilian_1998)Monte Carlo methodWe use Monte Carlo need approximate distribution variable whose distribution unknown (: \\(\\Psi_j\\)’s) function another variable whose distribution known (, \\(\\Phi_j\\)’s).instance, suppose know distribution random variable \\(X\\), takes values \\(\\mathbb{R}\\), density function \\(p\\). Assume want compute mean \\(\\varphi(X)\\). :\n\\[\n\\mathbb{E}(\\varphi(X))=\\int_{-\\infty}^{+\\infty}\\varphi(x)p(x)dx\n\\]\nSuppose integral simple expression. compute \\(\\mathbb{E}(\\varphi(X))\\) , virtue law large numbers (Theorem ??), can approximate follows:\n\\[\n\\mathbb{E}(\\varphi(X))\\approx\\frac{1}{N}\\sum_{=1}^N\\varphi(X^{()}),\n\\]\n\\(\\{X^{()}\\}_{=1,...,N}\\) \\(N\\) independent draws \\(X\\). generally, distribution \\(\\varphi(X)\\) can approximated empirical distribution \\(\\varphi(X^{()}\\)’s. Typically, 10’000 values \\(\\varphi(X^{()}\\) drawn, \\(5^{th}\\) percentile p.d.f. \\(\\varphi(X)\\) can approximated \\(500^{th}\\) value 10’000 draws \\(\\varphi(X^{()}\\) (arranging values ascending order).regards computation confidence intervals around IRFs, one think \\(\\{\\widehat{\\Phi}_j\\}_{j=1,...,p}\\), \\(\\widehat{\\Omega}\\) \\(X\\) \\(\\{\\widehat{\\Psi}_j\\}_{j=1,...}\\) \\(\\varphi(X)\\). (Proposition 2.14 provides us asymptotic distribution “\\(X\\).”)summarize, steps one can implement derive confidence intervals IRFs using Monte-Carlo approach:iteration \\(k\\):Draw \\(\\{\\widehat{\\Phi}_j^{(k)}\\}_{j=1,...,p}\\) \\(\\widehat{\\Omega}^{(k)}\\) asymptotic distribution (using Proposition 2.14).Compute matrix \\(B^{(k)}\\) \\(\\widehat{\\Omega}^{(k)}=B^{(k)}B^{(k)'}\\), according identification strategy.Compute associated IRFs \\(\\{\\widehat{\\Psi}_j\\}^{(k)}\\).Perform \\(N\\) replications report median impulse response (confidence intervals).Delta methodSuppose \\(\\beta\\) vector parameters \\(\\beta\\) estimator \n\\[\n\\sqrt{T}(\\hat\\beta-\\beta)\\overset{d}{\\rightarrow}\\mathcal{N}(0,\\Sigma_\\beta),\n\\]\n\\(d\\) denotes convergence distribution, \\(N(0,\\Sigma_\\beta)\\) denotes multivariate normal distribution mean vector 0 covariance matrix \\(\\Sigma_\\beta\\) \\(T\\) sample size used estimation.Let \\(g(\\beta) = (g_l(\\beta),..., g_m(\\beta))'\\) continuously differentiable function values \\(\\mathbb{R}^m\\), assume \\(\\partial g_i/\\partial \\beta' = (\\partial g_i/\\partial \\beta_j)\\) nonzero \\(\\beta\\) \\(= 1,\\dots, m\\). \n\\[\n\\sqrt{T}(g(\\hat\\beta)-g(\\beta))\\overset{d}{\\rightarrow}\\mathcal{N}\\left(0,\\frac{\\partial g}{\\partial \\beta'}\\Sigma_\\beta\\frac{\\partial g'}{\\partial \\beta}\\right).\n\\]\n(formula underlies Delta method, see Eq. (??).)Using property, Lütkepohl (1990) provides asymptotic distributions \\(\\Psi_j\\)’s.limit last two approaches (Monte Carlo Delta method) critically rely asymptotic results. Boostrapping approaches robust small-sample situations.BootstrapIRFs’ confidence intervals intervals 90% (95%,75%,…) IRFs lie, repeat estimation large number times similar conditions (\\(T\\) observations). obviously , one sample: \\(\\{y_t\\}_{t=1,..,T}\\). can try construct samples.Bootstrapping consists :re-sampling \\(N\\) times, .e., constructing \\(N\\) samples \\(T\\) observations, using estimated\nVAR coefficients anda sample residuals distribution \\(N(0,BB')\\) (parametric approach), ora sample residuals drawn randomly set actual estimated residuals \\(\\{\\hat\\varepsilon_t\\}_{t=1,..,T}\\). (non-parametric approach).re-estimating SVAR \\(N\\) times.algorithm:Construct sample\n\\[\ny_t^{(k)}=\\widehat{\\Phi}_1 y_{t-1}^{(k)} + \\dots + \\widehat{\\Phi}_p y_{t-p}^{(k)} + \\hat\\varepsilon_t^{(k)},\n\\]\n\\(\\hat\\varepsilon_{t}^{(k)}=\\hat\\varepsilon_{s_t^{(k)}}\\), \\(\\{s_1^{(k)},..,s_T^{(k)}\\}\\) random set \\(\\{1,..,T\\}^T\\).Re-estimate SVAR compute IRFs \\(\\{\\widehat{\\Psi}_j\\}^{(k)}\\).Perform \\(N\\) replications report median impulse response (confidence intervals).Bootstrap--bootstrap (Kilian (1998))previous simple bootstrapping procedure deals non-normality small sample distribution, since use actual residuals. However, deal small sample bias, stemming, particular, small-sample bias associated OLS coefficient estimates \\(\\{\\widehat{\\Phi}_j\\}_{j=1,..,p}\\). main idea bootstrap--bootstrap Kilian (1998) run two consecutive boostraps: objective first compute bias, can used correct initial estimates \\(\\Phi_i\\)’s. , corrected estimates used —second boostrap— compute set IRFs (standard boostrap).formally, algorithm follows:Estimate SVAR coefficients \\(\\{\\widehat{\\Phi}_j\\}_{j=1,..,p}\\) \\(\\widehat{\\Omega}\\)First bootstrap. iteration \\(k\\):Construct sample\n\\[\ny_t^{(k)}=\\widehat{\\Phi}_1 y_{t-1}^{(k)} + \\dots + \\widehat{\\Phi}_p y_{t-p}^{(k)} + \\hat\\varepsilon_t^{(k)},\n\\]\n\\(\\hat\\varepsilon_{t}^{(k)}=\\hat\\varepsilon_{s_t^{(k)}}\\), \\(\\{s_1^{(k)},..,s_T^{(k)}\\}\\) random set \\(\\{1,..,T\\}^T\\).Re-estimate VAR compute coefficients \\(\\{\\widehat{\\Phi}_j\\}_{j=1,..,p}^{(k)}\\).Perform \\(N\\) replications compute median coefficients \\(\\{\\widehat{\\Phi}_j\\}_{j=1,..,p}^*\\).Approximate bias terms \\(\\widehat{\\Theta}_j=\\widehat{\\Phi}_j^*-\\widehat{\\Phi}_j\\).Construct bias-corrected terms \\(\\widetilde{\\Phi}_j=\\widehat{\\Phi}_j-\\widehat{\\Theta}_j\\).Second bootstrap. iteration \\(k\\):Construct sample now \n\\[y_t^{(k)}=\\widetilde{\\Phi}_1 y_{t-1}^{(k)} + \\dots + \\widetilde{\\Phi}_p y_{t-p}^{(k)} + \\hat\\varepsilon_t^{(k)}.\n\\]Re-estimate VAR compute coefficients \\(\\{\\widehat{\\Phi}^*_j\\}_{j=1,..,p}^{(k)}\\).Construct bias-corrected estimates \\(\\widetilde{\\Phi}_j^{*(k)}=\\widehat{\\Phi}_j^{*(k)}-\\widehat{\\Theta}_j\\).Compute associated IRFs \\(\\{\\widetilde{\\Psi}_j^{*(k)}\\}_{j\\ge 1}\\).Perform \\(N\\) replications compute median confidence interval set IRFs.noted correcting bias can generate non-stationary results (\\(\\tilde \\Phi\\) eigenvalue modulus \\(>1\\)). Solution (Kilian (1998)):step 5, check largest eigenvalue \\(\\tilde\\Phi\\) modulus <1.\n, shrink bias: \\(j\\)s, set \\(\\widehat{\\Theta}_j^{(+1)}=\\delta_{+1}\\widehat{\\Theta}_j^{()}\\), \\(\\delta_{+1}=\\delta_i-0.01\\), starting \\(\\delta_1=1\\) \\(\\widehat{\\Theta}_j^{(1)} =\\widehat{\\Theta}_j\\), compute \\(\\widetilde{\\Phi}_j^{(+1)}=\\widehat{\\Phi}_j-\\widehat{\\Theta}_j^{(+1)}\\) largest eigenvalue \\(\\tilde\\Phi^{(+1)}\\) modulus <1.Function VAR.Boot package VAR.etp (Kim (2022)) can used operate bias-correction approach Kilian (1998):","code":"\nlibrary(VAR.etp)\nlibrary(vars) #standard VAR models\ndata(dat) # part of VAR.etp package\ncorrected <- VAR.Boot(dat,p=2,nb=200,type=\"const\")\nnoncorrec <- VAR(dat,p=2)\nrbind(corrected$coef[1,],\n      (corrected$coef+corrected$Bias)[1,],\n      noncorrec$varresult$inv$coefficients)##         inv(-1)   inc(-1)   con(-1)    inv(-2)   inc(-2)   con(-2)       const\n## [1,] -0.3239808 0.1462410 0.9157203 -0.1398299 0.1785468 0.9125583 -0.01693370\n## [2,] -0.3196310 0.1459888 0.9612190 -0.1605511 0.1146050 0.9343938 -0.01672199\n## [3,] -0.3196310 0.1459888 0.9612190 -0.1605511 0.1146050 0.9343938 -0.01672199"},{"path":"TS.html","id":"forecasting","chapter":"2 Time Series","heading":"2.4 Forecasting","text":"Forecasting always important part time series field (De Gooijer Hyndman (2006)). Macroeconomic forecasts done many places: Public Administration (notably Treasuries), Central Banks, International Institutions (e.g. IMF, OECD), banks, big firms. institutions interested point estimates (\\(\\sim\\) likely value) variable interest. also sometimes need measure uncertainty (\\(\\sim\\) dispersion likely outcomes) associated point estimates.5Forecasts produced professional forecasters available web pages:Philly Fed Survey Professional Forecasters.ECB Survey Professional Forecasters.IMF World Economic Outlook.OECD Global Economic Outlook.European Commission Economic Forecasts.formalize forecasting problem? Assume current date \\(t\\). want forecast value variable \\(y_t\\) take date \\(t+1\\) (.e., \\(y_{t+1}\\)) based observation set variables gathered vector \\(x_t\\) (\\(x_t\\) may contain lagged values \\(y_t\\)).forecaster aims minimizing (function ) forecast error. usal consider following (quadratic) loss function:\n\\[\n\\underbrace{\\mathbb{E}([y_{t+1} - y^*_{t+1}]^2)}_{\\mbox{Mean square error (MSE)}}\n\\]\n\\(y^*_{t+1}\\) forecast \\(y_{t+1}\\) (function \\(x_t\\)).Proposition 2.15  (Smallest MSE) smallest MSE obtained MSEthe expectation \\(y_{t+1}\\) conditional \\(x_t\\).Proof. See Appendix 3.5.Proposition 2.16  Among class linear forecasts, smallest MSE obtained linear projection \\(y_{t+1}\\) \\(x_t\\).\nprojection, denoted \\(\\hat{P}(y_{t+1}|x_t):=\\boldsymbol\\alpha'x_t\\), satisfies:\n\\[\\begin{equation}\n\\mathbb{E}\\left( [y_{t+1} - \\boldsymbol\\alpha'x_t]x_t \\right)=\\mathbf{0}.\\tag{2.57}\n\\end{equation}\\]Proof. Consider function \\(f:\\) \\(\\boldsymbol\\alpha \\rightarrow \\mathbb{E}\\left( [y_{t+1} - \\boldsymbol\\alpha'x_t]^2 \\right)\\). :\n\\[\nf(\\boldsymbol\\alpha) = \\mathbb{E}\\left( y_{t+1}^2 - 2 y_t x_t'\\boldsymbol\\alpha + \\boldsymbol\\alpha'x_t x_t'\\boldsymbol\\alpha] \\right).\n\\]\n\\(\\partial f(\\boldsymbol\\alpha)/\\partial \\boldsymbol\\alpha = \\mathbb{E}(-2 y_{t+1} x_t + 2 x_t x_t'\\boldsymbol\\alpha)\\). function minimised \\(\\partial f(\\boldsymbol\\alpha)/\\partial \\boldsymbol\\alpha =0\\).Eq. (2.57) implies \\(\\mathbb{E}\\left( y_{t+1}x_t \\right)=\\mathbb{E}\\left(x_tx_t' \\right)\\boldsymbol\\alpha\\). (Note \\(x_t x_t'\\boldsymbol\\alpha=x_t (x_t'\\boldsymbol\\alpha)=(\\boldsymbol\\alpha'x_t) x_t'\\).)Hence, \\(\\mathbb{E}\\left(x_tx_t' \\right)\\) nonsingular,\n\\[\\begin{equation}\n\\boldsymbol\\alpha=[\\mathbb{E}\\left(x_tx_t' \\right)]^{-1}\\mathbb{E}\\left( y_{t+1}x_t \\right).\\tag{2.58}\n\\end{equation}\\]MSE :\n\\[\n\\mathbb{E}([y_{t+1} - \\boldsymbol\\alpha'x_t]^2) = \\mathbb{E}{(y_{t+1}^2)} - \\mathbb{E}\\left( y_{t+1}x_t' \\right)[\\mathbb{E}\\left(x_tx_t' \\right)]^{-1}\\mathbb{E}\\left(x_ty_{t+1} \\right).\n\\]Consider regression \\(y_{t+1} = \\boldsymbol\\beta'\\mathbf{x}_t + \\varepsilon_{t+1}\\). OLS estimate :\n\\[\n\\mathbf{b} = \\left[ \\underbrace{ \\frac{1}{T} \\sum_{=1}^T \\mathbf{x}_t\\mathbf{x}_t'}_{\\mathbf{m}_1} \\right]^{-1}\\left[  \\underbrace{ \\frac{1}{T} \\sum_{=1}^T \\mathbf{x}_t'y_{t+1}}_{\\mathbf{m}_2} \\right].\n\\]\n\\(\\{x_t,y_t\\}\\) covariance-stationary ergodic second moments sample moments (\\(\\mathbf{m}_1\\) \\(\\mathbf{m}_2\\)) converges probability associated population moments \\(\\mathbf{b} \\overset{p}{\\rightarrow} \\boldsymbol\\alpha\\) (\\(\\boldsymbol\\alpha\\) defined Eq. (2.58)).Example 2.11  (Forecasting MA(q) process) Consider MA(q) process:\n\\[\ny_t = \\mu + \\varepsilon_t + \\theta_1 \\varepsilon_{t-1} + \\dots + \\theta_q \\varepsilon_{t-q},\n\\]\n\\(\\{\\varepsilon_t\\}\\) white noise sequence (Def. 2.1).:\n\\[\\begin{eqnarray*}\n&&\\mathbb{E}(y_{t+h}|\\varepsilon_{t},\\varepsilon_{t-1},\\dots) =\\\\\n&&\\left\\{\n\\begin{array}{lll}\n\\mu + \\theta_h \\varepsilon_{t} + \\dots + \\theta_q \\varepsilon_{t-q+h}  \\quad && \\quad h \\[1,q]\\\\\n\\mu \\quad && \\quad h > q\n\\end{array}\n\\right.\n\\end{eqnarray*}\\]\n\n\\[\\begin{eqnarray*}\n&&\\mathbb{V}ar(y_{t+h}|\\varepsilon_{t},\\varepsilon_{t-1},\\dots)= \\mathbb{E}\\left( [y_{t+h} - \\mathbb{E}(y_{t+h}|\\varepsilon_{t},\\varepsilon_{t-1},\\dots)]^2 \\right) =\\\\\n&&\\left\\{\n\\begin{array}{lll}\n\\sigma^2(1+\\theta_1^2+\\dots+\\theta_{h-1}^2) \\quad && \\quad h \\[1,q]\\\\\n\\sigma^2(1+\\theta_1^2+\\dots+\\theta_q^2) \\quad && \\quad h>q.\n\\end{array}\n\\right.\n\\end{eqnarray*}\\]Remark: previous reasoning relies assumption \\(\\varepsilon_t\\)s observed. generally case practice. Note consistent estimates available MA process invertible (see Eq. (2.28)) .Example 2.12  (Forecasting AR(p) process) (See web interface.) Consider AR(p) process:\n\\[\ny_t = c + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\dots + \\phi_p y_{t-p} + \\varepsilon_t,\n\\]\n\\(\\{\\varepsilon_t\\}\\) white noise sequence (Def. 2.1).Using notation Eq. (2.9), :\n\\[\n\\mathbf{y}_t - \\boldsymbol\\mu = F (\\mathbf{y}_{t-1}- \\boldsymbol\\mu) + \\boldsymbol\\xi_t,\n\\]\n\\(\\boldsymbol\\mu = [\\mu,\\dots,\\mu]'\\) (\\(\\mu\\) defined Eq. (2.13)). Hence:\n\\[\n\\mathbf{y}_{t+h} - \\boldsymbol\\mu = \\boldsymbol\\xi_{t+h} + F \\boldsymbol\\xi_{t+h-1} + \\dots + F^{h-1} \\boldsymbol\\xi_{t+1} + F^h (\\mathbf{y}_{t}- \\mu).\n\\]\nTherefore:\n\\[\\begin{eqnarray*}\n\\mathbb{E}(\\mathbf{y}_{t+h}|y_{t},y_{t-1},\\dots) &=& \\boldsymbol\\mu + F^{h}(\\mathbf{y}_t - \\boldsymbol\\mu)\\\\\n\\mathbb{V}ar\\left( [\\mathbf{y}_{t+h} - \\mathbb{E}(\\mathbf{y}_{t+h}|y_{t},y_{t-1},\\dots)] \\right) &=& \\Sigma + F\\Sigma F' + \\dots + F^{h-1}\\Sigma (F^{h-1})',\n\\end{eqnarray*}\\]\n:\n\\[\n\\Sigma = \\left[\n\\begin{array}{ccc}\n\\sigma^2  & 0& \\dots\\\\\n0  & 0 & \\\\\n\\vdots  & & \\ddots \\\\\n\\end{array}\n\\right].\n\\]Alternative approach: Taking (conditional) expectations sides \n\\[\ny_{t+h} - \\mu = \\phi_1 (y_{t+h-1} - \\mu) + \\phi_2 (y_{t+h-2} - \\mu) + \\dots + \\phi_p (y_{t-p} - \\mu) + \\varepsilon_{t+h},\n\\]\nobtain:\n\\[\\begin{eqnarray*}\n\\mathbb{E}(y_{t+h}|y_{t},y_{t-1},\\dots) &=& \\mu + \\phi_1\\left(\\mathbb{E}[y_{t+h-1}|y_{t},y_{t-1},\\dots] - \\mu\\right)+\\\\\n&&\\phi_2\\left(\\mathbb{E}[y_{t+h-2}|y_{t},y_{t-1},\\dots] - \\mu\\right) + \\dots +\\\\\n&& \\phi_p\\left(\\mathbb{E}[y_{t+h-p}|y_{t},y_{t-1},\\dots] - \\mu\\right),\n\\end{eqnarray*}\\]\ncan exploited recursively.recursion begins \\(\\mathbb{E}(y_{t-k}|y_{t},y_{t-1},\\dots)=y_{t-k}\\) (\\(k \\ge 0\\)).Example 2.13  (Forecasting ARMA(p,q) process) Consider process:\n\\[\\begin{equation}\ny_t = c + \\phi_1 y_{t-1} + \\dots + \\phi_p y_{t-p} + \\varepsilon_t + \\theta_1 \\varepsilon_{t-1} + \\dots + \\theta_q \\varepsilon_{t-q},\\tag{2.59}\n\\end{equation}\\]\n\\(\\{\\varepsilon_t\\}\\) white noise sequence (Def. 2.1). assume MA part process invertible (see Eq. (2.28)), implies information contained \\(\\{y_{t},y_{t-1},y_{t-2},\\dots\\}\\) identical \\(\\{\\varepsilon_{t},\\varepsilon_{t-1},\\varepsilon_{t-2},\\dots\\}\\).one use recursive algorithm compute conditional mean (Example 2.12), convenient employ Wold decomposition process (see Theorem 2.3 Prop. 2.8 computation \\(\\psi_i\\)’s context ARMA processes):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\ny_t = \\mu + \\sum_{=0}^{+\\infty} \\psi_i \\varepsilon_{t-}.\n\\]\nimplies:\n\\[\\begin{eqnarray*}\ny_{t+h} &=& \\mu + \\sum_{=0}^{h-1} \\psi_i \\varepsilon_{t+h-} + \\sum_{=h}^{+\\infty} \\psi_i \\varepsilon_{t+h-}\\\\\n&=& \\mu + \\sum_{=0}^{h-1} \\psi_i \\varepsilon_{t+h-} + \\sum_{=0}^{+\\infty} \\psi_{+h} \\varepsilon_{t-}.\n\\end{eqnarray*}\\]Since \\(\\mathbb{E}(y_{t+h}|y_t,y_{t-1},\\dots)=\\mu+\\sum_{=0}^{+\\infty} \\psi_{+h} \\varepsilon_{t-}\\), get:\n\\[\n\\mathbb{V}ar(y_{t+h}|y_t,y_{t-1},\\dots) =\\mathbb{V}ar\\left(\\sum_{=0}^{h-1} \\psi_i \\varepsilon_{t+h-}\\right)= \\sigma^2 \\sum_{=0}^{h-1} \\psi_i^2.\n\\]use previous formulas practice?One first select specification estimate model.\nTwo methods determine relevant specifications:Information criteria (see Definition 2.20).Box-Jenkins approach.Box Jenkins (1976) proposed approach now widely used.Data transformation. data transformed “make stationary”. , one can e.g. take logarithms, take changes considered series, remove (deterministic) trends.Select \\(p\\) \\(q\\). can based PACF approach (see Section 2.2.4), selection criteria (see Definition 2.20).Estimate model parameters. See Section 2.2.8.Check estimated model consistent data. See .Assessing performances forecasting modelOnce one fitted model given dataset (length \\(T\\), say), one compute MSE (mean square errors) evaluate performance model. MSE -sample one. easy reduce -sample MSE. Typically, model estimated OLS, adding covariates mechanically reduces MSE (see Props. ?? ??). , even additional data irrelevant, \\(R^2\\) regression increases. Adding irrelevant variables increases (-sample) \\(R^2\\) bound increase --sample MSE.Therefore, important analyse --sample performances forecasting model:Estimate model sample reduced size (\\(1,\\dots,T^*\\), \\(T^*<T\\))Use remaining available periods (\\(T^*+1,\\dots,T\\)) compute --sample forecasting errors (compute MSE). --sample exercise, important make sure data used produce forecasts (date \\(T^*\\)) indeed available date \\(T^*\\).Diebold-Mariano testHow compare different forecasting approaches? Diebold Mariano (1995) proposed simple test address question.Assume want compare approaches B. historical data sets implemented approaches past, providing two sets forecasting errors: \\(\\{e^{}_t\\}_{t=1,\\dots,T}\\) \\(\\{e^{B}_t\\}_{t=1,\\dots,T}\\).may case forecasts serve specific purpose , instance, dislike positive forecasting errors care less negative errors. assume able formalise means loss function \\(L(e)\\). instance:dislike large positive errors, may set \\(L(e)=\\exp(e)\\).concerned positive negative errors (indifferently), may set \\(L(e)=e^2\\) (standard approach).Let us define sequence \\(\\{d_t\\}_{t=1,\\dots,T} \\equiv \\{L(e^{}_t)-L(e^{B}_t)\\}_{t=1,\\dots,T}\\) assume sequence covariance stationary. consider following null hypothesis: \\(H_0:\\) \\(\\bar{d}=0\\), \\(\\bar{d}\\) denotes population mean \\(d_t\\)s. \\(H_0\\) assumption covariance-stationarity \\(d_t\\), (Theorem @ref{(hm:CLTcovstat)):\n\\[\n\\sqrt{T} \\bar{d}_T \\overset{d}{\\rightarrow} \\mathcal{N}\\left(0,\\sum_{j=-\\infty}^{+\\infty} \\gamma_j \\right),\n\\]\n\\(\\gamma_j\\)s autocovariances \\(d_t\\).Hence, assuming \\(\\hat{\\sigma}^2\\) consistent estimate \\(\\sum_{j=-\\infty}^{+\\infty} \\gamma_j\\) (instance one given Newey-West formula, see Def. ??), , \\(H_0\\):\n\\[\nDM_T := \\sqrt{T}\\frac{\\bar{d}_T}{\\sqrt{\\hat{\\sigma}^2}} \\overset{d}{\\rightarrow}  \\mathcal{N}(0,1).\n\\]\n\\(DM_T\\) test statistics. test size \\(\\alpha\\), critical region :6\n\\[\n]-\\infty,-\\Phi^{-1}(1-\\alpha/2)] \\cup [\\Phi^{-1}(1-\\alpha/2),+\\infty[,\n\\]\n\\(\\Phi\\) c.d.f. standard normal distribution.Example 2.14  (Forecasting Swiss GDP growth) use long historical time series Swiss GDP growth taken Jordà, Schularick, Taylor (2017) dataset (see Figure 2.3, Example 2.6).want forecast GDP growth. envision two specifications : AR(1) specification (one advocated AIC criteria, see Example 2.6), ARMA(2,2) specification. interested 2-year-ahead forecasts (.e., \\(h=2\\) since data yearly).alternative = \"greater\" alternative hypothesis method 2 accurate method 1. Since reject null (p-value 0.795), led use sophisticated model (ARMA(2,2)) keep simple AR(1) model.Assume now want compare AR(1) process VAR model (see Def. 2.21). consider bivariate VAR, GDP growth complemented CPI-based inflation rate., find alternative model (VAR(1) model) better AR(1) model forecast GDP growth.","code":"\nlibrary(AEC)\nlibrary(forecast)## Registered S3 method overwritten by 'quantmod':\n##   method            from\n##   as.zoo.data.frame zoo\ndata <- subset(JST,iso==\"CHE\")\nT <- dim(data)[1]\ny <- c(NaN,log(data$gdp[2:T]/data$gdp[1:(T-1)]))\nfirst.date <- T-50\ne1 <- NULL; e2 <- NULL;h<-2\nfor(T.star in first.date:(T-h)){\n  estim.model.1 <- arima(y[1:T.star],order=c(1,0,0))\n  estim.model.2 <- arima(y[1:T.star],order=c(2,0,2))\n  e1 <- c(e1,y[T.star+h] - predict(estim.model.1,n.ahead=h)$pred[h])\n  e2 <- c(e2,y[T.star+h] - predict(estim.model.2,n.ahead=h)$pred[h])\n}\nres.DM <- dm.test(e1,e2,h = h,alternative = \"greater\")\nres.DM## \n##  Diebold-Mariano Test\n## \n## data:  e1e2\n## DM = -0.82989, Forecast horizon = 2, Loss function power = 2, p-value =\n## 0.7946\n## alternative hypothesis: greater\nlibrary(vars)\ninfl <- c(NaN,log(data$cpi[2:T]/data$cpi[1:(T-1)]))\ny_var <- cbind(y,infl)\ne3 <- NULL\nfor(T.star in first.date:(T-h)){\n  estim.model.3 <- VAR(y_var[2:T.star,],p=1)\n  e3 <- c(e3,y[T.star+h] - predict(estim.model.3,n.ahead=h)$fcst$y[h,1])\n}\nres.DM <- dm.test(e1,e2,h = h,alternative = \"greater\")\nres.DM## \n##  Diebold-Mariano Test\n## \n## data:  e1e2\n## DM = -0.82989, Forecast horizon = 2, Loss function power = 2, p-value =\n## 0.7946\n## alternative hypothesis: greater"},{"path":"append.html","id":"append","chapter":"3 Appendix","heading":"3 Appendix","text":"","code":""},{"path":"append.html","id":"PCAapp","chapter":"3 Appendix","heading":"3.1 Principal component analysis (PCA)","text":"Principal component analysis (PCA) classical easy--use statistical method reduce dimension large datasets containing variables linearly driven relatively small number factors. approach widely used data analysis image compression.Suppose \\(T\\) observations \\(n\\)-dimensional random vector \\(x\\), denoted \\(x_{1},x_{2},\\ldots,x_{T}\\). suppose component \\(x\\) mean zero.Let denote \\(X\\) matrix given \\(\\left[\\begin{array}{cccc} x_{1} & x_{2} & \\ldots & x_{T}\\end{array}\\right]'\\). Denote \\(j^{th}\\) column \\(X\\) \\(X_{j}\\).want find linear combination \\(x_{}\\)’s (\\(x.u\\)), \\(\\left\\Vert u\\right\\Vert =1\\), “maximum variance.” , want solve:\n\\[\\begin{equation}\n\\begin{array}{clll}\n\\underset{u}{\\arg\\max} & u'X'Xu. \\\\\n\\mbox{s.t. } & \\left| u\\right| =1\n\\end{array}\\tag{3.1}\n\\end{equation}\\]Since \\(X'X\\) positive definite matrix, admits following decomposition:\n\\[\\begin{eqnarray*}\nX'X & = & PDP'\\\\\n& = & P\\left[\\begin{array}{ccc}\n\\lambda_{1}\\\\\n& \\ddots\\\\\n&  & \\lambda_{n}\n\\end{array}\\right]P',\n\\end{eqnarray*}\\]\n\\(P\\) orthogonal matrix whose columns eigenvectors \\(X'X\\).can order eigenvalues \\(\\lambda_{1}\\geq\\ldots\\geq\\lambda_{n}\\). (Since \\(X'X\\) positive definite, eigenvalues positive.)Since \\(P\\) orthogonal, \\(u'X'Xu=u'PDP'u=y'Dy\\) \\(\\left\\Vert y\\right\\Vert =1\\). Therefore, \\(y_{}^{2}\\leq 1\\) \\(\\leq n\\).consequence:\n\\[\ny'Dy=\\sum_{=1}^{n}y_{}^{2}\\lambda_{}\\leq\\lambda_{1}\\sum_{=1}^{n}y_{}^{2}=\\lambda_{1}.\n\\]easily seen maximum reached \\(y=\\left[1,0,\\cdots,0\\right]'\\). Therefore, maximum optimization program (Eq. (3.1)) obtained \\(u=P\\left[1,0,\\cdots,0\\right]'\\). , \\(u\\) eigenvector \\(X'X\\) associated larger eigenvalue (first column \\(P\\)).Let us denote \\(F\\) vector given matrix product \\(XP\\) (note last column equal \\(Xu\\)). columns \\(F\\), denoted \\(F_{j}\\), called factors. :\n\\[\nF'F=P'X'XP=D.\n\\]\nTherefore, particular, \\(F_{j}\\)’s orthogonal.Since \\(X=FP'\\), \\(X_{j}\\)’s linear combinations factors. Let us denote \\(\\hat{X}_{,j}\\) part \\(X_{}\\) explained factor \\(F_{j}\\), :\n\\[\\begin{eqnarray*}\n\\hat{X}_{,j} & = & p_{ij}F_{j}\\\\\nX_{} & = & \\sum_{j}\\hat{X}_{,j}=\\sum_{j}p_{ij}F_{j}.\n\\end{eqnarray*}\\]Consider share variance explained –\\(n\\) variables (\\(X_{1},\\ldots,X_{n}\\))– first factor \\(F_{1}\\):\n\\[\\begin{eqnarray*}\n\\frac{\\sum_{}\\hat{X}_{,1}\\hat{X}'_{,1}}{\\sum_{}X_{}X'_{}} & = & \\frac{\\sum_{}p_{i1}F_{1}F'_{1}p_{i1}}{tr(X'X)} = \\frac{\\sum_{}p_{i1}^{2}\\lambda_{1}}{tr(X'X)} = \\frac{\\lambda_{1}}{\\sum_{}\\lambda_{}}.\n\\end{eqnarray*}\\]Intuitively, first eigenvalue large, means first factor embed large share fluctutaions \\(n\\) \\(X_{}\\)’s.Let us illustrate PCA term structure yields. term strucutre yields (yield curve) know driven small number factors (e.g., Litterman Scheinkman (1991)). One can typically employ PCA recover factors. data used example taken Fred database (tickers: “DGS6MO”,“DGS1”, …). second plot shows factor loardings, indicate first factor level factor (loadings = black line), second factor slope factor (loadings = blue line), third factor curvature factor (loadings = red line).run PCA, one simply apply function prcomp matrix data:Let us know visualize results. first plot Figure 3.1 shows share total variance explained different principal components (PCs). second plot shows facotr loadings. two bottom plots show yields (black) fitted linear combinations first two PCs .\nFigure 3.1: PCA results. dataset contains 8 time series U.S. interest rates different maturities.\n","code":"\nlibrary(AEC)\nUSyields <- USyields[complete.cases(USyields),]\nyds <- USyields[c(\"Y1\",\"Y2\",\"Y3\",\"Y5\",\"Y7\",\"Y10\",\"Y20\",\"Y30\")]\nPCA.yds <- prcomp(yds,center=TRUE,scale. = TRUE)\npar(mfrow=c(2,2))\npar(plt=c(.1,.95,.2,.8))\nbarplot(PCA.yds$sdev^2/sum(PCA.yds$sdev^2),\n        main=\"Share of variance expl. by PC's\")\naxis(1, at=1:dim(yds)[2], labels=colnames(PCA.yds$x))\nnb.PC <- 2\nplot(-PCA.yds$rotation[,1],type=\"l\",lwd=2,ylim=c(-1,1),\n     main=\"Factor loadings (1st 3 PCs)\",xaxt=\"n\",xlab=\"\")\naxis(1, at=1:dim(yds)[2], labels=colnames(yds))\nlines(PCA.yds$rotation[,2],type=\"l\",lwd=2,col=\"blue\")\nlines(PCA.yds$rotation[,3],type=\"l\",lwd=2,col=\"red\")\nY1.hat <- PCA.yds$x[,1:nb.PC] %*% PCA.yds$rotation[\"Y1\",1:2]\nY1.hat <- mean(USyields$Y1) + sd(USyields$Y1) * Y1.hat\nplot(USyields$date,USyields$Y1,type=\"l\",lwd=2,\n     main=\"Fit of 1-year yields (2 PCs)\",\n     ylab=\"Obs (black) / Fitted by 2PCs (dashed blue)\")\nlines(USyields$date,Y1.hat,col=\"blue\",lty=2,lwd=2)\nY10.hat <- PCA.yds$x[,1:nb.PC] %*% PCA.yds$rotation[\"Y10\",1:2]\nY10.hat <- mean(USyields$Y10) + sd(USyields$Y10) * Y10.hat\nplot(USyields$date,USyields$Y10,type=\"l\",lwd=2,\n     main=\"Fit of 10-year yields (2 PCs)\",\n     ylab=\"Obs (black) / Fitted by 2PCs (dashed blue)\")\nlines(USyields$date,Y10.hat,col=\"blue\",lty=2,lwd=2)"},{"path":"append.html","id":"LinAlgebra","chapter":"3 Appendix","heading":"3.2 Linear algebra: definitions and results","text":"Definition 3.1  (Eigenvalues) eigenvalues matrix \\(M\\) numbers \\(\\lambda\\) :\n\\[\n|M - \\lambda | = 0,\n\\]\n\\(| \\bullet |\\) determinant operator.Proposition 3.1  (Properties determinant) :\\(|MN|=|M|\\times|N|\\).\\(|M^{-1}|=|M|^{-1}\\).\\(M\\) admits diagonal representation \\(M=TDT^{-1}\\), \\(D\\) diagonal matrix whose diagonal entries \\(\\{\\lambda_i\\}_{=1,\\dots,n}\\), :\n\\[\n|M - \\lambda |=\\prod_{=1}^n (\\lambda_i - \\lambda).\n\\]Definition 3.2  (Moore-Penrose inverse) \\(M \\\\mathbb{R}^{m \\times n}\\), Moore-Penrose pseudo inverse (exists ) unique matrix \\(M^* \\\\mathbb{R}^{n \\times m}\\) satisfies:\\(M M^* M = M\\)\\(M^* M M^* = M^*\\)\\((M M^*)'=M M^*\\)\n.iv \\((M^* M)'=M^* M\\).Proposition 3.2  (Properties Moore-Penrose inverse) \\(M\\) invertible \\(M^* = M^{-1}\\).pseudo-inverse zero matrix transpose.\n*\npseudo-inverse pseudo-inverse original matrix.Definition 3.3  (Idempotent matrix) Matrix \\(M\\) idempotent \\(M^2=M\\).\\(M\\) symmetric idempotent matrix, \\(M'M=M\\).Proposition 3.3  (Roots idempotent matrix) eigenvalues idempotent matrix either 1 0.Proof. \\(\\lambda\\) eigenvalue idempotent matrix \\(M\\) \\(\\exists x \\ne 0\\) s.t. \\(Mx=\\lambda x\\). Hence \\(M^2x=\\lambda M x \\Rightarrow (1-\\lambda)Mx=0\\). Either element \\(Mx\\) zero, case \\(\\lambda=0\\) least one element \\(Mx\\) nonzero, case \\(\\lambda=1\\).Proposition 3.4  (Idempotent matrix chi-square distribution) rank symmetric idempotent matrix equal trace.Proof. result follows Prop. 3.3, combined fact rank symmetric matrix equal number nonzero eigenvalues.Proposition 3.5  (Constrained least squares) solution following optimisation problem:\n\\[\\begin{eqnarray*}\n\\underset{\\boldsymbol\\beta}{\\min} && || \\mathbf{y} - \\mathbf{X}\\boldsymbol\\beta ||^2 \\\\\n&& \\mbox{subject } \\mathbf{R}\\boldsymbol\\beta = \\mathbf{q}\n\\end{eqnarray*}\\]\ngiven :\n\\[\n\\boxed{\\boldsymbol\\beta^r = \\boldsymbol\\beta_0 - (\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{R}'\\{\\mathbf{R}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{R}'\\}^{-1}(\\mathbf{R}\\boldsymbol\\beta_0 - \\mathbf{q}),}\n\\]\n\\(\\boldsymbol\\beta_0=(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{y}\\).Proof. See instance Jackman, 2007.Proposition 3.6  (Inverse partitioned matrix) :\n\\[\\begin{eqnarray*}\n&&\\left[ \\begin{array}{cc} \\mathbf{}_{11} & \\mathbf{}_{12} \\\\ \\mathbf{}_{21} & \\mathbf{}_{22} \\end{array}\\right]^{-1} = \\\\\n&&\\left[ \\begin{array}{cc} (\\mathbf{}_{11} - \\mathbf{}_{12}\\mathbf{}_{22}^{-1}\\mathbf{}_{21})^{-1} & - \\mathbf{}_{11}^{-1}\\mathbf{}_{12}(\\mathbf{}_{22} - \\mathbf{}_{21}\\mathbf{}_{11}^{-1}\\mathbf{}_{12})^{-1} \\\\\n-(\\mathbf{}_{22} - \\mathbf{}_{21}\\mathbf{}_{11}^{-1}\\mathbf{}_{12})^{-1}\\mathbf{}_{21}\\mathbf{}_{11}^{-1} & (\\mathbf{}_{22} - \\mathbf{}_{21}\\mathbf{}_{11}^{-1}\\mathbf{}_{12})^{-1} \\end{array} \\right].\n\\end{eqnarray*}\\]Definition 3.4  (Matrix derivatives) Consider fonction \\(f: \\mathbb{R}^K \\rightarrow \\mathbb{R}\\). first-order derivative :\n\\[\n\\frac{\\partial f}{\\partial \\mathbf{b}}(\\mathbf{b}) =\n\\left[\\begin{array}{c}\n\\frac{\\partial f}{\\partial b_1}(\\mathbf{b})\\\\\n\\vdots\\\\\n\\frac{\\partial f}{\\partial b_K}(\\mathbf{b})\n\\end{array}\n\\right].\n\\]\nuse notation:\n\\[\n\\frac{\\partial f}{\\partial \\mathbf{b}'}(\\mathbf{b}) = \\left(\\frac{\\partial f}{\\partial \\mathbf{b}}(\\mathbf{b})\\right)'.\n\\]Proposition 3.7  :\\(f(\\mathbf{b}) = ' \\mathbf{b}\\) \\(\\) \\(K \\times 1\\) vector \\(\\frac{\\partial f}{\\partial \\mathbf{b}}(\\mathbf{b}) = \\).\\(f(\\mathbf{b}) = \\mathbf{b}'\\mathbf{b}\\) \\(\\) \\(K \\times K\\) matrix, \\(\\frac{\\partial f}{\\partial \\mathbf{b}}(\\mathbf{b}) = 2A\\mathbf{b}\\).Proposition 3.8  (Square absolute summability) :\n\\[\n\\underbrace{\\sum_{=0}^{\\infty}|\\theta_i| < + \\infty}_{\\mbox{Absolute summability}} \\Rightarrow \\underbrace{\\sum_{=0}^{\\infty} \\theta_i^2 < + \\infty}_{\\mbox{Square summability}}.\n\\]Proof. See Appendix 3.Hamilton. Idea: Absolute summability implies exist \\(N\\) , \\(j>N\\), \\(|\\theta_j| < 1\\) (deduced Cauchy criterion, Theorem 3.2 therefore \\(\\theta_j^2 < |\\theta_j|\\).","code":""},{"path":"append.html","id":"variousResults","chapter":"3 Appendix","heading":"3.3 Statistical analysis: definitions and results","text":"","code":""},{"path":"append.html","id":"moments-and-statistics","chapter":"3 Appendix","heading":"3.3.1 Moments and statistics","text":"Definition 3.5  (Partial correlation) partial correlation \\(y\\) \\(z\\), controlling variables \\(\\mathbf{X}\\) sample correlation \\(y^*\\) \\(z^*\\), latter two variables residuals regressions \\(y\\) \\(\\mathbf{X}\\) \\(z\\) \\(\\mathbf{X}\\), respectively.correlation denoted \\(r_{yz}^\\mathbf{X}\\). definition, :\n\\[\\begin{equation}\nr_{yz}^\\mathbf{X} = \\frac{\\mathbf{z^*}'\\mathbf{y^*}}{\\sqrt{(\\mathbf{z^*}'\\mathbf{z^*})(\\mathbf{y^*}'\\mathbf{y^*})}}.\\tag{3.2}\n\\end{equation}\\]Definition 3.6  (Skewness kurtosis) Let \\(Y\\) random variable whose fourth moment exists. expectation \\(Y\\) denoted \\(\\mu\\).skewness \\(Y\\) given :\n\\[\n\\frac{\\mathbb{E}[(Y-\\mu)^3]}{\\{\\mathbb{E}[(Y-\\mu)^2]\\}^{3/2}}.\n\\]kurtosis \\(Y\\) given :\n\\[\n\\frac{\\mathbb{E}[(Y-\\mu)^4]}{\\{\\mathbb{E}[(Y-\\mu)^2]\\}^{2}}.\n\\]Theorem 3.1  (Cauchy-Schwarz inequality) :\n\\[\n|\\mathbb{C}ov(X,Y)| \\le \\sqrt{\\mathbb{V}ar(X)\\mathbb{V}ar(Y)}\n\\]\n, \\(X \\ne =\\) \\(Y \\ne 0\\), equality holds iff \\(X\\) \\(Y\\) affine transformation.Proof. \\(\\mathbb{V}ar(X)=0\\), trivial. case, let’s define \\(Z\\) \\(Z = Y - \\frac{\\mathbb{C}ov(X,Y)}{\\mathbb{V}ar(X)}X\\). easily seen \\(\\mathbb{C}ov(X,Z)=0\\). , variance \\(Y=Z+\\frac{\\mathbb{C}ov(X,Y)}{\\mathbb{V}ar(X)}X\\) equal sum variance \\(Z\\) variance \\(\\frac{\\mathbb{C}ov(X,Y)}{\\mathbb{V}ar(X)}X\\), :\n\\[\n\\mathbb{V}ar(Y) = \\mathbb{V}ar(Z) + \\left(\\frac{\\mathbb{C}ov(X,Y)}{\\mathbb{V}ar(X)}\\right)^2\\mathbb{V}ar(X) \\ge \\left(\\frac{\\mathbb{C}ov(X,Y)}{\\mathbb{V}ar(X)}\\right)^2\\mathbb{V}ar(X).\n\\]\nequality holds iff \\(\\mathbb{V}ar(Z)=0\\), .e. iff \\(Y = \\frac{\\mathbb{C}ov(X,Y)}{\\mathbb{V}ar(X)}X+cst\\).Definition 3.7  (Asymptotic level) asymptotic test critical region \\(\\Omega_n\\) asymptotic level equal \\(\\alpha\\) :\n\\[\n\\underset{\\theta \\\\Theta}{\\mbox{sup}} \\quad \\underset{n \\rightarrow \\infty}{\\mbox{lim}} \\mathbb{P}_\\theta (S_n \\\\Omega_n) = \\alpha,\n\\]\n\\(S_n\\) test statistic \\(\\Theta\\) null hypothesis \\(H_0\\) equivalent \\(\\theta \\\\Theta\\).Definition 3.8  (Asymptotically consistent test) asymptotic test critical region \\(\\Omega_n\\) consistent :\n\\[\n\\forall \\theta \\\\Theta^c, \\quad \\mathbb{P}_\\theta (S_n \\\\Omega_n) \\rightarrow 1,\n\\]\n\\(S_n\\) test statistic \\(\\Theta^c\\) null hypothesis \\(H_0\\) equivalent \\(\\theta \\notin \\Theta^c\\).Definition 3.9  (Kullback discrepancy) Given two p.d.f. \\(f\\) \\(f^*\\), Kullback discrepancy defined :\n\\[\n(f,f^*) = \\mathbb{E}^* \\left( \\log \\frac{f^*(Y)}{f(Y)} \\right) = \\int \\log \\frac{f^*(y)}{f(y)} f^*(y) dy.\n\\]Proposition 3.9  (Properties Kullback discrepancy) :\\((f,f^*) \\ge 0\\)\\((f,f^*) = 0\\) iff \\(f \\equiv f^*\\).Proof. \\(x \\rightarrow -\\log(x)\\) convex function. Therefore \\(\\mathbb{E}^*(-\\log f(Y)/f^*(Y)) \\ge -\\log \\mathbb{E}^*(f(Y)/f^*(Y)) = 0\\) (proves ()). Since \\(x \\rightarrow -\\log(x)\\) strictly convex, equality () holds \\(f(Y)/f^*(Y)\\) constant (proves (ii)).Definition 3.10  (Characteristic function) real-valued random variable \\(X\\), characteristic function defined :\n\\[\n\\phi_X: u \\rightarrow \\mathbb{E}[\\exp(iuX)].\n\\]","code":""},{"path":"append.html","id":"standard-distributions","chapter":"3 Appendix","heading":"3.3.2 Standard distributions","text":"Definition 3.11  (F distribution) Consider \\(n=n_1+n_2\\) ..d. \\(\\mathcal{N}(0,1)\\) r.v. \\(X_i\\). r.v. \\(F\\) defined :\n\\[\nF = \\frac{\\sum_{=1}^{n_1} X_i^2}{\\sum_{j=n_1+1}^{n_1+n_2} X_j^2}\\frac{n_2}{n_1}\n\\]\n\\(F \\sim \\mathcal{F}(n_1,n_2)\\). (See Table 3.4 quantiles.)Definition 3.12  (Student-t distribution) \\(Z\\) follows Student-t (\\(t\\)) distribution \\(\\nu\\) degrees freedom (d.f.) :\n\\[\nZ = X_0 \\bigg/ \\sqrt{\\frac{\\sum_{=1}^{\\nu}X_i^2}{\\nu}}, \\quad X_i \\sim ..d. \\mathcal{N}(0,1).\n\\]\n\\(\\mathbb{E}(Z)=0\\), \\(\\mathbb{V}ar(Z)=\\frac{\\nu}{\\nu-2}\\) \\(\\nu>2\\). (See Table 3.2 quantiles.)Definition 3.13  (Chi-square distribution) \\(Z\\) follows \\(\\chi^2\\) distribution \\(\\nu\\) d.f. \\(Z = \\sum_{=1}^{\\nu}X_i^2\\) \\(X_i \\sim ..d. \\mathcal{N}(0,1)\\).\n\\(\\mathbb{E}(Z)=\\nu\\). (See Table 3.3 quantiles.)Proposition 3.10  (Inner product multivariate Gaussian variable) Let \\(X\\) \\(n\\)-dimensional multivariate Gaussian variable: \\(X \\sim \\mathcal{N}(0,\\Sigma)\\). :\n\\[\nX' \\Sigma^{-1}X \\sim \\chi^2(n).\n\\]Proof. \\(\\Sigma\\) symmetrical definite positive matrix, admits spectral decomposition \\(PDP'\\) \\(P\\) orthogonal matrix (.e. \\(PP'=Id\\)) D diagonal matrix non-negative entries. Denoting \\(\\sqrt{D^{-1}}\\) diagonal matrix whose diagonal entries inverse \\(D\\), easily checked covariance matrix \\(Y:=\\sqrt{D^{-1}}P'X\\) \\(Id\\). Therefore \\(Y\\) vector uncorrelated Gaussian variables. properties Gaussian variables imply components \\(Y\\) also independent. Hence \\(Y'Y=\\sum_i Y_i^2 \\sim \\chi^2(n)\\).remains note \\(Y'Y=X'PD^{-1}P'X=X'\\mathbb{V}ar(X)^{-1}X\\) conclude.Definition 3.14  (Generalized Extreme Value (GEV) distribution) vector disturbances \\(\\boldsymbol\\varepsilon=[\\varepsilon_{1,1},\\dots,\\varepsilon_{1,K_1},\\dots,\\varepsilon_{J,1},\\dots,\\varepsilon_{J,K_J}]'\\) follows Generalized Extreme Value (GEV) distribution c.d.f. :\n\\[\nF(\\boldsymbol\\varepsilon,\\boldsymbol\\rho) = \\exp(-G(e^{-\\varepsilon_{1,1}},\\dots,e^{-\\varepsilon_{J,K_J}};\\boldsymbol\\rho))\n\\]\n\n\\[\\begin{eqnarray*}\nG(\\mathbf{Y};\\boldsymbol\\rho) &\\equiv&  G(Y_{1,1},\\dots,Y_{1,K_1},\\dots,Y_{J,1},\\dots,Y_{J,K_J};\\boldsymbol\\rho) \\\\\n&=& \\sum_{j=1}^J\\left(\\sum_{k=1}^{K_j} Y_{jk}^{1/\\rho_j}\n\\right)^{\\rho_j}\n\\end{eqnarray*}\\]","code":""},{"path":"append.html","id":"stochastic-convergences","chapter":"3 Appendix","heading":"3.3.3 Stochastic convergences","text":"Proposition 3.11  (Chebychev's inequality) \\(\\mathbb{E}(|X|^r)\\) finite \\(r>0\\) :\n\\[\n\\forall \\varepsilon > 0, \\quad \\mathbb{P}(|X - c|>\\varepsilon) \\le \\frac{\\mathbb{E}[|X - c|^r]}{\\varepsilon^r}.\n\\]\nparticular, \\(r=2\\):\n\\[\n\\forall \\varepsilon > 0, \\quad \\mathbb{P}(|X - c|>\\varepsilon) \\le \\frac{\\mathbb{E}[(X - c)^2]}{\\varepsilon^2}.\n\\]Proof. Remark \\(\\varepsilon^r \\mathbb{}_{\\{|X| \\ge \\varepsilon\\}} \\le |X|^r\\) take expectation sides.Definition 3.15  (Convergence probability) random variable sequence \\(x_n\\) converges probability constant \\(c\\) \\(\\forall \\varepsilon\\), \\(\\lim_{n \\rightarrow \\infty} \\mathbb{P}(|x_n - c|>\\varepsilon) = 0\\).denoted : \\(\\mbox{plim } x_n = c\\).Definition 3.16  (Convergence Lr norm) \\(x_n\\) converges \\(r\\)-th mean (\\(L^r\\)-norm) towards \\(x\\), \\(\\mathbb{E}(|x_n|^r)\\) \\(\\mathbb{E}(|x|^r)\\) exist \n\\[\n\\lim_{n \\rightarrow \\infty} \\mathbb{E}(|x_n - x|^r) = 0.\n\\]\ndenoted : \\(x_n \\overset{L^r}{\\rightarrow} c\\).\\(r=2\\), convergence called mean square convergence.Definition 3.17  (Almost sure convergence) random variable sequence \\(x_n\\) converges almost surely \\(c\\) \\(\\mathbb{P}(\\lim_{n \\rightarrow \\infty} x_n = c) = 1\\).denoted : \\(x_n \\overset{.s.}{\\rightarrow} c\\).Definition 3.18  (Convergence distribution) \\(x_n\\) said converge distribution (law) \\(x\\) \n\\[\n\\lim_{n \\rightarrow \\infty} F_{x_n}(s) = F_{x}(s)\n\\]\n\\(s\\) \\(F_X\\) –cumulative distribution \\(X\\)– continuous.denoted : \\(x_n \\overset{d}{\\rightarrow} x\\).Proposition 3.12  (Rules limiting distributions (Slutsky)) :Slutsky’s theorem: \\(x_n \\overset{d}{\\rightarrow} x\\) \\(y_n \\overset{p}{\\rightarrow} c\\) \n\\[\\begin{eqnarray*}\nx_n y_n &\\overset{d}{\\rightarrow}& x c \\\\\nx_n + y_n &\\overset{d}{\\rightarrow}& x + c \\\\\nx_n/y_n &\\overset{d}{\\rightarrow}& x / c \\quad (\\mbox{}c \\ne 0)\n\\end{eqnarray*}\\]Slutsky’s theorem: \\(x_n \\overset{d}{\\rightarrow} x\\) \\(y_n \\overset{p}{\\rightarrow} c\\) \n\\[\\begin{eqnarray*}\nx_n y_n &\\overset{d}{\\rightarrow}& x c \\\\\nx_n + y_n &\\overset{d}{\\rightarrow}& x + c \\\\\nx_n/y_n &\\overset{d}{\\rightarrow}& x / c \\quad (\\mbox{}c \\ne 0)\n\\end{eqnarray*}\\]Continuous mapping theorem: \\(x_n \\overset{d}{\\rightarrow} x\\) \\(g\\) continuous function \\(g(x_n) \\overset{d}{\\rightarrow} g(x).\\)Continuous mapping theorem: \\(x_n \\overset{d}{\\rightarrow} x\\) \\(g\\) continuous function \\(g(x_n) \\overset{d}{\\rightarrow} g(x).\\)Proposition 3.13  (Implications stochastic convergences) :\n\\[\\begin{align*}\n&\\boxed{\\overset{L^s}{\\rightarrow}}& &\\underset{1 \\le r \\le s}{\\Rightarrow}& &\\boxed{\\overset{L^r}{\\rightarrow}}&\\\\\n&& && &\\Downarrow&\\\\\n&\\boxed{\\overset{.s.}{\\rightarrow}}& &\\Rightarrow& &\\boxed{\\overset{p}{\\rightarrow}}& \\Rightarrow \\qquad \\boxed{\\overset{d}{\\rightarrow}}.\n\\end{align*}\\]Proof. (fact \\(\\left(\\overset{p}{\\rightarrow}\\right) \\Rightarrow \\left( \\overset{d}{\\rightarrow}\\right)\\)). Assume \\(X_n \\overset{p}{\\rightarrow} X\\). Denoting \\(F\\) \\(F_n\\) c.d.f. \\(X\\) \\(X_n\\), respectively:\n\\[\\begin{equation}\nF_n(x) = \\mathbb{P}(X_n \\le x,X\\le x+\\varepsilon) + \\mathbb{P}(X_n \\le x,X > x+\\varepsilon) \\le F(x+\\varepsilon) + \\mathbb{P}(|X_n - X|>\\varepsilon).\\tag{3.3}\n\\end{equation}\\]\nBesides,\n\\[\nF(x-\\varepsilon) = \\mathbb{P}(X \\le x-\\varepsilon,X_n \\le x) + \\mathbb{P}(X \\le x-\\varepsilon,X_n > x) \\le F_n(x) + \\mathbb{P}(|X_n - X|>\\varepsilon),\n\\]\nimplies:\n\\[\\begin{equation}\nF(x-\\varepsilon) - \\mathbb{P}(|X_n - X|>\\varepsilon) \\le F_n(x).\\tag{3.4}\n\\end{equation}\\]\nEqs. (3.3) (3.4) imply:\n\\[\nF(x-\\varepsilon) - \\mathbb{P}(|X_n - X|>\\varepsilon) \\le F_n(x)  \\le F(x+\\varepsilon) + \\mathbb{P}(|X_n - X|>\\varepsilon).\n\\]\nTaking limits \\(n \\rightarrow \\infty\\) yields\n\\[\nF(x-\\varepsilon) \\le \\underset{n \\rightarrow \\infty}{\\mbox{lim inf}}\\; F_n(x) \\le \\underset{n \\rightarrow \\infty}{\\mbox{lim sup}}\\; F_n(x)  \\le F(x+\\varepsilon).\n\\]\nresult obtained taking limits \\(\\varepsilon \\rightarrow 0\\) (\\(F\\) continuous \\(x\\)).Proposition 3.14  (Convergence distribution constant) \\(X_n\\) converges distribution constant \\(c\\), \\(X_n\\) converges probability \\(c\\).Proof. \\(\\varepsilon>0\\), \\(\\mathbb{P}(X_n < c - \\varepsilon) \\underset{n \\rightarrow \\infty}{\\rightarrow} 0\\) .e. \\(\\mathbb{P}(X_n \\ge c - \\varepsilon) \\underset{n \\rightarrow \\infty}{\\rightarrow} 1\\) \\(\\mathbb{P}(X_n < c + \\varepsilon) \\underset{n \\rightarrow \\infty}{\\rightarrow} 1\\). Therefore \\(\\mathbb{P}(c - \\varepsilon \\le X_n < c + \\varepsilon) \\underset{n \\rightarrow \\infty}{\\rightarrow} 1\\),\ngives result.Example 3.1  (Convergence probability $L^r$) Let \\(\\{x_n\\}_{n \\\\mathbb{N}}\\) series random variables defined :\n\\[\nx_n = n u_n,\n\\]\n\\(u_n\\) independent random variables s.t. \\(u_n \\sim \\mathcal{B}(1/n)\\).\\(x_n \\overset{p}{\\rightarrow} 0\\) \\(x_n \\overset{L^r}{\\nrightarrow} 0\\) \\(\\mathbb{E}(|X_n-0|)=\\mathbb{E}(X_n)=1\\).Theorem 3.2  (Cauchy criterion (non-stochastic case)) \\(\\sum_{=0}^{T} a_i\\) converges (\\(T \\rightarrow \\infty\\)) iff, \\(\\eta > 0\\), exists integer \\(N\\) , \\(M\\ge N\\),\n\\[\n\\left|\\sum_{=N+1}^{M} a_i\\right| < \\eta.\n\\]Theorem 3.3  (Cauchy criterion (stochastic case)) \\(\\sum_{=0}^{T} \\theta_i \\varepsilon_{t-}\\) converges mean square (\\(T \\rightarrow \\infty\\)) random variable iff, \\(\\eta > 0\\), exists integer \\(N\\) , \\(M\\ge N\\),\n\\[\n\\mathbb{E}\\left[\\left(\\sum_{=N+1}^{M} \\theta_i \\varepsilon_{t-}\\right)^2\\right] < \\eta.\n\\]","code":""},{"path":"append.html","id":"central-limit-theorem","chapter":"3 Appendix","heading":"3.3.4 Central limit theorem","text":"Theorem 3.4  (Law large numbers) sample mean consistent estimator population mean.Proof. Let’s denote \\(\\phi_{X_i}\\) characteristic function r.v. \\(X_i\\). mean \\(X_i\\) \\(\\mu\\) Talyor expansion characteristic function :\n\\[\n\\phi_{X_i}(u) = \\mathbb{E}(\\exp(iuX)) = 1 + iu\\mu + o(u).\n\\]\nproperties characteristic function (see Def. 3.10) imply :\n\\[\n\\phi_{\\frac{1}{n}(X_1+\\dots+X_n)}(u) = \\prod_{=1}^{n} \\left(1 + \\frac{u}{n}\\mu + o\\left(\\frac{u}{n}\\right) \\right) \\rightarrow e^{iu\\mu}.\n\\]\nfacts () \\(e^{iu\\mu}\\) characteristic function constant \\(\\mu\\) (b) characteristic function uniquely characterises distribution imply sample mean converges distribution constant \\(\\mu\\), implies converges probability \\(\\mu\\).Theorem 3.5  (Lindberg-Levy Central limit theorem, CLT) \\(x_n\\) ..d. sequence random variables mean \\(\\mu\\) variance \\(\\sigma^2\\) (\\(\\]0,+\\infty[\\)), :\n\\[\n\\boxed{\\sqrt{n} (\\bar{x}_n - \\mu) \\overset{d}{\\rightarrow} \\mathcal{N}(0,\\sigma^2), \\quad \\mbox{} \\quad \\bar{x}_n = \\frac{1}{n} \\sum_{=1}^{n} x_i.}\n\\]Proof. Let us introduce r.v. \\(Y_n:= \\sqrt{n}(\\bar{X}_n - \\mu)\\). \\(\\phi_{Y_n}(u) = \\left[ \\mathbb{E}\\left( \\exp(\\frac{1}{\\sqrt{n}} u (X_1 - \\mu)) \\right) \\right]^n\\). :\n\\[\\begin{eqnarray*}\n\\left[ \\mathbb{E}\\left( \\exp\\left(\\frac{1}{\\sqrt{n}} u (X_1 - \\mu)\\right) \\right) \\right]^n &=& \\left[ \\mathbb{E}\\left( 1 + \\frac{1}{\\sqrt{n}} u (X_1 - \\mu) - \\frac{1}{2n} u^2 (X_1 - \\mu)^2 + o(u^2) \\right) \\right]^n \\\\\n&=& \\left( 1 - \\frac{1}{2n}u^2\\sigma^2 + o(u^2)\\right)^n.\n\\end{eqnarray*}\\]\nTherefore \\(\\phi_{Y_n}(u) \\underset{n \\rightarrow \\infty}{\\rightarrow} \\exp \\left( - \\frac{1}{2}u^2\\sigma^2 \\right)\\), characteristic function \\(\\mathcal{N}(0,\\sigma^2)\\).","code":""},{"path":"append.html","id":"GaussianVar","chapter":"3 Appendix","heading":"3.4 Some properties of Gaussian variables","text":"Proposition 3.15  \\(\\mathbf{}\\) idempotent \\(\\mathbf{x}\\) Gaussian, \\(\\mathbf{L}\\mathbf{x}\\) \\(\\mathbf{x}'\\mathbf{}\\mathbf{x}\\) independent \\(\\mathbf{L}\\mathbf{}=\\mathbf{0}\\).Proof. \\(\\mathbf{L}\\mathbf{}=\\mathbf{0}\\), two Gaussian vectors \\(\\mathbf{L}\\mathbf{x}\\) \\(\\mathbf{}\\mathbf{x}\\) independent. implies independence function \\(\\mathbf{L}\\mathbf{x}\\) function \\(\\mathbf{}\\mathbf{x}\\). results follows observation \\(\\mathbf{x}'\\mathbf{}\\mathbf{x}=(\\mathbf{}\\mathbf{x})'(\\mathbf{}\\mathbf{x})\\), function \\(\\mathbf{}\\mathbf{x}\\).Proposition 3.16  (Bayesian update vector Gaussian variables) \n\\[\n\\left[\n\\begin{array}{c}\nY_1\\\\\nY_2\n\\end{array}\n\\right]\n\\sim \\mathcal{N}\n\\left(0,\n\\left[\\begin{array}{cc}\n\\Omega_{11} & \\Omega_{12}\\\\\n\\Omega_{21} & \\Omega_{22}\n\\end{array}\\right]\n\\right),\n\\]\n\n\\[\nY_{2}|Y_{1} \\sim \\mathcal{N}\n\\left(\n\\Omega_{21}\\Omega_{11}^{-1}Y_{1},\\Omega_{22}-\\Omega_{21}\\Omega_{11}^{-1}\\Omega_{12}\n\\right).\n\\]\n\\[\nY_{1}|Y_{2} \\sim \\mathcal{N}\n\\left(\n\\Omega_{12}\\Omega_{22}^{-1}Y_{2},\\Omega_{11}-\\Omega_{12}\\Omega_{22}^{-1}\\Omega_{21}\n\\right).\n\\]Proposition 3.17  (Truncated distributions) \\(X\\) random variable distributed according p.d.f. \\(f\\), c.d.f. \\(F\\), infinite support. p.d.f. \\(X|\\le X < b\\) \n\\[\ng(x) = \\frac{f(x)}{F(b)-F()}\\mathbb{}_{\\{\\le x < b\\}},\n\\]\n\\(<b\\).partiucular, Gaussian variable \\(X \\sim \\mathcal{N}(\\mu,\\sigma^2)\\), \n\\[\nf(X=x|\\le X<b) = \\dfrac{\\dfrac{1}{\\sigma}\\phi\\left(\\dfrac{x - \\mu}{\\sigma}\\right)}{Z}.\n\\]\n\\(Z = \\Phi(\\beta)-\\Phi(\\alpha)\\), \\(\\alpha = \\dfrac{- \\mu}{\\sigma}\\) \\(\\beta = \\dfrac{b - \\mu}{\\sigma}\\).Moreover:\n\\[\\begin{eqnarray}\n\\mathbb{E}(X|\\le X<b) &=& \\mu - \\frac{\\phi\\left(\\beta\\right)-\\phi\\left(\\alpha\\right)}{Z}\\sigma. \\tag{3.5}\n\\end{eqnarray}\\]also :\n\\[\\begin{eqnarray}\n&& \\mathbb{V}ar(X|\\le X<b) \\nonumber\\\\\n&=& \\sigma^2\\left[\n1 -  \\frac{\\beta\\phi\\left(\\beta\\right)-\\alpha\\phi\\left(\\alpha\\right)}{Z} -  \\left(\\frac{\\phi\\left(\\beta\\right)-\\phi\\left(\\alpha\\right)}{Z}\\right)^2 \\right] \\tag{3.6}\n\\end{eqnarray}\\]particular, \\(b \\rightarrow \\infty\\), get:\n\\[\\begin{equation}\n\\mathbb{V}ar(X|< X) = \\sigma^2\\left[1 + \\alpha\\lambda(-\\alpha) - \\lambda(-\\alpha)^2 \\right], \\tag{3.7}\n\\end{equation}\\]\n\\(\\lambda(x)=\\dfrac{\\phi(x)}{\\Phi(x)}\\) called inverse Mills ratio.Consider case \\(\\rightarrow - \\infty\\) (.e. conditioning set \\(X<b\\)) \\(\\mu=0\\), \\(\\sigma=1\\). Eq. (3.5) gives \\(\\mathbb{E}(X|X<b) = - \\lambda(b) = - \\dfrac{\\phi(b)}{\\Phi(b)}\\), \\(\\lambda\\) function computing inverse Mills ratio.\nFigure 3.2: \\(\\mathbb{E}(X|X<b)\\) function \\(b\\) \\(X\\sim \\mathcal{N}(0,1)\\) (black).\nProposition 3.18  (p.d.f. multivariate Gaussian variable) \\(Y \\sim \\mathcal{N}(\\mu,\\Omega)\\) \\(Y\\) \\(n\\)-dimensional vector, density function \\(Y\\) :\n\\[\n\\frac{1}{(2 \\pi)^{n/2}|\\Omega|^{1/2}}\\exp\\left[-\\frac{1}{2}\\left(Y-\\mu\\right)'\\Omega^{-1}\\left(Y-\\mu\\right)\\right].\n\\]","code":""},{"path":"append.html","id":"AppendixProof","chapter":"3 Appendix","heading":"3.5 Proofs","text":"Proof Proposition ??Proof. Assumptions () (ii) (set Assumptions ??) imply \\(\\boldsymbol\\theta_{MLE}\\) exists (\\(=\\mbox{argmax}_\\theta (1/n)\\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})\\)).\\((1/n)\\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})\\) can interpreted sample mean r.v. \\(\\log f(Y_i;\\boldsymbol\\theta)\\) ..d. Therefore \\((1/n)\\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})\\) converges \\(\\mathbb{E}_{\\boldsymbol\\theta_0}(\\log f(Y;\\boldsymbol\\theta))\\) – exists (Assumption iv).latter convergence uniform (Assumption v), solution \\(\\boldsymbol\\theta_{MLE}\\) almost surely converges solution limit problem:\n\\[\n\\mbox{argmax}_\\theta \\mathbb{E}_{\\boldsymbol\\theta_0}(\\log f(Y;\\boldsymbol\\theta)) = \\mbox{argmax}_\\theta \\int_{\\mathcal{Y}} \\log f(y;\\boldsymbol\\theta)f(y;\\boldsymbol\\theta_0) dy.\n\\]Properties Kullback information measure (see Prop. 3.9), together identifiability assumption (ii) implies solution limit problem unique equal \\(\\boldsymbol\\theta_0\\).Consider r.v. sequence \\(\\boldsymbol\\theta\\) converges \\(\\boldsymbol\\theta_0\\). Taylor expansion score neighborood \\(\\boldsymbol\\theta_0\\) yields :\n\\[\n\\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})}{\\partial \\boldsymbol\\theta} = \\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta} + \\frac{\\partial^2 \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta'}(\\boldsymbol\\theta - \\boldsymbol\\theta_0) + o_p(\\boldsymbol\\theta - \\boldsymbol\\theta_0)\n\\]\\(\\boldsymbol\\theta_{MLE}\\) converges \\(\\boldsymbol\\theta_0\\) satisfies likelihood equation \\(\\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})}{\\partial \\boldsymbol\\theta} = \\mathbf{0}\\). Therefore:\n\\[\n\\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\approx - \\frac{\\partial^2 \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta'}(\\boldsymbol\\theta_{MLE} - \\boldsymbol\\theta_0),\n\\]\nequivalently:\n\\[\n\\frac{1}{\\sqrt{n}} \\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\approx\n\\left(- \\frac{1}{n} \\sum_{=1}^n \\frac{\\partial^2 \\log f(y_i;\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta'} \\right)\\sqrt{n}(\\boldsymbol\\theta_{MLE} - \\boldsymbol\\theta_0),\n\\]law large numbers, : \\(\\left(- \\frac{1}{n} \\sum_{=1}^n \\frac{\\partial^2 \\log f(y_i;\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta'} \\right) \\overset{}\\rightarrow \\frac{1}{n} \\mathbf{}(\\boldsymbol\\theta_0) = \\mathcal{}_Y(\\boldsymbol\\theta_0)\\).Besides, :\n\\[\\begin{eqnarray*}\n\\frac{1}{\\sqrt{n}} \\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta} &=& \\sqrt{n} \\left( \\frac{1}{n} \\sum_i \\frac{\\partial \\log f(y_i;\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta} \\right) \\\\\n&=& \\sqrt{n} \\left( \\frac{1}{n} \\sum_i \\left\\{ \\frac{\\partial \\log f(y_i;\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta} - \\mathbb{E}_{\\boldsymbol\\theta_0} \\frac{\\partial \\log f(Y_i;\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta} \\right\\} \\right)\n\\end{eqnarray*}\\]\nconverges \\(\\mathcal{N}(0,\\mathcal{}_Y(\\boldsymbol\\theta_0))\\) CLT.Collecting preceding results leads (b). fact \\(\\boldsymbol\\theta_{MLE}\\) achieves FDCR bound proves (c).Proof Proposition ??Proof. \\(\\sqrt{n}(\\hat{\\boldsymbol\\theta}_{n} - \\boldsymbol\\theta_{0}) \\overset{d}{\\rightarrow} \\mathcal{N}(0,\\mathcal{}(\\boldsymbol\\theta_0)^{-1})\\) (Eq. (eq:normMLE)). Taylor expansion around \\(\\boldsymbol\\theta_0\\) yields :\n\\[\\begin{equation}\n\\sqrt{n}(h(\\hat{\\boldsymbol\\theta}_{n}) - h(\\boldsymbol\\theta_{0})) \\overset{d}{\\rightarrow} \\mathcal{N}\\left(0,\\frac{\\partial h(\\boldsymbol\\theta_{0})}{\\partial \\boldsymbol\\theta'}\\mathcal{}(\\boldsymbol\\theta_0)^{-1}\\frac{\\partial h(\\boldsymbol\\theta_{0})'}{\\partial \\boldsymbol\\theta}\\right). \\tag{3.8}\n\\end{equation}\\]\n\\(H_0\\), \\(h(\\boldsymbol\\theta_{0})=0\\) therefore:\n\\[\\begin{equation}\n\\sqrt{n} h(\\hat{\\boldsymbol\\theta}_{n}) \\overset{d}{\\rightarrow} \\mathcal{N}\\left(0,\\frac{\\partial h(\\boldsymbol\\theta_{0})}{\\partial \\boldsymbol\\theta'}\\mathcal{}(\\boldsymbol\\theta_0)^{-1}\\frac{\\partial h(\\boldsymbol\\theta_{0})'}{\\partial \\boldsymbol\\theta}\\right). \\tag{3.9}\n\\end{equation}\\]\nHence\n\\[\n\\sqrt{n} \\left(\n\\frac{\\partial h(\\boldsymbol\\theta_{0})}{\\partial \\boldsymbol\\theta'}\\mathcal{}(\\boldsymbol\\theta_0)^{-1}\\frac{\\partial h(\\boldsymbol\\theta_{0})'}{\\partial \\boldsymbol\\theta}\n\\right)^{-1/2} h(\\hat{\\boldsymbol\\theta}_{n}) \\overset{d}{\\rightarrow} \\mathcal{N}\\left(0,Id\\right).\n\\]\nTaking quadratic form, obtain:\n\\[\nn h(\\hat{\\boldsymbol\\theta}_{n})'  \\left(\n\\frac{\\partial h(\\boldsymbol\\theta_{0})}{\\partial \\boldsymbol\\theta'}\\mathcal{}(\\boldsymbol\\theta_0)^{-1}\\frac{\\partial h(\\boldsymbol\\theta_{0})'}{\\partial \\boldsymbol\\theta}\n\\right)^{-1} h(\\hat{\\boldsymbol\\theta}_{n}) \\overset{d}{\\rightarrow} \\chi^2(r).\n\\]fact test asymptotic level \\(\\alpha\\) directly stems precedes. Consistency test: Consider \\(\\theta_0 \\\\Theta\\). MLE consistent, \\(h(\\hat{\\boldsymbol\\theta}_{n})\\) converges \\(h(\\boldsymbol\\theta_0) \\ne 0\\). Eq. (3.8) still valid. implies \\(\\xi^W_n\\) converges \\(+\\infty\\) therefore \\(\\mathbb{P}_{\\boldsymbol\\theta}(\\xi^W_n \\ge \\chi^2_{1-\\alpha}(r)) \\rightarrow 1\\).Proof Proposition ??Proof. Notations: “\\(\\approx\\)” means “equal term converges 0 probability”. \\(H_0\\). \\(\\hat{\\boldsymbol\\theta}^0\\) constrained ML estimator; \\(\\hat{\\boldsymbol\\theta}\\) denotes unconstrained one.combine two Taylor expansion: \\(h(\\hat{\\boldsymbol\\theta}_n) \\approx \\dfrac{\\partial h(\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta'}(\\hat{\\boldsymbol\\theta}_n - \\boldsymbol\\theta_0)\\) \\(h(\\hat{\\boldsymbol\\theta}_n^0) \\approx \\dfrac{\\partial h(\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta'}(\\hat{\\boldsymbol\\theta}_n^0 - \\boldsymbol\\theta_0)\\) use \\(h(\\hat{\\boldsymbol\\theta}_n^0)=0\\) (definition) get:\n\\[\\begin{equation}\n\\sqrt{n}h(\\hat{\\boldsymbol\\theta}_n) \\approx \\dfrac{\\partial h(\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta'}\\sqrt{n}(\\hat{\\boldsymbol\\theta}_n - \\hat{\\boldsymbol\\theta}^0_n). \\tag{3.10}\n\\end{equation}\\]\nBesides, (using definition information matrix):\n\\[\\begin{equation}\n\\frac{1}{\\sqrt{n}}\\frac{\\partial \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\approx\n\\frac{1}{\\sqrt{n}}\\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta} - \\mathcal{}(\\boldsymbol\\theta_0)\\sqrt{n}(\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0) \\tag{3.11}\n\\end{equation}\\]\n:\n\\[\\begin{equation}\n0=\\frac{1}{\\sqrt{n}}\\frac{\\partial \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\approx\n\\frac{1}{\\sqrt{n}}\\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta} - \\mathcal{}(\\boldsymbol\\theta_0)\\sqrt{n}(\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0).\\tag{3.12}\n\\end{equation}\\]\nTaking difference multiplying \\(\\mathcal{}(\\boldsymbol\\theta_0)^{-1}\\):\n\\[\\begin{equation}\n\\sqrt{n}(\\hat{\\boldsymbol\\theta}_n-\\hat{\\boldsymbol\\theta}_n^0) \\approx\n\\mathcal{}(\\boldsymbol\\theta_0)^{-1}\\frac{1}{\\sqrt{n}}\\frac{\\partial \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta}\n\\mathcal{}(\\boldsymbol\\theta_0).\\tag{3.13}\n\\end{equation}\\]\nEqs. (3.10) (3.13) yield :\n\\[\\begin{equation}\n\\sqrt{n}h(\\hat{\\boldsymbol\\theta}_n) \\approx \\dfrac{\\partial h(\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta'} \\mathcal{}(\\boldsymbol\\theta_0)^{-1}\\frac{1}{\\sqrt{n}}\\frac{\\partial \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta}.\\tag{3.14}\n\\end{equation}\\]Recall \\(\\hat{\\boldsymbol\\theta}^0_n\\) MLE \\(\\boldsymbol\\theta_0\\) constraint \\(h(\\boldsymbol\\theta)=0\\). vector Lagrange multipliers \\(\\hat\\lambda_n\\) associated program satisfies:\n\\[\\begin{equation}\n\\frac{\\partial \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta}+ \\frac{\\partial h'(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta}\\hat\\lambda_n = 0.\\tag{3.15}\n\\end{equation}\\]\nSubstituting latter equation Eq. (3.14) gives:\n\\[\n\\sqrt{n}h(\\hat{\\boldsymbol\\theta}_n) \\approx\n- \\dfrac{\\partial h(\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta'} \\mathcal{}(\\boldsymbol\\theta_0)^{-1}\n\\frac{\\partial h'(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\frac{\\hat\\lambda_n}{\\sqrt{n}} \\approx\n- \\dfrac{\\partial h(\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta'} \\mathcal{}(\\boldsymbol\\theta_0)^{-1}\n\\frac{\\partial h'(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\frac{\\hat\\lambda_n}{\\sqrt{n}}\n\\]\nyields:\n\\[\\begin{equation}\n\\frac{\\hat\\lambda_n}{\\sqrt{n}} \\approx - \\left(\n\\dfrac{\\partial h(\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta'} \\mathcal{}(\\boldsymbol\\theta_0)^{-1}\n\\frac{\\partial h'(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta}\n\\right)^{-1}\n\\sqrt{n}h(\\hat{\\boldsymbol\\theta}_n).\\tag{3.16}\n\\end{equation}\\]\nfollows, Eq. (3.9), :\n\\[\n\\frac{\\hat\\lambda_n}{\\sqrt{n}} \\overset{d}{\\rightarrow} \\mathcal{N}\\left(0,\\left(\n\\dfrac{\\partial h(\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta'} \\mathcal{}(\\boldsymbol\\theta_0)^{-1}\n\\frac{\\partial h'(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta}\n\\right)^{-1}\\right).\n\\]\nTaking quadratic form last equation gives:\n\\[\n\\frac{1}{n}\\hat\\lambda_n' \\dfrac{\\partial h(\\hat{\\boldsymbol\\theta}^0_n)}{\\partial \\boldsymbol\\theta'} \\mathcal{}(\\hat{\\boldsymbol\\theta}^0_n)^{-1}\n\\frac{\\partial h'(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\hat\\lambda_n \\overset{d}{\\rightarrow} \\chi^2(r).\n\\]\nUsing Eq. (3.15), appears left-hand side term last equation \\(\\xi^{LM}\\) defined Eq. (??). Consistency: see Remark 17.3 Gouriéroux Monfort (1995).Proof Proposition ??Proof. Let us first demonstrate asymptotic equivalence \\(\\xi^{LM}\\) \\(\\xi^{LR}\\).second-order taylor expansions \\(\\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n,\\mathbf{y})\\) \\(\\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}_n,\\mathbf{y})\\) :\n\\[\\begin{eqnarray*}\n\\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}_n,\\mathbf{y}) &\\approx& \\log \\mathcal{L}(\\boldsymbol\\theta_0,\\mathbf{y})\n+ \\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0,\\mathbf{y})}{\\partial \\boldsymbol\\theta'}(\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0)\n- \\frac{n}{2} (\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0)' \\mathcal{}(\\boldsymbol\\theta_0) (\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0)\\\\\n\\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n,\\mathbf{y}) &\\approx& \\log \\mathcal{L}(\\boldsymbol\\theta_0,\\mathbf{y})\n+ \\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0,\\mathbf{y})}{\\partial \\boldsymbol\\theta'}(\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0)\n- \\frac{n}{2} (\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0)' \\mathcal{}(\\boldsymbol\\theta_0) (\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0).\n\\end{eqnarray*}\\]\nTaking difference, obtain:\n\\[\n\\xi_n^{LR} \\approx 2\\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0,\\mathbf{y})}{\\partial \\boldsymbol\\theta'}\n(\\hat{\\boldsymbol\\theta}_n-\\hat{\\boldsymbol\\theta}^0_n) + n (\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0)' \\mathcal{}(\\boldsymbol\\theta_0) (\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0) - n (\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0)' \\mathcal{}(\\boldsymbol\\theta_0) (\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0).\n\\]\nUsing \\(\\dfrac{1}{\\sqrt{n}}\\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\approx \\mathcal{}(\\boldsymbol\\theta_0)\\sqrt{n}(\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0)\\) (Eq. (3.12)), :\n\\[\n\\xi_n^{LR} \\approx\n2n(\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0)'\\mathcal{}(\\boldsymbol\\theta_0)\n(\\hat{\\boldsymbol\\theta}_n-\\hat{\\boldsymbol\\theta}^0_n)\n+ n (\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0)' \\mathcal{}(\\boldsymbol\\theta_0) (\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0) - n (\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0)' \\mathcal{}(\\boldsymbol\\theta_0) (\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0).\n\\]\nsecond three terms sum, replace \\((\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0)\\) \\((\\hat{\\boldsymbol\\theta}^0_n-\\hat{\\boldsymbol\\theta}_n+\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0)\\) develop associated product. leads :\n\\[\\begin{equation}\n\\xi_n^{LR} \\approx n (\\hat{\\boldsymbol\\theta}^0_n-\\hat{\\boldsymbol\\theta}_n)' \\mathcal{}(\\boldsymbol\\theta_0)^{-1} (\\hat{\\boldsymbol\\theta}^0_n-\\hat{\\boldsymbol\\theta}_n). \\tag{3.17}\n\\end{equation}\\]\ndifference Eqs. (3.11) (3.12) implies:\n\\[\n\\frac{1}{\\sqrt{n}}\\frac{\\partial \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\approx\n\\mathcal{}(\\boldsymbol\\theta_0)\\sqrt{n}(\\hat{\\boldsymbol\\theta}_n-\\hat{\\boldsymbol\\theta}^0_n),\n\\]\n, associated Eq. @(eq:lr10), gives:\n\\[\n\\xi_n^{LR} \\approx \\frac{1}{n} \\frac{\\partial \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta'} \\mathcal{}(\\boldsymbol\\theta_0)^{-1} \\frac{\\partial \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\approx \\xi_n^{LM}.\n\\]\nHence \\(\\xi_n^{LR}\\) asymptotic distribution \\(\\xi_n^{LM}\\).Let’s show LR test consistent. , note :\n\\[\n\\frac{\\log \\mathcal{L}(\\hat{\\boldsymbol\\theta},\\mathbf{y}) - \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0,\\mathbf{y})}{n} = \\frac{1}{n} \\sum_{=1}^n[\\log f(y_i;\\hat{\\boldsymbol\\theta}_n) - \\log f(y_i;\\hat{\\boldsymbol\\theta}_n^0)] \\rightarrow \\mathbb{E}_0[\\log f(Y;\\boldsymbol\\theta_0) - \\log f(Y;\\boldsymbol\\theta_\\infty)],\n\\]\n\\(\\boldsymbol\\theta_\\infty\\), pseudo true value, \\(h(\\boldsymbol\\theta_\\infty) \\ne 0\\) (definition \\(H_1\\)). Kullback inequality asymptotic identifiability \\(\\boldsymbol\\theta_0\\), follows \\(\\mathbb{E}_0[\\log f(Y;\\boldsymbol\\theta_0) - \\log f(Y;\\boldsymbol\\theta_\\infty)] >0\\). Therefore \\(\\xi_n^{LR} \\rightarrow + \\infty\\) \\(H_1\\).Let us now demonstrate equivalence \\(\\xi^{LM} \\xi^{W}\\).(using Eq. (eq:multiplier)):\n\\[\n\\xi^{LM}_n = \\frac{1}{n}\\hat\\lambda_n' \\dfrac{\\partial h(\\hat{\\boldsymbol\\theta}^0_n)}{\\partial \\boldsymbol\\theta'} \\mathcal{}(\\hat{\\boldsymbol\\theta}^0_n)^{-1}\n\\frac{\\partial h'(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\hat\\lambda_n.\n\\]\nSince, \\(H_0\\), \\(\\hat{\\boldsymbol\\theta}_n^0\\approx\\hat{\\boldsymbol\\theta}_n \\approx {\\boldsymbol\\theta}_0\\), Eq. (3.16) therefore implies :\n\\[\n\\xi^{LM} \\approx n h(\\hat{\\boldsymbol\\theta}_n)' \\left(\n\\dfrac{\\partial h(\\hat{\\boldsymbol\\theta}_n)}{\\partial \\boldsymbol\\theta'} \\mathcal{}(\\hat{\\boldsymbol\\theta}_n)^{-1}\n\\frac{\\partial h'(\\hat{\\boldsymbol\\theta}_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta}\n\\right)^{-1}\nh(\\hat{\\boldsymbol\\theta}_n) = \\xi^{W},\n\\]\ngives result.Proof Eq. (2.3)Proof. :\n\\[\\begin{eqnarray*}\n&&T\\mathbb{E}\\left[(\\bar{y}_T - \\mu)^2\\right]\\\\\n&=& T\\mathbb{E}\\left[\\left(\\frac{1}{T}\\sum_{t=1}^T(y_t - \\mu)\\right)^2\\right] = \\frac{1}{T} \\mathbb{E}\\left[\\sum_{t=1}^T(y_t - \\mu)^2+2\\sum_{s<t\\le T}(y_t - \\mu)(y_s - \\mu)\\right]\\\\\n&=& \\gamma_0 +\\frac{2}{T}\\left(\\sum_{t=2}^{T}\\mathbb{E}\\left[(y_t - \\mu)(y_{t-1} - \\mu)\\right]\\right) +\\frac{2}{T}\\left(\\sum_{t=3}^{T}\\mathbb{E}\\left[(y_t - \\mu)(y_{t-2} - \\mu)\\right]\\right) + \\dots \\\\\n&&+ \\frac{2}{T}\\left(\\sum_{t=T-1}^{T}\\mathbb{E}\\left[(y_t - \\mu)(y_{t-(T-2)} - \\mu)\\right]\\right) + \\frac{2}{T}\\mathbb{E}\\left[(y_t - \\mu)(y_{t-(T-1)} - \\mu)\\right]\\\\\n&=&  \\gamma_0 + 2 \\frac{T-1}{T}\\gamma_1 + \\dots + 2 \\frac{1}{T}\\gamma_{T-1} .\n\\end{eqnarray*}\\]\nTherefore:\n\\[\\begin{eqnarray*}\nT\\mathbb{E}\\left[(\\bar{y}_T - \\mu)^2\\right] - \\sum_{j=-\\infty}^{+\\infty} \\gamma_j &=& - 2\\frac{1}{T}\\gamma_1 - 2\\frac{2}{T}\\gamma_2 - \\dots - 2\\frac{T-1}{T}\\gamma_{T-1} - 2\\gamma_T - 2 \\gamma_{T+1} + \\dots\n\\end{eqnarray*}\\]\n:\n\\[\n\\left|T\\mathbb{E}\\left[(\\bar{y}_T - \\mu)^2\\right] - \\sum_{j=-\\infty}^{+\\infty} \\gamma_j\\right| \\le 2\\frac{1}{T}|\\gamma_1| + 2\\frac{2}{T}|\\gamma_2| + \\dots + 2\\frac{T-1}{T}|\\gamma_{T-1}| + 2|\\gamma_T| + 2 |\\gamma_{T+1}| + \\dots\n\\]\\(q \\le T\\), :\n\\[\\begin{eqnarray*}\n\\left|T\\mathbb{E}\\left[(\\bar{y}_T - \\mu)^2\\right] - \\sum_{j=-\\infty}^{+\\infty} \\gamma_j\\right| &\\le& 2\\frac{1}{T}|\\gamma_1| + 2\\frac{2}{T}|\\gamma_2| + \\dots + 2\\frac{q-1}{T}|\\gamma_{q-1}| +2\\frac{q}{T}|\\gamma_q| +\\\\\n&&2\\frac{q+1}{T}|\\gamma_{q+1}| + \\dots  + 2\\frac{T-1}{T}|\\gamma_{T-1}| + 2|\\gamma_T| + 2 |\\gamma_{T+1}| + \\dots\\\\\n&\\le& \\frac{2}{T}\\left(|\\gamma_1| + 2|\\gamma_2| + \\dots + (q-1)|\\gamma_{q-1}| +q|\\gamma_q|\\right) +\\\\\n&&2|\\gamma_{q+1}| + \\dots  + 2|\\gamma_{T-1}| + 2|\\gamma_T| + 2 |\\gamma_{T+1}| + \\dots\n\\end{eqnarray*}\\]Consider \\(\\varepsilon > 0\\). fact autocovariances absolutely summable implies exists \\(q_0\\) (Cauchy criterion, Theorem 3.2):\n\\[\n2|\\gamma_{q_0+1}|+2|\\gamma_{q_0+2}|+2|\\gamma_{q_0+3}|+\\dots < \\varepsilon/2.\n\\]\n, \\(T > q_0\\), comes :\n\\[\n\\left|T \\mathbb{E}\\left[(\\bar{y}_T - \\mu)^2\\right] - \\sum_{j=-\\infty}^{+\\infty} \\gamma_j\\right|\\le \\frac{2}{T}\\left(|\\gamma_1| + 2|\\gamma_2| + \\dots + (q_0-1)|\\gamma_{q_0-1}| +q_0|\\gamma_{q_0}|\\right) + \\varepsilon/2.\n\\]\n\\(T \\ge 2\\left(|\\gamma_1| + 2|\\gamma_2| + \\dots + (q_0-1)|\\gamma_{q_0-1}| +q_0|\\gamma_{q_0}|\\right)/(\\varepsilon/2)\\) (\\(= f(q_0)\\), say) \n\\[\n\\frac{2}{T}\\left(|\\gamma_1| + 2|\\gamma_2| + \\dots + (q_0-1)|\\gamma_{q_0-1}| +q_0|\\gamma_{q_0}|\\right) \\le \\varepsilon/2.\n\\]\n, \\(T>f(q_0)\\) \\(T>q_0\\), .e. \\(T>\\max(f(q_0),q_0)\\), :\n\\[\n\\left|T \\mathbb{E}\\left[(\\bar{y}_T - \\mu)^2\\right] - \\sum_{j=-\\infty}^{+\\infty} \\gamma_j\\right|\\le \\varepsilon.\n\\]Proof Proposition 2.15Proof. :\n\\[\\begin{eqnarray}\n\\mathbb{E}([y_{t+1} - y^*_{t+1}]^2) &=& \\mathbb{E}\\left([\\color{blue}{\\{y_{t+1} - \\mathbb{E}(y_{t+1}|x_t)\\}} + \\color{red}{\\{\\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}\\}}]^2\\right)\\nonumber\\\\\n&=&  \\mathbb{E}\\left(\\color{blue}{[y_{t+1} - \\mathbb{E}(y_{t+1}|x_t)]}^2\\right) + \\mathbb{E}\\left(\\color{red}{[\\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]}^2\\right)\\nonumber\\\\\n&& + 2\\mathbb{E}\\left( \\color{blue}{[y_{t+1} - \\mathbb{E}(y_{t+1}|x_t)]}\\color{red}{ [\\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]}\\right). \\tag{3.18}\n\\end{eqnarray}\\]\nLet us focus last term. :\n\\[\\begin{eqnarray*}\n&&\\mathbb{E}\\left(\\color{blue}{[y_{t+1} - \\mathbb{E}(y_{t+1}|x_t)]}\\color{red}{ [\\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]}\\right)\\\\\n&=& \\mathbb{E}( \\mathbb{E}( \\color{blue}{[y_{t+1} - \\mathbb{E}(y_{t+1}|x_t)]}\\color{red}{ \\underbrace{[\\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]}_{\\mbox{function $x_t$}}}|x_t))\\\\\n&=& \\mathbb{E}( \\color{red}{ [\\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]} \\mathbb{E}( \\color{blue}{[y_{t+1} - \\mathbb{E}(y_{t+1}|x_t)]}|x_t))\\\\\n&=& \\mathbb{E}( \\color{red}{ [\\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]} \\color{blue}{\\underbrace{[\\mathbb{E}(y_{t+1}|x_t) - \\mathbb{E}(y_{t+1}|x_t)]}_{=0}})=0.\n\\end{eqnarray*}\\]Therefore, Eq. (3.18) becomes:\n\\[\\begin{eqnarray*}\n&&\\mathbb{E}([y_{t+1} - y^*_{t+1}]^2) \\\\\n&=&  \\underbrace{\\mathbb{E}\\left(\\color{blue}{[y_{t+1} - \\mathbb{E}(y_{t+1}|x_t)]}^2\\right)}_{\\mbox{$\\ge 0$ depend $y^*_{t+1}$}} + \\underbrace{\\mathbb{E}\\left(\\color{red}{[\\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]}^2\\right)}_{\\mbox{$\\ge 0$ depends $y^*_{t+1}$}}.\n\\end{eqnarray*}\\]\nimplies \\(\\mathbb{E}([y_{t+1} - y^*_{t+1}]^2)\\) always larger \\(\\color{blue}{\\mathbb{E}([y_{t+1} - \\mathbb{E}(y_{t+1}|x_t)]^2)}\\), therefore minimized second term equal zero, \\(\\mathbb{E}(y_{t+1}|x_t) = y^*_{t+1}\\).Proof Proposition 2.12Proof. Using Proposition ?? (Appendix ??), obtain , conditionally \\(x_1\\), log-likelihood given \n\\[\\begin{eqnarray*}\n\\log\\mathcal{L}(Y_{T};\\theta) & = & -(Tn/2)\\log(2\\pi)+(T/2)\\log\\left|\\Omega^{-1}\\right|\\\\\n&  & -\\frac{1}{2}\\sum_{t=1}^{T}\\left[\\left(y_{t}-\\Pi'x_{t}\\right)'\\Omega^{-1}\\left(y_{t}-\\Pi'x_{t}\\right)\\right].\n\\end{eqnarray*}\\]\nLet’s rewrite last term log-likelihood:\n\\[\\begin{eqnarray*}\n\\sum_{t=1}^{T}\\left[\\left(y_{t}-\\Pi'x_{t}\\right)'\\Omega^{-1}\\left(y_{t}-\\Pi'x_{t}\\right)\\right] & =\\\\\n\\sum_{t=1}^{T}\\left[\\left(y_{t}-\\hat{\\Pi}'x_{t}+\\hat{\\Pi}'x_{t}-\\Pi'x_{t}\\right)'\\Omega^{-1}\\left(y_{t}-\\hat{\\Pi}'x_{t}+\\hat{\\Pi}'x_{t}-\\Pi'x_{t}\\right)\\right] & =\\\\\n\\sum_{t=1}^{T}\\left[\\left(\\hat{\\varepsilon}_{t}+(\\hat{\\Pi}-\\Pi)'x_{t}\\right)'\\Omega^{-1}\\left(\\hat{\\varepsilon}_{t}+(\\hat{\\Pi}-\\Pi)'x_{t}\\right)\\right],\n\\end{eqnarray*}\\]\n\\(j^{th}\\) element \\((n\\times1)\\) vector \\(\\hat{\\varepsilon}_{t}\\) sample residual, observation \\(t\\), OLS regression \\(y_{j,t}\\) \\(x_{t}\\). Expanding previous equation, get:\n\\[\\begin{eqnarray*}\n&&\\sum_{t=1}^{T}\\left[\\left(y_{t}-\\Pi'x_{t}\\right)'\\Omega^{-1}\\left(y_{t}-\\Pi'x_{t}\\right)\\right]  = \\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}'\\Omega^{-1}\\hat{\\varepsilon}_{t}\\\\\n&&+2\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}'\\Omega^{-1}(\\hat{\\Pi}-\\Pi)'x_{t}+\\sum_{t=1}^{T}x'_{t}(\\hat{\\Pi}-\\Pi)\\Omega^{-1}(\\hat{\\Pi}-\\Pi)'x_{t}.\n\\end{eqnarray*}\\]\nLet’s apply trace operator second term (scalar):\n\\[\\begin{eqnarray*}\n\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}'\\Omega^{-1}(\\hat{\\Pi}-\\Pi)'x_{t} & = & Tr\\left(\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}'\\Omega^{-1}(\\hat{\\Pi}-\\Pi)'x_{t}\\right)\\\\\n=  Tr\\left(\\sum_{t=1}^{T}\\Omega^{-1}(\\hat{\\Pi}-\\Pi)'x_{t}\\hat{\\varepsilon}_{t}'\\right) & = & Tr\\left(\\Omega^{-1}(\\hat{\\Pi}-\\Pi)'\\sum_{t=1}^{T}x_{t}\\hat{\\varepsilon}_{t}'\\right).\n\\end{eqnarray*}\\]\nGiven , construction (property OLS estimates), sample residuals orthogonal explanatory variables, term zero. Introducing \\(\\tilde{x}_{t}=(\\hat{\\Pi}-\\Pi)'x_{t}\\), \n\\[\\begin{eqnarray*}\n\\sum_{t=1}^{T}\\left[\\left(y_{t}-\\Pi'x_{t}\\right)'\\Omega^{-1}\\left(y_{t}-\\Pi'x_{t}\\right)\\right] =\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}'\\Omega^{-1}\\hat{\\varepsilon}_{t}+\\sum_{t=1}^{T}\\tilde{x}'_{t}\\Omega^{-1}\\tilde{x}_{t}.\n\\end{eqnarray*}\\]\nSince \\(\\Omega\\) positive definite matrix, \\(\\Omega^{-1}\\) well. Consequently, smallest value last term can take obtained \\(\\tilde{x}_{t}=0\\), .e. \\(\\Pi=\\hat{\\Pi}.\\)MLE \\(\\Omega\\) matrix \\(\\hat{\\Omega}\\) maximizes \\(\\Omega\\overset{\\ell}{\\rightarrow}L(Y_{T};\\hat{\\Pi},\\Omega)\\). :\n\\[\\begin{eqnarray*}\n\\log\\mathcal{L}(Y_{T};\\hat{\\Pi},\\Omega) & = & -(Tn/2)\\log(2\\pi)+(T/2)\\log\\left|\\Omega^{-1}\\right| -\\frac{1}{2}\\sum_{t=1}^{T}\\left[\\hat{\\varepsilon}_{t}'\\Omega^{-1}\\hat{\\varepsilon}_{t}\\right].\n\\end{eqnarray*}\\]Matrix \\(\\hat{\\Omega}\\) symmetric positive definite. easily checked (unrestricted) matrix maximizes latter expression symmetric positive definite matrix. Indeed:\n\\[\n\\frac{\\partial \\log\\mathcal{L}(Y_{T};\\hat{\\Pi},\\Omega)}{\\partial\\Omega}=\\frac{T}{2}\\Omega'-\\frac{1}{2}\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}\\hat{\\varepsilon}'_{t}\\Rightarrow\\hat{\\Omega}'=\\frac{1}{T}\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}\\hat{\\varepsilon}'_{t},\n\\]\nleads result.Proof Proposition 2.13Proof. Let us drop \\(\\) subscript. Rearranging Eq. (2.24), :\n\\[\n\\sqrt{T}(\\mathbf{b}-\\boldsymbol{\\beta}) =  (X'X/T)^{-1}\\sqrt{T}(X'\\boldsymbol\\varepsilon/T).\n\\]\nLet us consider autocovariances \\(\\mathbf{v}_t = x_t \\varepsilon_t\\), denoted \\(\\gamma^v_j\\). Using fact \\(x_t\\) linear combination past \\(\\varepsilon_t\\)s \\(\\varepsilon_t\\) white noise, get \\(\\mathbb{E}(\\varepsilon_t x_t)=0\\). Therefore\n\\[\n\\gamma^v_j = \\mathbb{E}(\\varepsilon_t\\varepsilon_{t-j}x_tx_{t-j}').\n\\]\n\\(j>0\\), \\(\\mathbb{E}(\\varepsilon_t\\varepsilon_{t-j}x_tx_{t-j}')=\\mathbb{E}(\\mathbb{E}[\\varepsilon_t\\varepsilon_{t-j}x_tx_{t-j}'|\\varepsilon_{t-j},x_t,x_{t-j}])=\\) \\(\\mathbb{E}(\\varepsilon_{t-j}x_tx_{t-j}'\\mathbb{E}[\\varepsilon_t|\\varepsilon_{t-j},x_t,x_{t-j}])=0\\). Note \\(\\mathbb{E}[\\varepsilon_t|\\varepsilon_{t-j},x_t,x_{t-j}]=0\\) \\(\\{\\varepsilon_t\\}\\) ..d. white noise sequence. \\(j=0\\), :\n\\[\n\\gamma^v_0 = \\mathbb{E}(\\varepsilon_t^2x_tx_{t}')= \\mathbb{E}(\\varepsilon_t^2) \\mathbb{E}(x_tx_{t}')=\\sigma^2\\mathbf{Q}.\n\\]\nconvergence distribution \\(\\sqrt{T}(X'\\boldsymbol\\varepsilon/T)=\\sqrt{T}\\frac{1}{T}\\sum_{t=1}^Tv_t\\) results Central Limit Theorem covariance-stationary processes, using \\(\\gamma_j^v\\) computed .","code":""},{"path":"append.html","id":"additional-codes","chapter":"3 Appendix","heading":"3.6 Additional codes","text":"","code":""},{"path":"append.html","id":"App:GEV","chapter":"3 Appendix","heading":"3.6.1 Simulating GEV distributions","text":"following lines code used generate Figure ??.","code":"\nn.sim <- 4000\npar(mfrow=c(1,3),\n    plt=c(.2,.95,.2,.85))\nall.rhos <- c(.3,.6,.95)\nfor(j in 1:length(all.rhos)){\n  theta <- 1/all.rhos[j]\n  v1 <- runif(n.sim)\n  v2 <- runif(n.sim)\n  w <- rep(.000001,n.sim)\n  # solve for f(w) = w*(1 - log(w)/theta) - v2 = 0\n  for(i in 1:20){\n    f.i <- w * (1 - log(w)/theta) - v2\n    f.prime <- 1 - log(w)/theta - 1/theta\n    w <- w - f.i/f.prime\n  }\n  u1 <- exp(v1^(1/theta) * log(w))\n  u2 <- exp((1-v1)^(1/theta) * log(w))\n\n  # Get eps1 and eps2 using the inverse of\n  # the Gumbel distribution's cdf:\n  eps1 <- -log(-log(u1))\n  eps2 <- -log(-log(u2))\n  cbind(cor(eps1,eps2),1-all.rhos[j]^2)\n  plot(eps1,eps2,pch=19,col=\"#FF000044\",\n       main=paste(\"rho = \",toString(all.rhos[j]),sep=\"\"),\n       xlab=expression(epsilon[1]),\n       ylab=expression(epsilon[2]),\n       cex.lab=2,cex.main=1.5)\n}"},{"path":"append.html","id":"IRFDELTA","chapter":"3 Appendix","heading":"3.6.2 Computing the covariance matrix of IRF using the delta method","text":"","code":"\nirf.function <- function(THETA){\n  c <- THETA[1]\n  phi <- THETA[2:(p+1)]\n  if(q>0){\n    theta <- c(1,THETA[(1+p+1):(1+p+q)])\n  }else{\n    theta <- 1\n  }\n  sigma <- THETA[1+p+q+1]\n  r <- dim(Matrix.of.Exog)[2] - 1\n  beta <- THETA[(1+p+q+1+1):(1+p+q+1+(r+1))]\n  \n  irf <- sim.arma(0,phi,beta,sigma=sd(Ramey$ED3_TC,na.rm=TRUE),T=60,\n                  y.0=rep(0,length(x$phi)),nb.sim=1,make.IRF=1,\n                  X=NaN,beta=NaN)\n  return(irf)\n}\n\nIRF.0 <- 100*irf.function(x$THETA)\neps <- .00000001\nd.IRF <- NULL\nfor(i in 1:length(x$THETA)){\n  THETA.i <- x$THETA\n  THETA.i[i] <- THETA.i[i] + eps\n  IRF.i <- 100*irf.function(THETA.i)\n  d.IRF <- cbind(d.IRF,\n                 (IRF.i - IRF.0)/eps\n                 )\n}\nmat.var.cov.IRF <- d.IRF %*% x$I %*% t(d.IRF)"},{"path":"append.html","id":"statistical-tables","chapter":"3 Appendix","heading":"3.7 Statistical Tables","text":"Table 3.1: Quantiles \\(\\mathcal{N}(0,1)\\) distribution. \\(\\) \\(b\\) respectively row column number; corresponding cell gives \\(\\mathbb{P}(0<X\\le +b)\\), \\(X \\sim \\mathcal{N}(0,1)\\).Table 3.2: Quantiles Student-\\(t\\) distribution. rows correspond different degrees freedom (\\(\\nu\\), say); columns correspond different probabilities (\\(z\\), say). cell gives \\(q\\) s.t. \\(\\mathbb{P}(-q<X<q)=z\\), \\(X \\sim t(\\nu)\\).Table 3.3: Quantiles \\(\\chi^2\\) distribution. rows correspond different degrees freedom; columns correspond different probabilities.Table 3.4: Quantiles \\(\\mathcal{F}\\) distribution. columns rows correspond different degrees freedom (resp. \\(n_1\\) \\(n_2\\)). different panels correspond different probabilities (\\(\\alpha\\)) corresponding cell gives \\(z\\) s.t. \\(\\mathbb{P}(X \\le z)=\\alpha\\), \\(X \\sim \\mathcal{F}(n_1,n_2)\\).","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
