<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 9 Appendix | The Identification of Dynamic Structural Shocks</title>
<meta name="author" content="Kenza Benhima and Jean-Paul Renne">
<meta name="description" content="9.1 Definitions and statistical results  Definition 9.1 (Covariance stationarity) The process \(y_t\) is covariance stationary —or weakly stationary— if, for all \(t\) and \(j\), \[...">
<meta name="generator" content="bookdown 0.27 with bs4_book()">
<meta property="og:title" content="Chapter 9 Appendix | The Identification of Dynamic Structural Shocks">
<meta property="og:type" content="book">
<meta property="og:description" content="9.1 Definitions and statistical results  Definition 9.1 (Covariance stationarity) The process \(y_t\) is covariance stationary —or weakly stationary— if, for all \(t\) and \(j\), \[...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 9 Appendix | The Identification of Dynamic Structural Shocks">
<meta name="twitter:description" content="9.1 Definitions and statistical results  Definition 9.1 (Covariance stationarity) The process \(y_t\) is covariance stationary —or weakly stationary— if, for all \(t\) and \(j\), \[...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.4.0/transition.js"></script><script src="libs/bs3compat-0.4.0/tabs.js"></script><script src="libs/bs3compat-0.4.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
<link rel="stylesheet" href="my-style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">The Identification of Dynamic Structural Shocks</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">The Identification of Dynamic Structural Shocks</a></li>
<li><a class="" href="basics.html"><span class="header-section-number">1</span> VARs and IRFs: the basics</a></li>
<li><a class="" href="identifStruct.html"><span class="header-section-number">2</span> Identification problem and standard identification techniques</a></li>
<li><a class="" href="Signs.html"><span class="header-section-number">3</span> Sign restrictions</a></li>
<li><a class="" href="forecast-error-variance-maximization.html"><span class="header-section-number">4</span> Forecast error variance maximization</a></li>
<li><a class="" href="NonGaussian.html"><span class="header-section-number">5</span> Identification based on non-normality of the shocks</a></li>
<li><a class="" href="Projections.html"><span class="header-section-number">6</span> Local projection methods</a></li>
<li><a class="" href="Inference.html"><span class="header-section-number">7</span> Inference</a></li>
<li><a class="" href="FAVAR.html"><span class="header-section-number">8</span> Factor-Augmented VAR</a></li>
<li><a class="active" href="append.html"><span class="header-section-number">9</span> Appendix</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="append" class="section level1" number="9">
<h1>
<span class="header-section-number">9</span> Appendix<a class="anchor" aria-label="anchor" href="#append"><i class="fas fa-link"></i></a>
</h1>
<div id="definitions-and-statistical-results" class="section level2" number="9.1">
<h2>
<span class="header-section-number">9.1</span> Definitions and statistical results<a class="anchor" aria-label="anchor" href="#definitions-and-statistical-results"><i class="fas fa-link"></i></a>
</h2>
<div class="definition">
<p><span id="def:covstat" class="definition"><strong>Definition 9.1  (Covariance stationarity) </strong></span>The process <span class="math inline">\(y_t\)</span> is covariance stationary —or weakly stationary— if, for all <span class="math inline">\(t\)</span> and <span class="math inline">\(j\)</span>,
<span class="math display">\[
\mathbb{E}(y_t) = \mu \quad \mbox{and} \quad \mathbb{E}\{(y_t - \mu)(y_{t-j} - \mu)\} = \gamma_j.
\]</span></p>
</div>
<div class="definition">
<p><span id="def:LR" class="definition"><strong>Definition 9.2  (Likelihood Ratio test statistics) </strong></span>The likelihood ratio associated to a restriction of the form <span class="math inline">\(H_0: h({\boldsymbol\theta})=0\)</span> (where <span class="math inline">\(h({\boldsymbol\theta})\)</span> is a <span class="math inline">\(r\)</span>-dimensional vector) is given by:
<span class="math display">\[
LR = \frac{\mathcal{L}_R(\boldsymbol\theta;\mathbf{y})}{\mathcal{L}_U(\boldsymbol\theta;\mathbf{y})} \quad (\in [0,1]),
\]</span>
where <span class="math inline">\(\mathcal{L}_R\)</span> (respectively <span class="math inline">\(\mathcal{L}_U\)</span>) is the likelihood function that imposes (resp. that does not impose) the restriction. The likelihood ratio test statistic is given by <span class="math inline">\(-2\log(LR)\)</span>, that is:
<span class="math display">\[
\boxed{\xi^{LR}= 2 (\log\mathcal{L}_U(\boldsymbol\theta;\mathbf{y})-\log\mathcal{L}_R(\boldsymbol\theta;\mathbf{y})).}
\]</span>
Under regularity assumptions and under the null hypothesis, the test statistic follows a chi-square distribution with <span class="math inline">\(r\)</span> degrees of freedom (see Table <a href="append.html#tab:Chi2">9.3</a>).</p>
</div>
<div class="proposition">
<p><span id="prp:pdfMultivarGaussian" class="proposition"><strong>Proposition 9.1  (p.d.f. of a multivariate Gaussian variable) </strong></span>If <span class="math inline">\(Y \sim \mathcal{N}(\mu,\Omega)\)</span> and if <span class="math inline">\(Y\)</span> is a <span class="math inline">\(n\)</span>-dimensional vector, then the density function of <span class="math inline">\(Y\)</span> is:
<span class="math display">\[
\frac{1}{(2 \pi)^{n/2}|\Omega|^{1/2}}\exp\left[-\frac{1}{2}\left(Y-\mu\right)'\Omega^{-1}\left(Y-\mu\right)\right].
\]</span></p>
</div>
<!-- ## Linear algebra: definitions and results {#LinAlgebra} -->
<!-- :::{.definition #determinant name="Eigenvalues"} -->
<!-- The eigenvalues of of a matrix $M$ are the numbers $\lambda$ for which: -->
<!-- $$ -->
<!-- |M - \lambda I| = 0, -->
<!-- $$ -->
<!-- where $| \bullet |$ is the determinant operator. -->
<!-- ::: -->
<!-- :::{.proposition #determinant name="Properties of the determinant"} -->
<!-- We have: -->
<!-- * $|MN|=|M|\times|N|$. -->
<!-- * $|M^{-1}|=|M|^{-1}$. -->
<!-- * If $M$ admits the diagonal representation $M=TDT^{-1}$, where $D$ is a diagonal matrix whose diagonal entries are $\{\lambda_i\}_{i=1,\dots,n}$, then: -->
<!-- $$ -->
<!-- |M - \lambda I |=\prod_{i=1}^n (\lambda_i - \lambda). -->
<!-- $$ -->
<!-- ::: -->
<!-- :::{.definition #MoorPenrose name="Moore-Penrose inverse"} -->
<!-- If $M \in \mathbb{R}^{m \times n}$, then its Moore-Penrose pseudo inverse (exists and) is the unique matrix $M^*  \in \mathbb{R}^{n \times m}$ that satisfies: -->
<!-- i. $M M^* M = M$ -->
<!-- ii. $M^* M M^* = M^*$ -->
<!-- iii. $(M M^*)'=M M^*$ -->
<!-- .iv $(M^* M)'=M^* M$. -->
<!-- ::: -->
<!-- :::{.proposition #MoorPenrose name="Properties of the Moore-Penrose inverse"} -->
<!-- * If $M$ is invertible then $M^* = M^{-1}$. -->
<!-- * The pseudo-inverse of a zero matrix is its transpose. -->
<!-- *\item* The pseudo-inverse of the pseudo-inverse is the original matrix. -->
<!-- ::: -->
<!-- :::{.definition #idempotent name="Idempotent matrix"} -->
<!-- Matrix $M$ is idempotent if $M^2=M$. -->
<!-- If $M$ is a symmetric idempotent matrix, then $M'M=M$. -->
<!-- ::: -->
<!-- :::{.proposition #rootsidempotent name="Roots of an idempotent matrix"} -->
<!-- The eigenvalues of an idempotent matrix are either 1 or 0. -->
<!-- ::: -->
<!-- :::{.proof} -->
<!-- If $\lambda$ is an eigenvalue of an idempotent matrix $M$ then $\exists x \ne 0$ s.t. $Mx=\lambda x$. Hence $M^2x=\lambda M x \Rightarrow (1-\lambda)Mx=0$. Either all element of  $Mx$ are zero, in which case $\lambda=0$ or at least one element of $Mx$ is nonzero, in which case $\lambda=1$. -->
<!-- ::: -->
<!-- :::{.proposition #chi2idempotent name="Idempotent matrix and chi-square distribution"} -->
<!-- The rank of a symmetric idempotent matrix is equal to its trace. -->
<!-- ::: -->
<!-- :::{.proof} -->
<!-- The result follows from Prop. \@ref(prp:rootsidempotent), combined with the fact that the rank of a symmetric matrix is equal to the number of its nonzero eigenvalues. -->
<!-- ::: -->
<!-- :::{.proposition #constrainedLS name="Constrained least squares"} -->
<!-- The solution of the following optimisation problem: -->
<!-- \begin{eqnarray*} -->
<!-- \underset{\boldsymbol\beta}{\min} && || \bv{y} - \bv{X}\boldsymbol\beta ||^2 \\ -->
<!-- && \mbox{subject to } \bv{R}\boldsymbol\beta = \bv{q} -->
<!-- \end{eqnarray*} -->
<!-- is given by: -->
<!-- $$ -->
<!-- \boxed{\boldsymbol\beta^r = \boldsymbol\beta_0 - (\bv{X}'\bv{X})^{-1} \bv{R}'\{\bv{R}(\bv{X}'\bv{X})^{-1}\bv{R}'\}^{-1}(\bv{R}\boldsymbol\beta_0 - \bv{q}),} -->
<!-- $$ -->
<!-- where $\boldsymbol\beta_0=(\bv{X}'\bv{X})^{-1}\bv{X}'\bv{y}$. -->
<!-- ::: -->
<!-- :::{.proof} -->
<!-- See for instance [Jackman, 2007](http://jackman.stanford.edu/classes/350B/07/ftestforWeb.pdf). -->
<!-- ::: -->
<!-- :::{.proposition #inversepartitioned name="Inverse of a partitioned matrix"} -->
<!-- We have: -->
<!-- \begin{eqnarray*} -->
<!-- &&\left[ \begin{array}{cc} \bv{A}_{11} & \bv{A}_{12} \\ \bv{A}_{21} & \bv{A}_{22} \end{array}\right]^{-1} = \\ -->
<!-- &&\left[ \begin{array}{cc} (\bv{A}_{11} - \bv{A}_{12}\bv{A}_{22}^{-1}\bv{A}_{21})^{-1} & - \bv{A}_{11}^{-1}\bv{A}_{12}(\bv{A}_{22} - \bv{A}_{21}\bv{A}_{11}^{-1}\bv{A}_{12})^{-1} \\ -->
<!-- -(\bv{A}_{22} - \bv{A}_{21}\bv{A}_{11}^{-1}\bv{A}_{12})^{-1}\bv{A}_{21}\bv{A}_{11}^{-1} & (\bv{A}_{22} - \bv{A}_{21}\bv{A}_{11}^{-1}\bv{A}_{12})^{-1} \end{array} \right]. -->
<!-- \end{eqnarray*} -->
<!-- ::: -->
<!-- :::{.definition #FOD name="Matrix derivatives"} -->
<!-- Consider a fonction $f: \mathbb{R}^K \rightarrow \mathbb{R}$. Its first-order derivative is: -->
<!-- $$ -->
<!-- \frac{\partial f}{\partial \bv{b}}(\bv{b}) = -->
<!-- \left[\begin{array}{c} -->
<!-- \frac{\partial f}{\partial b_1}(\bv{b})\\ -->
<!-- \vdots\\ -->
<!-- \frac{\partial f}{\partial b_K}(\bv{b}) -->
<!-- \end{array} -->
<!-- \right]. -->
<!-- $$ -->
<!-- We use the notation: -->
<!-- $$ -->
<!-- \frac{\partial f}{\partial \bv{b}'}(\bv{b}) = \left(\frac{\partial f}{\partial \bv{b}}(\bv{b})\right)'. -->
<!-- $$ -->
<!-- ::: -->
<!-- :::{.proposition #partial} -->
<!-- We have: -->
<!-- * If $f(\bv{b}) = A' \bv{b}$ where $A$ is a $K \times 1$ vector then $\frac{\partial f}{\partial \bv{b}}(\bv{b}) = A$. -->
<!-- * If $f(\bv{b}) = \bv{b}'A\bv{b}$ where $A$ is a $K \times K$ matrix, then $\frac{\partial f}{\partial \bv{b}}(\bv{b}) = 2A\bv{b}$. -->
<!-- ::: -->
<!-- :::{.proposition #absMs name="Square and absolute summability"} -->
<!-- We have: -->
<!-- $$ -->
<!-- \underbrace{\sum_{i=0}^{\infty}|\theta_i| < + \infty}_{\mbox{Absolute summability}} \Rightarrow \underbrace{\sum_{i=0}^{\infty} \theta_i^2 < + \infty}_{\mbox{Square summability}}. -->
<!-- $$ -->
<!-- ::: -->
<!-- :::{.proof} -->
<!-- See Appendix 3.A in Hamilton. Idea: Absolute summability implies that there exist $N$ such that, for $j>N$, $|\theta_j| < 1$ (deduced from Cauchy criterion, Theorem \@ref(thm:cauchycritstatic) and therefore $\theta_j^2 < |\theta_j|$. -->
<!-- ::: -->
<!-- ## Statistical analysis: definitions and results {#variousResults} -->
<!-- ### Moments and statistics -->
<!-- :::{.definition #partialcorrel name="Partial correlation"} -->
<!-- The **partial correlation** between $y$ and $z$, controlling for some variables $\bv{X}$ is the sample correlation between $y^*$ and $z^*$, where the latter two variables are the residuals in regressions of $y$ on $\bv{X}$ and of $z$ on $\bv{X}$, respectively. -->
<!-- This correlation is denoted by $r_{yz}^\bv{X}$. By definition, we have: -->
<!-- \begin{equation} -->
<!-- r_{yz}^\bv{X} = \frac{\bv{z^*}'\bv{y^*}}{\sqrt{(\bv{z^*}'\bv{z^*})(\bv{y^*}'\bv{y^*})}}.(\#eq:pc) -->
<!-- \end{equation} -->
<!-- ::: -->
<!-- :::{.definition #skewnesskurtosis name="Skewness and kurtosis"} -->
<!-- Let $Y$ be a random variable whose fourth moment exists. The expectation of $Y$ is denoted by $\mu$. -->
<!-- * The skewness of $Y$ is given by: -->
<!-- $$ -->
<!-- \frac{\mathbb{E}[(Y-\mu)^3]}{\{\mathbb{E}[(Y-\mu)^2]\}^{3/2}}. -->
<!-- $$ -->
<!-- * The kurtosis of $Y$ is given by: -->
<!-- $$ -->
<!-- \frac{\mathbb{E}[(Y-\mu)^4]}{\{\mathbb{E}[(Y-\mu)^2]\}^{2}}. -->
<!-- $$ -->
<!-- ::: -->
<!-- :::{.theorem #CauchySchwarz name="Cauchy-Schwarz inequality"} -->
<!-- We have: -->
<!-- $$ -->
<!-- |\mathbb{C}ov(X,Y)| \le \sqrt{\mathbb{V}ar(X)\mathbb{V}ar(Y)} -->
<!-- $$ -->
<!-- and, if $X \ne =$ and $Y \ne 0$, the equality holds iff $X$ and $Y$ are the same up to an affine transformation. -->
<!-- ::: -->
<!-- :::{.proof} -->
<!-- If $\mathbb{V}ar(X)=0$, this is trivial. If this is not the case, then let's define $Z$ as $Z = Y - \frac{\mathbb{C}ov(X,Y)}{\mathbb{V}ar(X)}X$. It is easily seen that $\mathbb{C}ov(X,Z)=0$. Then, the variance of $Y=Z+\frac{\mathbb{C}ov(X,Y)}{\mathbb{V}ar(X)}X$ is equal to the sum of the variance of $Z$ and of the variance of $\frac{\mathbb{C}ov(X,Y)}{\mathbb{V}ar(X)}X$, that is: -->
<!-- $$ -->
<!-- \mathbb{V}ar(Y) = \mathbb{V}ar(Z) + \left(\frac{\mathbb{C}ov(X,Y)}{\mathbb{V}ar(X)}\right)^2\mathbb{V}ar(X) \ge \left(\frac{\mathbb{C}ov(X,Y)}{\mathbb{V}ar(X)}\right)^2\mathbb{V}ar(X). -->
<!-- $$ -->
<!-- The equality holds iff $\mathbb{V}ar(Z)=0$, i.e. iff $Y = \frac{\mathbb{C}ov(X,Y)}{\mathbb{V}ar(X)}X+cst$. -->
<!-- ::: -->
<!-- :::{.definition #asmyptlevel name="Asymptotic level"} -->
<!-- An asymptotic test with critical region $\Omega_n$ has an asymptotic level equal to $\alpha$ if: -->
<!-- $$ -->
<!-- \underset{\theta \in \Theta}{\mbox{sup}} \quad \underset{n \rightarrow \infty}{\mbox{lim}} \mathbb{P}_\theta (S_n \in \Omega_n) = \alpha, -->
<!-- $$ -->
<!-- where $S_n$ is the test statistic and $\Theta$ is such that the null hypothesis $H_0$ is equivalent to $\theta \in \Theta$. -->
<!-- ::: -->
<!-- :::{.definition #asmyptconsisttest name="Asymptotically consistent test"} -->
<!-- An asymptotic test with critical region $\Omega_n$ is consistent if: -->
<!-- $$ -->
<!-- \forall \theta \in \Theta^c, \quad \mathbb{P}_\theta (S_n \in \Omega_n) \rightarrow 1, -->
<!-- $$ -->
<!-- where $S_n$ is the test statistic and $\Theta^c$ is such that the null hypothesis $H_0$ is equivalent to $\theta \notin \Theta^c$. -->
<!-- ::: -->
<!-- :::{.definition #Kullback name="Kullback discrepancy"} -->
<!-- Given two p.d.f. $f$ and $f^*$, the Kullback discrepancy is defined by: -->
<!-- $$ -->
<!-- I(f,f^*) = \mathbb{E}^* \left( \log \frac{f^*(Y)}{f(Y)} \right) = \int \log \frac{f^*(y)}{f(y)} f^*(y) dy. -->
<!-- $$ -->
<!-- ::: -->
<!-- :::{.proposition #Kullback name="Properties of the Kullback discrepancy"} -->
<!-- We have: -->
<!-- i. $I(f,f^*) \ge 0$ -->
<!-- ii. $I(f,f^*) = 0$ iff $f \equiv f^*$. -->
<!-- ::: -->
<!-- :::{.proof} -->
<!-- $x \rightarrow -\log(x)$ is a convex function. Therefore $\mathbb{E}^*(-\log f(Y)/f^*(Y)) \ge -\log \mathbb{E}^*(f(Y)/f^*(Y)) = 0$ (proves (i)). Since $x \rightarrow -\log(x)$ is strictly convex, equality in (i) holds if and only if $f(Y)/f^*(Y)$ is constant (proves (ii)). -->
<!-- ::: -->
<!-- :::{.definition #characteristic name="Characteristic function"} -->
<!-- For any real-valued random variable $X$, the characteristic function is defined by: -->
<!-- $$ -->
<!-- \phi_X: u \rightarrow \mathbb{E}[\exp(iuX)]. -->
<!-- $$ -->
<!-- ::: -->
<!-- ### Standard distributions -->
<!-- :::{.definition #fstatistics name="F distribution"} -->
<!-- Consider $n=n_1+n_2$ i.i.d. $\mathcal{N}(0,1)$ r.v. $X_i$. If the r.v. $F$ is defined by: -->
<!-- $$ -->
<!-- F = \frac{\sum_{i=1}^{n_1} X_i^2}{\sum_{j=n_1+1}^{n_1+n_2} X_j^2}\frac{n_2}{n_1} -->
<!-- $$ -->
<!-- then $F \sim \mathcal{F}(n_1,n_2)$. (See Table \@ref(tab:Fstat) for quantiles.) -->
<!-- ::: -->
<!-- :::{.definition #tStudent name="Student-t distribution"} -->
<!-- $Z$ follows a Student-t (or $t$) distribution with $\nu$ degrees of freedom (d.f.) if: -->
<!-- $$ -->
<!-- Z = X_0 \bigg/ \sqrt{\frac{\sum_{i=1}^{\nu}X_i^2}{\nu}}, \quad X_i \sim i.i.d. \mathcal{N}(0,1). -->
<!-- $$ -->
<!-- We have $\mathbb{E}(Z)=0$, and $\mathbb{V}ar(Z)=\frac{\nu}{\nu-2}$ if $\nu>2$. (See Table \@ref(tab:Student) for quantiles.) -->
<!-- ::: -->
<!-- :::{.definition #chi2 name="Chi-square distribution"} -->
<!-- $Z$ follows a $\chi^2$ distribution with $\nu$ d.f. if $Z = \sum_{i=1}^{\nu}X_i^2$ where $X_i \sim i.i.d. \mathcal{N}(0,1)$. -->
<!-- We have $\mathbb{E}(Z)=\nu$. (See Table \@ref(tab:Chi2) for quantiles.) -->
<!-- ::: -->
<!-- :::{.proposition #waldtypeproduct name="Inner product of a multivariate Gaussian variable"} -->
<!-- Let $X$ be a $n$-dimensional multivariate Gaussian variable: $X \sim \mathcal{N}(0,\Sigma)$. We have: -->
<!-- $$ -->
<!-- X' \Sigma^{-1}X \sim \chi^2(n). -->
<!-- $$ -->
<!-- ::: -->
<!-- :::{.proof} -->
<!-- Because $\Sigma$ is a symmetrical definite positive matrix, it admits the spectral decomposition $PDP'$ where $P$ is an orthogonal matrix (i.e. $PP'=Id$) and D is a diagonal matrix with non-negative entries. Denoting by $\sqrt{D^{-1}}$ the diagonal matrix whose diagonal entries are the inverse of those of $D$, it is easily checked that the covariance matrix of $Y:=\sqrt{D^{-1}}P'X$ is $Id$. Therefore $Y$ is a vector of uncorrelated Gaussian variables. The properties of Gaussian variables imply that the components of $Y$ are then also independent. Hence $Y'Y=\sum_i Y_i^2 \sim \chi^2(n)$. -->
<!-- It remains to note that $Y'Y=X'PD^{-1}P'X=X'\mathbb{V}ar(X)^{-1}X$ to conclude. -->
<!-- ::: -->
<!-- :::{.definition #GEVdistri name="Generalized Extreme Value (GEV) distribution"} -->
<!-- The vector of disturbances $\boldsymbol\varepsilon=[\varepsilon_{1,1},\dots,\varepsilon_{1,K_1},\dots,\varepsilon_{J,1},\dots,\varepsilon_{J,K_J}]'$ follows the Generalized Extreme Value (GEV) distribution if its c.d.f. is: -->
<!-- $$ -->
<!-- F(\boldsymbol\varepsilon,\boldsymbol\rho) = \exp(-G(e^{-\varepsilon_{1,1}},\dots,e^{-\varepsilon_{J,K_J}};\boldsymbol\rho)) -->
<!-- $$ -->
<!-- with -->
<!-- \begin{eqnarray*} -->
<!-- G(\bv{Y};\boldsymbol\rho) &\equiv&  G(Y_{1,1},\dots,Y_{1,K_1},\dots,Y_{J,1},\dots,Y_{J,K_J};\boldsymbol\rho) \\ -->
<!-- &=& \sum_{j=1}^J\left(\sum_{k=1}^{K_j} Y_{jk}^{1/\rho_j} -->
<!-- \right)^{\rho_j} -->
<!-- \end{eqnarray*} -->
<!-- ::: -->
<!-- ### Stochastic convergences -->
<!-- :::{.proposition #chebychev name="Chebychev's inequality"} -->
<!-- If $\mathbb{E}(|X|^r)$ is finite for some $r>0$ then: -->
<!-- $$ -->
<!-- \forall \varepsilon > 0, \quad \mathbb{P}(|X - c|>\varepsilon) \le \frac{\mathbb{E}[|X - c|^r]}{\varepsilon^r}. -->
<!-- $$ -->
<!-- In particular, for $r=2$: -->
<!-- $$ -->
<!-- \forall \varepsilon > 0, \quad \mathbb{P}(|X - c|>\varepsilon) \le \frac{\mathbb{E}[(X - c)^2]}{\varepsilon^2}. -->
<!-- $$ -->
<!-- ::: -->
<!-- :::{.proof} -->
<!-- Remark that $\varepsilon^r \mathbb{I}_{\{|X| \ge \varepsilon\}} \le |X|^r$ and take the expectation of both sides. -->
<!-- ::: -->
<!-- :::{.definition #convergenceproba name="Convergence in probability"} -->
<!-- The random variable sequence $x_n$ converges in probability to a constant $c$ if $\forall \varepsilon$, $\lim_{n \rightarrow \infty} \mathbb{P}(|x_n - c|>\varepsilon) = 0$. -->
<!-- It is denoted as: $\mbox{plim } x_n = c$. -->
<!-- ::: -->
<!-- :::{.definition #convergenceLr name="Convergence in the Lr norm"} -->
<!-- $x_n$ converges in the $r$-th mean (or in the $L^r$-norm) towards $x$, if $\mathbb{E}(|x_n|^r)$ and $\mathbb{E}(|x|^r)$ exist and if -->
<!-- $$ -->
<!-- \lim_{n \rightarrow \infty} \mathbb{E}(|x_n - x|^r) = 0. -->
<!-- $$ -->
<!-- It is denoted as: $x_n \overset{L^r}{\rightarrow} c$. -->
<!-- For $r=2$, this convergence is called **mean square convergence**. -->
<!-- ::: -->
<!-- :::{.definition #convergenceAlmost name="Almost sure convergence"} -->
<!-- The random variable sequence $x_n$ converges almost surely to $c$ if $\mathbb{P}(\lim_{n \rightarrow \infty} x_n = c) = 1$. -->
<!-- It is denoted as: $x_n \overset{a.s.}{\rightarrow} c$. -->
<!-- ::: -->
<!-- :::{.definition #cvgceDistri name="Convergence in distribution"} -->
<!-- $x_n$ is said to converge in distribution (or in law) to $x$ if -->
<!-- $$ -->
<!-- \lim_{n \rightarrow \infty} F_{x_n}(s) = F_{x}(s) -->
<!-- $$ -->
<!-- for all $s$ at which $F_X$ --the cumulative distribution of $X$-- is continuous. -->
<!-- It is denoted as: $x_n \overset{d}{\rightarrow} x$. -->
<!-- ::: -->
<!-- :::{.proposition #Slutsky name="Rules for limiting distributions (Slutsky)"} -->
<!-- We have: -->
<!-- i. **Slutsky's theorem:** If $x_n \overset{d}{\rightarrow} x$ and $y_n \overset{p}{\rightarrow} c$ then -->
<!-- \begin{eqnarray*} -->
<!-- x_n y_n &\overset{d}{\rightarrow}& x c \\ -->
<!-- x_n + y_n &\overset{d}{\rightarrow}& x + c \\ -->
<!-- x_n/y_n &\overset{d}{\rightarrow}& x / c \quad (\mbox{if }c \ne 0) -->
<!-- \end{eqnarray*} -->
<!-- ii. **Continuous mapping theorem:** If $x_n \overset{d}{\rightarrow} x$ and $g$ is a continuous function then $g(x_n) \overset{d}{\rightarrow} g(x).$  -->
<!-- ::: -->
<!-- :::{.proposition #implicationsconv name="Implications of stochastic convergences"} -->
<!-- We have: -->
<!-- \begin{align*} -->
<!-- &\boxed{\overset{L^s}{\rightarrow}}& &\underset{1 \le r \le s}{\Rightarrow}& &\boxed{\overset{L^r}{\rightarrow}}&\\ -->
<!-- && && &\Downarrow&\\ -->
<!-- &\boxed{\overset{a.s.}{\rightarrow}}& &\Rightarrow& &\boxed{\overset{p}{\rightarrow}}& \Rightarrow \qquad \boxed{\overset{d}{\rightarrow}}. -->
<!-- \end{align*} -->
<!-- ::: -->
<!-- :::{.proof} -->
<!-- (of the fact that $\left(\overset{p}{\rightarrow}\right) \Rightarrow \left( \overset{d}{\rightarrow}\right)$). Assume that $X_n \overset{p}{\rightarrow} X$. Denoting by $F$ and $F_n$ the c.d.f. of $X$ and $X_n$, respectively: -->
<!-- \begin{equation} -->
<!-- F_n(x) = \mathbb{P}(X_n \le x,X\le x+\varepsilon) + \mathbb{P}(X_n \le x,X > x+\varepsilon) \le F(x+\varepsilon) + \mathbb{P}(|X_n - X|>\varepsilon).(\#eq:convgce1) -->
<!-- \end{equation} -->
<!-- Besides, -->
<!-- $$ -->
<!-- F(x-\varepsilon) = \mathbb{P}(X \le x-\varepsilon,X_n \le x) + \mathbb{P}(X \le x-\varepsilon,X_n > x) \le F_n(x) + \mathbb{P}(|X_n - X|>\varepsilon), -->
<!-- $$ -->
<!-- which implies: -->
<!-- \begin{equation} -->
<!-- F(x-\varepsilon) - \mathbb{P}(|X_n - X|>\varepsilon) \le F_n(x).(\#eq:convgce2) -->
<!-- \end{equation} -->
<!-- Eqs. \@ref(eq:convgce1) and \@ref(eq:convgce2) imply: -->
<!-- $$ -->
<!-- F(x-\varepsilon) - \mathbb{P}(|X_n - X|>\varepsilon) \le F_n(x)  \le F(x+\varepsilon) + \mathbb{P}(|X_n - X|>\varepsilon). -->
<!-- $$ -->
<!-- Taking limits as $n \rightarrow \infty$ yields -->
<!-- $$ -->
<!-- F(x-\varepsilon) \le \underset{n \rightarrow \infty}{\mbox{lim inf}}\; F_n(x) \le \underset{n \rightarrow \infty}{\mbox{lim sup}}\; F_n(x)  \le F(x+\varepsilon). -->
<!-- $$ -->
<!-- The result is then obtained by taking limits as $\varepsilon \rightarrow 0$ (if $F$ is continuous at $x$). -->
<!-- ::: -->
<!-- :::{.proposition #cvgce11 name="Convergence in distribution to a constant"} -->
<!-- If $X_n$ converges in distribution to a constant $c$, then $X_n$ converges in probability to $c$. -->
<!-- ::: -->
<!-- :::{.proof} -->
<!-- If $\varepsilon>0$, we have $\mathbb{P}(X_n < c - \varepsilon) \underset{n \rightarrow \infty}{\rightarrow} 0$ i.e. $\mathbb{P}(X_n \ge c - \varepsilon) \underset{n \rightarrow \infty}{\rightarrow} 1$ and $\mathbb{P}(X_n < c + \varepsilon) \underset{n \rightarrow \infty}{\rightarrow} 1$. Therefore $\mathbb{P}(c - \varepsilon \le X_n < c + \varepsilon) \underset{n \rightarrow \infty}{\rightarrow} 1$, -->
<!-- which gives the result. -->
<!-- ::: -->
<!-- :::{.example #plimButNotLr name="Convergence in probability but not $L^r$"} -->
<!-- Let $\{x_n\}_{n \in \mathbb{N}}$ be a series of random variables defined by: -->
<!-- $$ -->
<!-- x_n = n u_n, -->
<!-- $$ -->
<!-- where $u_n$ are independent random variables s.t. $u_n \sim \mathcal{B}(1/n)$. -->
<!-- We have $x_n \overset{p}{\rightarrow} 0$ but $x_n \overset{L^r}{\nrightarrow} 0$ because $\mathbb{E}(|X_n-0|)=\mathbb{E}(X_n)=1$. -->
<!-- ::: -->
<!-- :::{.theorem #cauchycritstatic name="Cauchy criterion (non-stochastic case)"} -->
<!-- We have that $\sum_{i=0}^{T} a_i$ converges ($T \rightarrow \infty$) iff, for any $\eta > 0$, there exists an integer $N$ such that, for all $M\ge N$, -->
<!-- $$ -->
<!-- \left|\sum_{i=N+1}^{M} a_i\right| < \eta. -->
<!-- $$ -->
<!-- ::: -->
<!-- :::{.theorem #cauchycritstochastic name="Cauchy criterion (stochastic case)"} -->
<!-- We have that $\sum_{i=0}^{T} \theta_i \varepsilon_{t-i}$ converges in mean square ($T \rightarrow \infty$) to a random variable iff, for any $\eta > 0$, there exists an integer $N$ such that, for all $M\ge N$, -->
<!-- $$ -->
<!-- \mathbb{E}\left[\left(\sum_{i=N+1}^{M} \theta_i \varepsilon_{t-i}\right)^2\right] < \eta. -->
<!-- $$ -->
<!-- ::: -->
<!-- ### Central limit theorem -->
<!-- :::{.theorem #LLNappendix name="Law of large numbers"} -->
<!-- The sample mean is a consistent estimator of the population mean. -->
<!-- ::: -->
<!-- :::{.proof} -->
<!-- Let's denote by $\phi_{X_i}$ the characteristic function of a r.v. $X_i$. If the mean of $X_i$ is $\mu$ then the Talyor expansion of the characteristic function is: -->
<!-- $$ -->
<!-- \phi_{X_i}(u) = \mathbb{E}(\exp(iuX)) = 1 + iu\mu + o(u). -->
<!-- $$ -->
<!-- The properties of the characteristic function (see Def. \@ref(def:characteristic)) imply that: -->
<!-- $$ -->
<!-- \phi_{\frac{1}{n}(X_1+\dots+X_n)}(u) = \prod_{i=1}^{n} \left(1 + i\frac{u}{n}\mu + o\left(\frac{u}{n}\right) \right) \rightarrow e^{iu\mu}. -->
<!-- $$ -->
<!-- The facts that (a) $e^{iu\mu}$ is the characteristic function of the constant $\mu$ and (b) that a characteristic function uniquely characterises a distribution imply that the sample mean converges in distribution to the constant $\mu$, which further implies that it converges in probability to $\mu$. -->
<!-- ::: -->
<!-- :::{.theorem #LindbergLevyCLT name="Lindberg-Levy Central limit theorem, CLT"} -->
<!-- If $x_n$ is an i.i.d. sequence of random variables with mean $\mu$ and variance $\sigma^2$ ($\in ]0,+\infty[$), then: -->
<!-- $$ -->
<!-- \boxed{\sqrt{n} (\bar{x}_n - \mu) \overset{d}{\rightarrow} \mathcal{N}(0,\sigma^2), \quad \mbox{where} \quad \bar{x}_n = \frac{1}{n} \sum_{i=1}^{n} x_i.} -->
<!-- $$ -->
<!-- ::: -->
<!-- :::{.proof} -->
<!-- Let us introduce the r.v. $Y_n:= \sqrt{n}(\bar{X}_n - \mu)$. We have $\phi_{Y_n}(u) = \left[ \mathbb{E}\left( \exp(i \frac{1}{\sqrt{n}} u (X_1 - \mu)) \right) \right]^n$. We have: -->
<!-- \begin{eqnarray*} -->
<!-- \left[ \mathbb{E}\left( \exp\left(i \frac{1}{\sqrt{n}} u (X_1 - \mu)\right) \right) \right]^n &=& \left[ \mathbb{E}\left( 1 + i \frac{1}{\sqrt{n}} u (X_1 - \mu) - \frac{1}{2n} u^2 (X_1 - \mu)^2 + o(u^2) \right) \right]^n \\ -->
<!-- &=& \left( 1 - \frac{1}{2n}u^2\sigma^2 + o(u^2)\right)^n. -->
<!-- \end{eqnarray*} -->
<!-- Therefore $\phi_{Y_n}(u) \underset{n \rightarrow \infty}{\rightarrow} \exp \left( - \frac{1}{2}u^2\sigma^2 \right)$, which is the characteristic function of $\mathcal{N}(0,\sigma^2)$. -->
<!-- ::: -->
<!-- ## Some properties of Gaussian variables {#GaussianVar} -->
<!-- :::{.proposition #bandsindependent} -->
<!-- If $\bv{A}$ is idempotent and if $\bv{x}$ is Gaussian, $\bv{L}\bv{x}$ and $\bv{x}'\bv{A}\bv{x}$ are independent if $\bv{L}\bv{A}=\bv{0}$. -->
<!-- ::: -->
<!-- :::{.proof} -->
<!-- If $\bv{L}\bv{A}=\bv{0}$, then the two Gaussian vectors $\bv{L}\bv{x}$ and  $\bv{A}\bv{x}$ are independent. This implies the independence of any function of $\bv{L}\bv{x}$ and any function of $\bv{A}\bv{x}$. The results then follows from the observation that $\bv{x}'\bv{A}\bv{x}=(\bv{A}\bv{x})'(\bv{A}\bv{x})$, which is a function of $\bv{A}\bv{x}$. -->
<!-- ::: -->
<!-- :::{.proposition #update name="Bayesian update in a vector of Gaussian variables"} -->
<!-- If -->
<!-- $$ -->
<!-- \left[ -->
<!-- \begin{array}{c} -->
<!-- Y_1\\ -->
<!-- Y_2 -->
<!-- \end{array} -->
<!-- \right] -->
<!-- \sim \mathcal{N} -->
<!-- \left(0, -->
<!-- \left[\begin{array}{cc} -->
<!-- \Omega_{11} & \Omega_{12}\\ -->
<!-- \Omega_{21} & \Omega_{22} -->
<!-- \end{array}\right] -->
<!-- \right), -->
<!-- $$ -->
<!-- then -->
<!-- $$ -->
<!-- Y_{2}|Y_{1} \sim \mathcal{N} -->
<!-- \left( -->
<!-- \Omega_{21}\Omega_{11}^{-1}Y_{1},\Omega_{22}-\Omega_{21}\Omega_{11}^{-1}\Omega_{12} -->
<!-- \right). -->
<!-- $$ -->
<!-- $$ -->
<!-- Y_{1}|Y_{2} \sim \mathcal{N} -->
<!-- \left( -->
<!-- \Omega_{12}\Omega_{22}^{-1}Y_{2},\Omega_{11}-\Omega_{12}\Omega_{22}^{-1}\Omega_{21} -->
<!-- \right). -->
<!-- $$ -->
<!-- ::: -->
<!-- :::{.proposition #truncated name="Truncated distributions"} -->
<!-- If $X$ is a random variable distributed according to some p.d.f. $f$, with c.d.f. $F$, with infinite support. Then the p.d.f. of $X|a \le X < b$ is -->
<!-- $$ -->
<!-- g(x) = \frac{f(x)}{F(b)-F(a)}\mathbb{I}_{\{a \le x < b\}}, -->
<!-- $$ -->
<!-- for any $a<b$. -->
<!-- In partiucular, for a Gaussian variable $X \sim \mathcal{N}(\mu,\sigma^2)$, we have -->
<!-- $$ -->
<!-- f(X=x|a\le X<b) = \dfrac{\dfrac{1}{\sigma}\phi\left(\dfrac{x - \mu}{\sigma}\right)}{Z}. -->
<!-- $$ -->
<!-- with $Z = \Phi(\beta)-\Phi(\alpha)$, where $\alpha = \dfrac{a - \mu}{\sigma}$ and $\beta = \dfrac{b - \mu}{\sigma}$. -->
<!-- Moreover: -->
<!-- \begin{eqnarray} -->
<!-- \mathbb{E}(X|a\le X<b) &=& \mu - \frac{\phi\left(\beta\right)-\phi\left(\alpha\right)}{Z}\sigma. (\#eq:Etrunc) -->
<!-- \end{eqnarray} -->
<!-- We also have: -->
<!-- \begin{eqnarray} -->
<!-- && \mathbb{V}ar(X|a\le X<b) \nonumber\\ -->
<!-- &=& \sigma^2\left[ -->
<!-- 1 -  \frac{\beta\phi\left(\beta\right)-\alpha\phi\left(\alpha\right)}{Z} -  \left(\frac{\phi\left(\beta\right)-\phi\left(\alpha\right)}{Z}\right)^2 \right] (\#eq:Vtrunc) -->
<!-- \end{eqnarray} -->
<!-- In particular, for $b \rightarrow \infty$, we get: -->
<!-- \begin{equation} -->
<!-- \mathbb{V}ar(X|a < X) = \sigma^2\left[1 + \alpha\lambda(-\alpha) - \lambda(-\alpha)^2 \right], (\#eq:Vtrunc2) -->
<!-- \end{equation} -->
<!-- with $\lambda(x)=\dfrac{\phi(x)}{\Phi(x)}$ is called the **inverse Mills ratio**. -->
<!-- ::: -->
<!-- Consider the case where $a \rightarrow - \infty$ (i.e. the conditioning set is $X<b$) and $\mu=0$, $\sigma=1$. Then Eq. \@ref(eq:Etrunc) gives $\mathbb{E}(X|X<b) = - \lambda(b) = - \dfrac{\phi(b)}{\Phi(b)}$, where $\lambda$ is the function computing the inverse Mills ratio. -->
<!-- ```{r inverseMills, echo=FALSE, fig.cap="$\\mathbb{E}(X|X<b)$ as a function of $b$ when $X\\sim \\mathcal{N}(0,1)$ (in black).", fig.align = 'left-aligned'} -->
<!-- x <- seq(-10,10,by=.01) -->
<!-- par(mfrow=c(1,1)) -->
<!-- par(plt=c(.15,.95,.25,.95)) -->
<!-- plot(x,-dnorm(x)/pnorm(x),type="l",lwd=2, -->
<!--      xlab="b",ylab="inverse Mills ratio", -->
<!--      ylim=c(-10,5)) -->
<!-- lines(c(-20,20),c(-20,20),col="red") -->
<!-- abline(h=0,col="grey") -->
<!-- lines(x,-dnorm(x)/pnorm(x),lwd=2) -->
<!-- ``` -->
<!-- :::{.proposition #pdfMultivarGaussian name="p.d.f. of a multivariate Gaussian variable"} -->
<!-- If $Y \sim \mathcal{N}(\mu,\Omega)$ and if $Y$ is a $n$-dimensional vector, then the density function of $Y$ is: -->
<!-- $$ -->
<!-- \frac{1}{(2 \pi)^{n/2}|\Omega|^{1/2}}\exp\left[-\frac{1}{2}\left(Y-\mu\right)'\Omega^{-1}\left(Y-\mu\right)\right]. -->
<!-- $$ -->
<!-- ::: -->
</div>
<div id="AppendixProof" class="section level2" number="9.2">
<h2>
<span class="header-section-number">9.2</span> Proofs<a class="anchor" aria-label="anchor" href="#AppendixProof"><i class="fas fa-link"></i></a>
</h2>
<!-- **Proof of Proposition \@ref(prp:MLEproperties)** -->
<!-- :::{.proof} -->
<!-- Assumptions (i) and (ii) (in the set of Assumptions \@ref(hyp:MLEregularity)) imply that $\boldsymbol\theta_{MLE}$ exists ($=\mbox{argmax}_\theta (1/n)\log \mathcal{L}(\boldsymbol\theta;\bv{y})$). -->
<!-- $(1/n)\log \mathcal{L}(\boldsymbol\theta;\bv{y})$ can be interpreted as the sample mean of the r.v. $\log f(Y_i;\boldsymbol\theta)$ that are i.i.d. Therefore $(1/n)\log \mathcal{L}(\boldsymbol\theta;\bv{y})$ converges to $\mathbb{E}_{\boldsymbol\theta_0}(\log f(Y;\boldsymbol\theta))$ -- which exists (Assumption iv). -->
<!-- Because the latter convergence is uniform (Assumption v), the solution $\boldsymbol\theta_{MLE}$ almost surely converges to the solution to the limit problem: -->
<!-- $$ -->
<!-- \mbox{argmax}_\theta \mathbb{E}_{\boldsymbol\theta_0}(\log f(Y;\boldsymbol\theta)) = \mbox{argmax}_\theta \int_{\mathcal{Y}} \log f(y;\boldsymbol\theta)f(y;\boldsymbol\theta_0) dy. -->
<!-- $$ -->
<!-- Properties of the Kullback information measure (see Prop. \@ref(prp:Kullback)), together with the identifiability assumption (ii) implies that the solution to the limit problem is unique and equal to $\boldsymbol\theta_0$. -->
<!-- Consider a r.v. sequence $\boldsymbol\theta$ that converges to $\boldsymbol\theta_0$. The Taylor expansion of the score in a neighborood of $\boldsymbol\theta_0$ yields to: -->
<!-- $$ -->
<!-- \frac{\partial \log \mathcal{L}(\boldsymbol\theta;\bv{y})}{\partial \boldsymbol\theta} = \frac{\partial \log \mathcal{L}(\boldsymbol\theta_0;\bv{y})}{\partial \boldsymbol\theta} + \frac{\partial^2 \log \mathcal{L}(\boldsymbol\theta_0;\bv{y})}{\partial \boldsymbol\theta \partial \boldsymbol\theta'}(\boldsymbol\theta - \boldsymbol\theta_0) + o_p(\boldsymbol\theta - \boldsymbol\theta_0) -->
<!-- $$ -->
<!-- $\boldsymbol\theta_{MLE}$ converges to $\boldsymbol\theta_0$ and satisfies the likelihood equation $\frac{\partial \log \mathcal{L}(\boldsymbol\theta;\bv{y})}{\partial \boldsymbol\theta} = \bv{0}$. Therefore: -->
<!-- $$ -->
<!-- \frac{\partial \log \mathcal{L}(\boldsymbol\theta_0;\bv{y})}{\partial \boldsymbol\theta} \approx - \frac{\partial^2 \log \mathcal{L}(\boldsymbol\theta_0;\bv{y})}{\partial \boldsymbol\theta \partial \boldsymbol\theta'}(\boldsymbol\theta_{MLE} - \boldsymbol\theta_0), -->
<!-- $$ -->
<!-- or equivalently: -->
<!-- $$ -->
<!-- \frac{1}{\sqrt{n}} \frac{\partial \log \mathcal{L}(\boldsymbol\theta_0;\bv{y})}{\partial \boldsymbol\theta} \approx -->
<!-- \left(- \frac{1}{n} \sum_{i=1}^n \frac{\partial^2 \log f(y_i;\boldsymbol\theta_0)}{\partial \boldsymbol\theta \partial \boldsymbol\theta'} \right)\sqrt{n}(\boldsymbol\theta_{MLE} - \boldsymbol\theta_0), -->
<!-- $$ -->
<!-- By the law of large numbers, we have: $\left(- \frac{1}{n} \sum_{i=1}^n \frac{\partial^2 \log f(y_i;\boldsymbol\theta_0)}{\partial \boldsymbol\theta \partial \boldsymbol\theta'} \right) \overset{}\rightarrow \frac{1}{n} \bv{I}(\boldsymbol\theta_0) = \mathcal{I}_Y(\boldsymbol\theta_0)$. -->
<!-- Besides, we have: -->
<!-- \begin{eqnarray*} -->
<!-- \frac{1}{\sqrt{n}} \frac{\partial \log \mathcal{L}(\boldsymbol\theta_0;\bv{y})}{\partial \boldsymbol\theta} &=& \sqrt{n} \left( \frac{1}{n} \sum_i \frac{\partial \log f(y_i;\boldsymbol\theta_0)}{\partial \boldsymbol\theta} \right) \\ -->
<!-- &=& \sqrt{n} \left( \frac{1}{n} \sum_i \left\{ \frac{\partial \log f(y_i;\boldsymbol\theta_0)}{\partial \boldsymbol\theta} - \mathbb{E}_{\boldsymbol\theta_0} \frac{\partial \log f(Y_i;\boldsymbol\theta_0)}{\partial \boldsymbol\theta} \right\} \right) -->
<!-- \end{eqnarray*} -->
<!-- which converges to $\mathcal{N}(0,\mathcal{I}_Y(\boldsymbol\theta_0))$ by the CLT. -->
<!-- Collecting the preceding results leads to (b). The fact that $\boldsymbol\theta_{MLE}$ achieves the FDCR bound proves (c). -->
<!-- ::: -->
<!-- **Proof of Proposition \@ref(prp:Walddistri)** -->
<!-- :::{.proof} -->
<!-- We have $\sqrt{n}(\hat{\boldsymbol\theta}_{n} - \boldsymbol\theta_{0}) \overset{d}{\rightarrow} \mathcal{N}(0,\mathcal{I}(\boldsymbol\theta_0)^{-1})$ (Eq. \ref(eq:normMLE)). A Taylor expansion around $\boldsymbol\theta_0$ yields to: -->
<!-- \begin{equation} -->
<!-- \sqrt{n}(h(\hat{\boldsymbol\theta}_{n}) - h(\boldsymbol\theta_{0})) \overset{d}{\rightarrow} \mathcal{N}\left(0,\frac{\partial h(\boldsymbol\theta_{0})}{\partial \boldsymbol\theta'}\mathcal{I}(\boldsymbol\theta_0)^{-1}\frac{\partial h(\boldsymbol\theta_{0})'}{\partial \boldsymbol\theta}\right). (\#eq:XXX) -->
<!-- \end{equation} -->
<!-- Under $H_0$, $h(\boldsymbol\theta_{0})=0$ therefore: -->
<!-- \begin{equation} -->
<!-- \sqrt{n} h(\hat{\boldsymbol\theta}_{n}) \overset{d}{\rightarrow} \mathcal{N}\left(0,\frac{\partial h(\boldsymbol\theta_{0})}{\partial \boldsymbol\theta'}\mathcal{I}(\boldsymbol\theta_0)^{-1}\frac{\partial h(\boldsymbol\theta_{0})'}{\partial \boldsymbol\theta}\right). (\#eq:lm10) -->
<!-- \end{equation} -->
<!-- Hence -->
<!-- $$ -->
<!-- \sqrt{n} \left( -->
<!-- \frac{\partial h(\boldsymbol\theta_{0})}{\partial \boldsymbol\theta'}\mathcal{I}(\boldsymbol\theta_0)^{-1}\frac{\partial h(\boldsymbol\theta_{0})'}{\partial \boldsymbol\theta} -->
<!-- \right)^{-1/2} h(\hat{\boldsymbol\theta}_{n}) \overset{d}{\rightarrow} \mathcal{N}\left(0,Id\right). -->
<!-- $$ -->
<!-- Taking the quadratic form, we obtain: -->
<!-- $$ -->
<!-- n h(\hat{\boldsymbol\theta}_{n})'  \left( -->
<!-- \frac{\partial h(\boldsymbol\theta_{0})}{\partial \boldsymbol\theta'}\mathcal{I}(\boldsymbol\theta_0)^{-1}\frac{\partial h(\boldsymbol\theta_{0})'}{\partial \boldsymbol\theta} -->
<!-- \right)^{-1} h(\hat{\boldsymbol\theta}_{n}) \overset{d}{\rightarrow} \chi^2(r). -->
<!-- $$ -->
<!-- The fact that the test has asymptotic level $\alpha$ directly stems from what precedes. **Consistency of the test**: Consider $\theta_0 \in \Theta$. Because the MLE is consistent, $h(\hat{\boldsymbol\theta}_{n})$ converges to $h(\boldsymbol\theta_0) \ne 0$. Eq. \@ref(eq:XXX) is still valid. It implies that $\xi^W_n$ converges to $+\infty$ and therefore that $\mathbb{P}_{\boldsymbol\theta}(\xi^W_n \ge \chi^2_{1-\alpha}(r)) \rightarrow 1$. -->
<!-- ::: -->
<!-- **Proof of Proposition \@ref(prp:LMdistri)** -->
<!-- :::{.proof} -->
<!-- Notations: "$\approx$" means "equal up to a term that converges to 0 in probability". We are under $H_0$. $\hat{\boldsymbol\theta}^0$ is the constrained ML estimator; $\hat{\boldsymbol\theta}$ denotes the unconstrained one. -->
<!-- We combine the two Taylor expansion: $h(\hat{\boldsymbol\theta}_n) \approx \dfrac{\partial h(\boldsymbol\theta_0)}{\partial \boldsymbol\theta'}(\hat{\boldsymbol\theta}_n - \boldsymbol\theta_0)$ and $h(\hat{\boldsymbol\theta}_n^0) \approx \dfrac{\partial h(\boldsymbol\theta_0)}{\partial \boldsymbol\theta'}(\hat{\boldsymbol\theta}_n^0 - \boldsymbol\theta_0)$ and we use $h(\hat{\boldsymbol\theta}_n^0)=0$ (by definition) to get: -->
<!-- \begin{equation} -->
<!-- \sqrt{n}h(\hat{\boldsymbol\theta}_n) \approx \dfrac{\partial h(\boldsymbol\theta_0)}{\partial \boldsymbol\theta'}\sqrt{n}(\hat{\boldsymbol\theta}_n - \hat{\boldsymbol\theta}^0_n). (\#eq:lm1) -->
<!-- \end{equation} -->
<!-- Besides, we have (using the definition of the information matrix): -->
<!-- \begin{equation} -->
<!-- \frac{1}{\sqrt{n}}\frac{\partial \log \mathcal{L}(\hat{\boldsymbol\theta}^0_n;\bv{y})}{\partial \boldsymbol\theta} \approx -->
<!-- \frac{1}{\sqrt{n}}\frac{\partial \log \mathcal{L}(\boldsymbol\theta_0;\bv{y})}{\partial \boldsymbol\theta} - \mathcal{I}(\boldsymbol\theta_0)\sqrt{n}(\hat{\boldsymbol\theta}^0_n-\boldsymbol\theta_0) (\#eq:lm29) -->
<!-- \end{equation} -->
<!-- and: -->
<!-- \begin{equation} -->
<!-- 0=\frac{1}{\sqrt{n}}\frac{\partial \log \mathcal{L}(\hat{\boldsymbol\theta}_n;\bv{y})}{\partial \boldsymbol\theta} \approx -->
<!-- \frac{1}{\sqrt{n}}\frac{\partial \log \mathcal{L}(\boldsymbol\theta_0;\bv{y})}{\partial \boldsymbol\theta} - \mathcal{I}(\boldsymbol\theta_0)\sqrt{n}(\hat{\boldsymbol\theta}_n-\boldsymbol\theta_0).(\#eq:lm30) -->
<!-- \end{equation} -->
<!-- Taking the difference and multiplying by $\mathcal{I}(\boldsymbol\theta_0)^{-1}$: -->
<!-- \begin{equation} -->
<!-- \sqrt{n}(\hat{\boldsymbol\theta}_n-\hat{\boldsymbol\theta}_n^0) \approx -->
<!-- \mathcal{I}(\boldsymbol\theta_0)^{-1}\frac{1}{\sqrt{n}}\frac{\partial \log \mathcal{L}(\hat{\boldsymbol\theta}^0_n;\bv{y})}{\partial \boldsymbol\theta} -->
<!-- \mathcal{I}(\boldsymbol\theta_0).(\#eq:lm2) -->
<!-- \end{equation} -->
<!-- Eqs. \@ref(eq:lm1) and \@ref(eq:lm2) yield to: -->
<!-- \begin{equation} -->
<!-- \sqrt{n}h(\hat{\boldsymbol\theta}_n) \approx \dfrac{\partial h(\boldsymbol\theta_0)}{\partial \boldsymbol\theta'} \mathcal{I}(\boldsymbol\theta_0)^{-1}\frac{1}{\sqrt{n}}\frac{\partial \log \mathcal{L}(\hat{\boldsymbol\theta}^0_n;\bv{y})}{\partial \boldsymbol\theta}.(\#eq:lm3) -->
<!-- \end{equation} -->
<!-- Recall that $\hat{\boldsymbol\theta}^0_n$ is the MLE of $\boldsymbol\theta_0$ under the constraint $h(\boldsymbol\theta)=0$. The vector of Lagrange multipliers $\hat\lambda_n$ associated to this program satisfies: -->
<!-- \begin{equation} -->
<!-- \frac{\partial \log \mathcal{L}(\hat{\boldsymbol\theta}^0_n;\bv{y})}{\partial \boldsymbol\theta}+ \frac{\partial h'(\hat{\boldsymbol\theta}^0_n;\bv{y})}{\partial \boldsymbol\theta}\hat\lambda_n = 0.(\#eq:multiplier) -->
<!-- \end{equation} -->
<!-- Substituting the latter equation in Eq. \@ref(eq:lm3) gives: -->
<!-- $$ -->
<!-- \sqrt{n}h(\hat{\boldsymbol\theta}_n) \approx -->
<!-- - \dfrac{\partial h(\boldsymbol\theta_0)}{\partial \boldsymbol\theta'} \mathcal{I}(\boldsymbol\theta_0)^{-1} -->
<!-- \frac{\partial h'(\hat{\boldsymbol\theta}^0_n;\bv{y})}{\partial \boldsymbol\theta} \frac{\hat\lambda_n}{\sqrt{n}} \approx -->
<!-- - \dfrac{\partial h(\boldsymbol\theta_0)}{\partial \boldsymbol\theta'} \mathcal{I}(\boldsymbol\theta_0)^{-1} -->
<!-- \frac{\partial h'(\boldsymbol\theta_0;\bv{y})}{\partial \boldsymbol\theta} \frac{\hat\lambda_n}{\sqrt{n}} -->
<!-- $$ -->
<!-- which yields: -->
<!-- \begin{equation} -->
<!-- \frac{\hat\lambda_n}{\sqrt{n}} \approx - \left( -->
<!-- \dfrac{\partial h(\boldsymbol\theta_0)}{\partial \boldsymbol\theta'} \mathcal{I}(\boldsymbol\theta_0)^{-1} -->
<!-- \frac{\partial h'(\boldsymbol\theta_0;\bv{y})}{\partial \boldsymbol\theta} -->
<!-- \right)^{-1} -->
<!-- \sqrt{n}h(\hat{\boldsymbol\theta}_n).(\#eq:lm20) -->
<!-- \end{equation} -->
<!-- It follows, from Eq. \@ref(eq:lm10), that: -->
<!-- $$ -->
<!-- \frac{\hat\lambda_n}{\sqrt{n}} \overset{d}{\rightarrow} \mathcal{N}\left(0,\left( -->
<!-- \dfrac{\partial h(\boldsymbol\theta_0)}{\partial \boldsymbol\theta'} \mathcal{I}(\boldsymbol\theta_0)^{-1} -->
<!-- \frac{\partial h'(\boldsymbol\theta_0;\bv{y})}{\partial \boldsymbol\theta} -->
<!-- \right)^{-1}\right). -->
<!-- $$ -->
<!-- Taking the quadratic form of the last equation gives: -->
<!-- $$ -->
<!-- \frac{1}{n}\hat\lambda_n' \dfrac{\partial h(\hat{\boldsymbol\theta}^0_n)}{\partial \boldsymbol\theta'} \mathcal{I}(\hat{\boldsymbol\theta}^0_n)^{-1} -->
<!-- \frac{\partial h'(\hat{\boldsymbol\theta}^0_n;\bv{y})}{\partial \boldsymbol\theta} \hat\lambda_n \overset{d}{\rightarrow} \chi^2(r). -->
<!-- $$ -->
<!-- Using Eq. \@ref(eq:multiplier), it appears that the left-hand side term of the last equation is $\xi^{LM}$ as defined in Eq. \@ref(eq:xiLM). Consistency: see Remark 17.3 in @gourieroux_monfort_1995. -->
<!-- ::: -->
<!-- **Proof of Proposition \@ref(prp:equivLRLMW)** -->
<!-- :::{.proof} -->
<!-- Let us first demonstrate the asymptotic equivalence of $\xi^{LM}$ and $\xi^{LR}$. -->
<!-- The second-order taylor expansions of $\log \mathcal{L}(\hat{\boldsymbol\theta}^0_n,\bv{y})$ and $\log \mathcal{L}(\hat{\boldsymbol\theta}_n,\bv{y})$ are: -->
<!-- \begin{eqnarray*} -->
<!-- \log \mathcal{L}(\hat{\boldsymbol\theta}_n,\bv{y}) &\approx& \log \mathcal{L}(\boldsymbol\theta_0,\bv{y}) -->
<!-- + \frac{\partial \log \mathcal{L}(\boldsymbol\theta_0,\bv{y})}{\partial \boldsymbol\theta'}(\hat{\boldsymbol\theta}_n-\boldsymbol\theta_0) -->
<!-- - \frac{n}{2} (\hat{\boldsymbol\theta}_n-\boldsymbol\theta_0)' \mathcal{I}(\boldsymbol\theta_0) (\hat{\boldsymbol\theta}_n-\boldsymbol\theta_0)\\ -->
<!-- \log \mathcal{L}(\hat{\boldsymbol\theta}^0_n,\bv{y}) &\approx& \log \mathcal{L}(\boldsymbol\theta_0,\bv{y}) -->
<!-- + \frac{\partial \log \mathcal{L}(\boldsymbol\theta_0,\bv{y})}{\partial \boldsymbol\theta'}(\hat{\boldsymbol\theta}^0_n-\boldsymbol\theta_0) -->
<!-- - \frac{n}{2} (\hat{\boldsymbol\theta}^0_n-\boldsymbol\theta_0)' \mathcal{I}(\boldsymbol\theta_0) (\hat{\boldsymbol\theta}^0_n-\boldsymbol\theta_0). -->
<!-- \end{eqnarray*} -->
<!-- Taking the difference, we obtain: -->
<!-- $$ -->
<!-- \xi_n^{LR} \approx 2\frac{\partial \log \mathcal{L}(\boldsymbol\theta_0,\bv{y})}{\partial \boldsymbol\theta'} -->
<!-- (\hat{\boldsymbol\theta}_n-\hat{\boldsymbol\theta}^0_n) + n (\hat{\boldsymbol\theta}^0_n-\boldsymbol\theta_0)' \mathcal{I}(\boldsymbol\theta_0) (\hat{\boldsymbol\theta}^0_n-\boldsymbol\theta_0) - n (\hat{\boldsymbol\theta}_n-\boldsymbol\theta_0)' \mathcal{I}(\boldsymbol\theta_0) (\hat{\boldsymbol\theta}_n-\boldsymbol\theta_0). -->
<!-- $$ -->
<!-- Using $\dfrac{1}{\sqrt{n}}\frac{\partial \log \mathcal{L}(\boldsymbol\theta_0;\bv{y})}{\partial \boldsymbol\theta} \approx -->
<!-- \mathcal{I}(\boldsymbol\theta_0)\sqrt{n}(\hat{\boldsymbol\theta}_n-\boldsymbol\theta_0)$ (Eq. \@ref(eq:lm30)), we have: -->
<!-- $$ -->
<!-- \xi_n^{LR} \approx -->
<!-- 2n(\hat{\boldsymbol\theta}_n-\boldsymbol\theta_0)'\mathcal{I}(\boldsymbol\theta_0) -->
<!-- (\hat{\boldsymbol\theta}_n-\hat{\boldsymbol\theta}^0_n) -->
<!-- + n (\hat{\boldsymbol\theta}^0_n-\boldsymbol\theta_0)' \mathcal{I}(\boldsymbol\theta_0) (\hat{\boldsymbol\theta}^0_n-\boldsymbol\theta_0) - n (\hat{\boldsymbol\theta}_n-\boldsymbol\theta_0)' \mathcal{I}(\boldsymbol\theta_0) (\hat{\boldsymbol\theta}_n-\boldsymbol\theta_0). -->
<!-- $$ -->
<!-- In the second of the three terms in the sum, we replace $(\hat{\boldsymbol\theta}^0_n-\boldsymbol\theta_0)$ by $(\hat{\boldsymbol\theta}^0_n-\hat{\boldsymbol\theta}_n+\hat{\boldsymbol\theta}_n-\boldsymbol\theta_0)$ and we develop the associated product. This leads to: -->
<!-- \begin{equation} -->
<!-- \xi_n^{LR} \approx n (\hat{\boldsymbol\theta}^0_n-\hat{\boldsymbol\theta}_n)' \mathcal{I}(\boldsymbol\theta_0)^{-1} (\hat{\boldsymbol\theta}^0_n-\hat{\boldsymbol\theta}_n). (\#eq:lr10) -->
<!-- \end{equation} -->
<!-- The difference between Eqs. \@ref(eq:lm29) and \@ref(eq:lm30) implies: -->
<!-- $$ -->
<!-- \frac{1}{\sqrt{n}}\frac{\partial \log \mathcal{L}(\hat{\boldsymbol\theta}^0_n;\bv{y})}{\partial \boldsymbol\theta} \approx -->
<!-- \mathcal{I}(\boldsymbol\theta_0)\sqrt{n}(\hat{\boldsymbol\theta}_n-\hat{\boldsymbol\theta}^0_n), -->
<!-- $$ -->
<!-- which, associated to Eq. @\ref(eq:lr10), gives: -->
<!-- $$ -->
<!-- \xi_n^{LR} \approx \frac{1}{n} \frac{\partial \log \mathcal{L}(\hat{\boldsymbol\theta}^0_n;\bv{y})}{\partial \boldsymbol\theta'} \mathcal{I}(\boldsymbol\theta_0)^{-1} \frac{\partial \log \mathcal{L}(\hat{\boldsymbol\theta}^0_n;\bv{y})}{\partial \boldsymbol\theta} \approx \xi_n^{LM}. -->
<!-- $$ -->
<!-- Hence $\xi_n^{LR}$ has the same asymptotic distribution as $\xi_n^{LM}$. -->
<!-- Let's show that the LR test is consistent. For this, note that: -->
<!-- $$ -->
<!-- \frac{\log \mathcal{L}(\hat{\boldsymbol\theta},\bv{y}) - \log \mathcal{L}(\hat{\boldsymbol\theta}^0,\bv{y})}{n} = \frac{1}{n} \sum_{i=1}^n[\log f(y_i;\hat{\boldsymbol\theta}_n) - \log f(y_i;\hat{\boldsymbol\theta}_n^0)] \rightarrow \mathbb{E}_0[\log f(Y;\boldsymbol\theta_0) - \log f(Y;\boldsymbol\theta_\infty)], -->
<!-- $$ -->
<!-- where $\boldsymbol\theta_\infty$, the pseudo true value, is such that $h(\boldsymbol\theta_\infty) \ne 0$ (by definition of $H_1$). From the Kullback inequality and the asymptotic identifiability of $\boldsymbol\theta_0$, it follows that $\mathbb{E}_0[\log f(Y;\boldsymbol\theta_0) - \log f(Y;\boldsymbol\theta_\infty)] >0$. Therefore $\xi_n^{LR} \rightarrow + \infty$ under $H_1$. -->
<!-- Let us now demonstrate the equivalence of $\xi^{LM} and \xi^{W}$. -->
<!-- We have (using Eq. \ref(eq:multiplier)): -->
<!-- $$ -->
<!-- \xi^{LM}_n = \frac{1}{n}\hat\lambda_n' \dfrac{\partial h(\hat{\boldsymbol\theta}^0_n)}{\partial \boldsymbol\theta'} \mathcal{I}(\hat{\boldsymbol\theta}^0_n)^{-1} -->
<!-- \frac{\partial h'(\hat{\boldsymbol\theta}^0_n;\bv{y})}{\partial \boldsymbol\theta} \hat\lambda_n. -->
<!-- $$ -->
<!-- Since, under $H_0$, $\hat{\boldsymbol\theta}_n^0\approx\hat{\boldsymbol\theta}_n \approx {\boldsymbol\theta}_0$, Eq. \@ref(eq:lm20) therefore implies that: -->
<!-- $$ -->
<!-- \xi^{LM} \approx n h(\hat{\boldsymbol\theta}_n)' \left( -->
<!-- \dfrac{\partial h(\hat{\boldsymbol\theta}_n)}{\partial \boldsymbol\theta'} \mathcal{I}(\hat{\boldsymbol\theta}_n)^{-1} -->
<!-- \frac{\partial h'(\hat{\boldsymbol\theta}_n;\bv{y})}{\partial \boldsymbol\theta} -->
<!-- \right)^{-1} -->
<!-- h(\hat{\boldsymbol\theta}_n) = \xi^{W}, -->
<!-- $$ -->
<!-- which gives the result. -->
<!-- ::: -->
<!-- **Proof of Eq. \@ref(eq:TCL2)** -->
<!-- :::{.proof} -->
<!-- We have: -->
<!-- \begin{eqnarray*} -->
<!-- &&T\mathbb{E}\left[(\bar{y}_T - \mu)^2\right]\\ -->
<!-- &=& T\mathbb{E}\left[\left(\frac{1}{T}\sum_{t=1}^T(y_t - \mu)\right)^2\right] = \frac{1}{T} \mathbb{E}\left[\sum_{t=1}^T(y_t - \mu)^2+2\sum_{s<t\le T}(y_t - \mu)(y_s - \mu)\right]\\ -->
<!-- &=& \gamma_0 +\frac{2}{T}\left(\sum_{t=2}^{T}\mathbb{E}\left[(y_t - \mu)(y_{t-1} - \mu)\right]\right) +\frac{2}{T}\left(\sum_{t=3}^{T}\mathbb{E}\left[(y_t - \mu)(y_{t-2} - \mu)\right]\right) + \dots \\ -->
<!-- &&+ \frac{2}{T}\left(\sum_{t=T-1}^{T}\mathbb{E}\left[(y_t - \mu)(y_{t-(T-2)} - \mu)\right]\right) + \frac{2}{T}\mathbb{E}\left[(y_t - \mu)(y_{t-(T-1)} - \mu)\right]\\ -->
<!-- &=&  \gamma_0 + 2 \frac{T-1}{T}\gamma_1 + \dots + 2 \frac{1}{T}\gamma_{T-1} . -->
<!-- \end{eqnarray*} -->
<!-- Therefore: -->
<!-- \begin{eqnarray*} -->
<!-- T\mathbb{E}\left[(\bar{y}_T - \mu)^2\right] - \sum_{j=-\infty}^{+\infty} \gamma_j &=& - 2\frac{1}{T}\gamma_1 - 2\frac{2}{T}\gamma_2 - \dots - 2\frac{T-1}{T}\gamma_{T-1} - 2\gamma_T - 2 \gamma_{T+1} + \dots -->
<!-- \end{eqnarray*} -->
<!-- And then: -->
<!-- $$ -->
<!-- \left|T\mathbb{E}\left[(\bar{y}_T - \mu)^2\right] - \sum_{j=-\infty}^{+\infty} \gamma_j\right| \le 2\frac{1}{T}|\gamma_1| + 2\frac{2}{T}|\gamma_2| + \dots + 2\frac{T-1}{T}|\gamma_{T-1}| + 2|\gamma_T| + 2 |\gamma_{T+1}| + \dots -->
<!-- $$ -->
<!-- For any $q \le T$, we have: -->
<!-- \begin{eqnarray*} -->
<!-- \left|T\mathbb{E}\left[(\bar{y}_T - \mu)^2\right] - \sum_{j=-\infty}^{+\infty} \gamma_j\right| &\le& 2\frac{1}{T}|\gamma_1| + 2\frac{2}{T}|\gamma_2| + \dots + 2\frac{q-1}{T}|\gamma_{q-1}| +2\frac{q}{T}|\gamma_q| +\\ -->
<!-- &&2\frac{q+1}{T}|\gamma_{q+1}| + \dots  + 2\frac{T-1}{T}|\gamma_{T-1}| + 2|\gamma_T| + 2 |\gamma_{T+1}| + \dots\\ -->
<!-- &\le& \frac{2}{T}\left(|\gamma_1| + 2|\gamma_2| + \dots + (q-1)|\gamma_{q-1}| +q|\gamma_q|\right) +\\ -->
<!-- &&2|\gamma_{q+1}| + \dots  + 2|\gamma_{T-1}| + 2|\gamma_T| + 2 |\gamma_{T+1}| + \dots -->
<!-- \end{eqnarray*} -->
<!-- Consider $\varepsilon > 0$. The fact that the autocovariances are absolutely summable implies that there exists $q_0$ such that (Cauchy criterion, Theorem \@ref(thm:cauchycritstatic)): -->
<!-- $$ -->
<!-- 2|\gamma_{q_0+1}|+2|\gamma_{q_0+2}|+2|\gamma_{q_0+3}|+\dots < \varepsilon/2. -->
<!-- $$ -->
<!-- Then, if $T > q_0$, it comes that: -->
<!-- $$ -->
<!-- \left|T \mathbb{E}\left[(\bar{y}_T - \mu)^2\right] - \sum_{j=-\infty}^{+\infty} \gamma_j\right|\le \frac{2}{T}\left(|\gamma_1| + 2|\gamma_2| + \dots + (q_0-1)|\gamma_{q_0-1}| +q_0|\gamma_{q_0}|\right) + \varepsilon/2. -->
<!-- $$ -->
<!-- If $T \ge 2\left(|\gamma_1| + 2|\gamma_2| + \dots + (q_0-1)|\gamma_{q_0-1}| +q_0|\gamma_{q_0}|\right)/(\varepsilon/2)$ ($= f(q_0)$, say) then -->
<!-- $$ -->
<!-- \frac{2}{T}\left(|\gamma_1| + 2|\gamma_2| + \dots + (q_0-1)|\gamma_{q_0-1}| +q_0|\gamma_{q_0}|\right) \le \varepsilon/2. -->
<!-- $$ -->
<!-- Then, if $T>f(q_0)$ and $T>q_0$, i.e. if $T>\max(f(q_0),q_0)$, we have: -->
<!-- $$ -->
<!-- \left|T \mathbb{E}\left[(\bar{y}_T - \mu)^2\right] - \sum_{j=-\infty}^{+\infty} \gamma_j\right|\le \varepsilon. -->
<!-- $$ -->
<!-- ::: -->
<!-- **Proof of Proposition \@ref(prp:smallestMSE)** -->
<!-- :::{.proof} -->
<!-- We have: -->
<!-- \begin{eqnarray} -->
<!-- \mathbb{E}([y_{t+1} - y^*_{t+1}]^2) &=& \mathbb{E}\left([\color{blue}{\{y_{t+1} - \mathbb{E}(y_{t+1}|x_t)\}} + \color{red}{\{\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}\}}]^2\right)\nonumber\\ -->
<!-- &=&  \mathbb{E}\left(\color{blue}{[y_{t+1} - \mathbb{E}(y_{t+1}|x_t)]}^2\right) + \mathbb{E}\left(\color{red}{[\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]}^2\right)\nonumber\\ -->
<!-- && + 2\mathbb{E}\left( \color{blue}{[y_{t+1} - \mathbb{E}(y_{t+1}|x_t)]}\color{red}{ [\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]}\right). (\#eq:1) -->
<!-- \end{eqnarray} -->
<!-- Let us focus on the last term. We have: -->
<!-- \begin{eqnarray*} -->
<!-- &&\mathbb{E}\left(\color{blue}{[y_{t+1} - \mathbb{E}(y_{t+1}|x_t)]}\color{red}{ [\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]}\right)\\ -->
<!-- &=& \mathbb{E}( \mathbb{E}( \color{blue}{[y_{t+1} - \mathbb{E}(y_{t+1}|x_t)]}\color{red}{ \underbrace{[\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]}_{\mbox{function of $x_t$}}}|x_t))\\ -->
<!-- &=& \mathbb{E}( \color{red}{ [\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]} \mathbb{E}( \color{blue}{[y_{t+1} - \mathbb{E}(y_{t+1}|x_t)]}|x_t))\\ -->
<!-- &=& \mathbb{E}( \color{red}{ [\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]} \color{blue}{\underbrace{[\mathbb{E}(y_{t+1}|x_t) - \mathbb{E}(y_{t+1}|x_t)]}_{=0}})=0. -->
<!-- \end{eqnarray*} -->
<!-- Therefore, Eq. \@ref(eq:1) becomes: -->
<!-- \begin{eqnarray*} -->
<!-- &&\mathbb{E}([y_{t+1} - y^*_{t+1}]^2) \\ -->
<!-- &=&  \underbrace{\mathbb{E}\left(\color{blue}{[y_{t+1} - \mathbb{E}(y_{t+1}|x_t)]}^2\right)}_{\mbox{$\ge 0$ and does not depend on $y^*_{t+1}$}} + \underbrace{\mathbb{E}\left(\color{red}{[\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]}^2\right)}_{\mbox{$\ge 0$ and depends on $y^*_{t+1}$}}. -->
<!-- \end{eqnarray*} -->
<!-- This implies that $\mathbb{E}([y_{t+1} - y^*_{t+1}]^2)$ is always larger than $\color{blue}{\mathbb{E}([y_{t+1} - \mathbb{E}(y_{t+1}|x_t)]^2)}$, and is therefore minimized if the second term is equal to zero, that is if $\mathbb{E}(y_{t+1}|x_t) = y^*_{t+1}$. -->
<!-- ::: -->
<p><strong>Proof of Proposition <a href="basics.html#prp:estimVARGaussian">1.2</a></strong></p>
<div class="proof">
<p><span id="unlabeled-div-6" class="proof"><em>Proof</em>. </span>Using Proposition <a href="append.html#prp:pdfMultivarGaussian">9.1</a>, we obtain that, conditionally on <span class="math inline">\(x_1\)</span>, the log-likelihood is given by
<span class="math display">\[\begin{eqnarray*}
\log\mathcal{L}(Y_{T};\theta) &amp; = &amp; -(Tn/2)\log(2\pi)+(T/2)\log\left|\Omega^{-1}\right|\\
&amp;  &amp; -\frac{1}{2}\sum_{t=1}^{T}\left[\left(y_{t}-\Pi'x_{t}\right)'\Omega^{-1}\left(y_{t}-\Pi'x_{t}\right)\right].
\end{eqnarray*}\]</span>
Let’s rewrite the last term of the log-likelihood:
<span class="math display">\[\begin{eqnarray*}
\sum_{t=1}^{T}\left[\left(y_{t}-\Pi'x_{t}\right)'\Omega^{-1}\left(y_{t}-\Pi'x_{t}\right)\right] &amp; =\\
\sum_{t=1}^{T}\left[\left(y_{t}-\hat{\Pi}'x_{t}+\hat{\Pi}'x_{t}-\Pi'x_{t}\right)'\Omega^{-1}\left(y_{t}-\hat{\Pi}'x_{t}+\hat{\Pi}'x_{t}-\Pi'x_{t}\right)\right] &amp; =\\
\sum_{t=1}^{T}\left[\left(\hat{\varepsilon}_{t}+(\hat{\Pi}-\Pi)'x_{t}\right)'\Omega^{-1}\left(\hat{\varepsilon}_{t}+(\hat{\Pi}-\Pi)'x_{t}\right)\right],
\end{eqnarray*}\]</span>
where the <span class="math inline">\(j^{th}\)</span> element of the <span class="math inline">\((n\times1)\)</span> vector <span class="math inline">\(\hat{\varepsilon}_{t}\)</span> is the sample residual, for observation <span class="math inline">\(t\)</span>, from an OLS regression of <span class="math inline">\(y_{j,t}\)</span> on <span class="math inline">\(x_{t}\)</span>. Expanding the previous equation, we get:
<span class="math display">\[\begin{eqnarray*}
&amp;&amp;\sum_{t=1}^{T}\left[\left(y_{t}-\Pi'x_{t}\right)'\Omega^{-1}\left(y_{t}-\Pi'x_{t}\right)\right]  = \sum_{t=1}^{T}\hat{\varepsilon}_{t}'\Omega^{-1}\hat{\varepsilon}_{t}\\
&amp;&amp;+2\sum_{t=1}^{T}\hat{\varepsilon}_{t}'\Omega^{-1}(\hat{\Pi}-\Pi)'x_{t}+\sum_{t=1}^{T}x'_{t}(\hat{\Pi}-\Pi)\Omega^{-1}(\hat{\Pi}-\Pi)'x_{t}.
\end{eqnarray*}\]</span>
Let’s apply the trace operator on the second term (that is a scalar):
<span class="math display">\[\begin{eqnarray*}
\sum_{t=1}^{T}\hat{\varepsilon}_{t}'\Omega^{-1}(\hat{\Pi}-\Pi)'x_{t} &amp; = &amp; Tr\left(\sum_{t=1}^{T}\hat{\varepsilon}_{t}'\Omega^{-1}(\hat{\Pi}-\Pi)'x_{t}\right)\\
=  Tr\left(\sum_{t=1}^{T}\Omega^{-1}(\hat{\Pi}-\Pi)'x_{t}\hat{\varepsilon}_{t}'\right) &amp; = &amp; Tr\left(\Omega^{-1}(\hat{\Pi}-\Pi)'\sum_{t=1}^{T}x_{t}\hat{\varepsilon}_{t}'\right).
\end{eqnarray*}\]</span>
Given that, by construction (property of OLS estimates), the sample residuals are orthogonal to the explanatory variables, this term is zero. Introducing <span class="math inline">\(\tilde{x}_{t}=(\hat{\Pi}-\Pi)'x_{t}\)</span>, we have
<span class="math display">\[\begin{eqnarray*}
\sum_{t=1}^{T}\left[\left(y_{t}-\Pi'x_{t}\right)'\Omega^{-1}\left(y_{t}-\Pi'x_{t}\right)\right] =\sum_{t=1}^{T}\hat{\varepsilon}_{t}'\Omega^{-1}\hat{\varepsilon}_{t}+\sum_{t=1}^{T}\tilde{x}'_{t}\Omega^{-1}\tilde{x}_{t}.
\end{eqnarray*}\]</span>
Since <span class="math inline">\(\Omega\)</span> is a positive definite matrix, <span class="math inline">\(\Omega^{-1}\)</span> is as well. Consequently, the smallest value that the last term can take is obtained for <span class="math inline">\(\tilde{x}_{t}=0\)</span>, i.e. when <span class="math inline">\(\Pi=\hat{\Pi}.\)</span></p>
<p>The MLE of <span class="math inline">\(\Omega\)</span> is the matrix <span class="math inline">\(\hat{\Omega}\)</span> that maximizes <span class="math inline">\(\Omega\overset{\ell}{\rightarrow}L(Y_{T};\hat{\Pi},\Omega)\)</span>. We have:
<span class="math display">\[\begin{eqnarray*}
\log\mathcal{L}(Y_{T};\hat{\Pi},\Omega) &amp; = &amp; -(Tn/2)\log(2\pi)+(T/2)\log\left|\Omega^{-1}\right| -\frac{1}{2}\sum_{t=1}^{T}\left[\hat{\varepsilon}_{t}'\Omega^{-1}\hat{\varepsilon}_{t}\right].
\end{eqnarray*}\]</span></p>
<p>Matrix <span class="math inline">\(\hat{\Omega}\)</span> is a symmetric positive definite. It is easily checked that the (unrestricted) matrix that maximizes the latter expression is symmetric positive definite matrix. Indeed:
<span class="math display">\[
\frac{\partial \log\mathcal{L}(Y_{T};\hat{\Pi},\Omega)}{\partial\Omega}=\frac{T}{2}\Omega'-\frac{1}{2}\sum_{t=1}^{T}\hat{\varepsilon}_{t}\hat{\varepsilon}'_{t}\Rightarrow\hat{\Omega}'=\frac{1}{T}\sum_{t=1}^{T}\hat{\varepsilon}_{t}\hat{\varepsilon}'_{t},
\]</span>
which leads to the result.</p>
</div>
<p><strong>Proof of Proposition <a href="basics.html#prp:OLSVAR">1.3</a></strong></p>
<div class="proof">
<p><span id="unlabeled-div-7" class="proof"><em>Proof</em>. </span>Let us drop the <span class="math inline">\(i\)</span> subscript. Rearranging Eq. <a href="basics.html#eq:olsar1">(1.12)</a>, we have:
<span class="math display">\[
\sqrt{T}(\mathbf{b}-\boldsymbol{\beta}) =  (X'X/T)^{-1}\sqrt{T}(X'\boldsymbol\varepsilon/T).
\]</span>
Let us consider the autocovariances of <span class="math inline">\(\mathbf{v}_t = x_t \varepsilon_t\)</span>, denoted by <span class="math inline">\(\gamma^v_j\)</span>. Using the fact that <span class="math inline">\(x_t\)</span> is a linear combination of past <span class="math inline">\(\varepsilon_t\)</span>s and that <span class="math inline">\(\varepsilon_t\)</span> is a white noise, we get that <span class="math inline">\(\mathbb{E}(\varepsilon_t x_t)=0\)</span>. Therefore
<span class="math display">\[
\gamma^v_j = \mathbb{E}(\varepsilon_t\varepsilon_{t-j}x_tx_{t-j}').
\]</span>
If <span class="math inline">\(j&gt;0\)</span>, we have <span class="math inline">\(\mathbb{E}(\varepsilon_t\varepsilon_{t-j}x_tx_{t-j}')=\mathbb{E}(\mathbb{E}[\varepsilon_t\varepsilon_{t-j}x_tx_{t-j}'|\varepsilon_{t-j},x_t,x_{t-j}])=\)</span> <span class="math inline">\(\mathbb{E}(\varepsilon_{t-j}x_tx_{t-j}'\mathbb{E}[\varepsilon_t|\varepsilon_{t-j},x_t,x_{t-j}])=0\)</span>. Note that we have <span class="math inline">\(\mathbb{E}[\varepsilon_t|\varepsilon_{t-j},x_t,x_{t-j}]=0\)</span> because <span class="math inline">\(\{\varepsilon_t\}\)</span> is an i.i.d. white noise sequence. If <span class="math inline">\(j=0\)</span>, we have:
<span class="math display">\[
\gamma^v_0 = \mathbb{E}(\varepsilon_t^2x_tx_{t}')= \mathbb{E}(\varepsilon_t^2) \mathbb{E}(x_tx_{t}')=\sigma^2\mathbf{Q}.
\]</span>
The convergence in distribution of <span class="math inline">\(\sqrt{T}(X'\boldsymbol\varepsilon/T)=\sqrt{T}\frac{1}{T}\sum_{t=1}^Tv_t\)</span> results from the Central Limit Theorem for covariance-stationary processes, using the <span class="math inline">\(\gamma_j^v\)</span> computed above.</p>
</div>
</div>
<div id="statistical-tables" class="section level2" number="9.3">
<h2>
<span class="header-section-number">9.3</span> Statistical Tables<a class="anchor" aria-label="anchor" href="#statistical-tables"><i class="fas fa-link"></i></a>
</h2>
<div class="inline-table"><table style="width:100%;" class="table table-sm">
<caption>
<span id="tab:Normal">Table 9.1: </span>Quantiles of the <span class="math inline">\(\mathcal{N}(0,1)\)</span> distribution. If <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are respectively the row and column number; then the corresponding cell gives <span class="math inline">\(\mathbb{P}(0&lt;X\le a+b)\)</span>, where <span class="math inline">\(X \sim \mathcal{N}(0,1)\)</span>.</caption>
<colgroup>
<col width="5%">
<col width="9%">
<col width="9%">
<col width="9%">
<col width="9%">
<col width="9%">
<col width="9%">
<col width="9%">
<col width="9%">
<col width="9%">
<col width="9%">
</colgroup>
<thead><tr class="header">
<th align="left"></th>
<th align="right">0</th>
<th align="right">0.01</th>
<th align="right">0.02</th>
<th align="right">0.03</th>
<th align="right">0.04</th>
<th align="right">0.05</th>
<th align="right">0.06</th>
<th align="right">0.07</th>
<th align="right">0.08</th>
<th align="right">0.09</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">0</td>
<td align="right">0.5000</td>
<td align="right">0.6179</td>
<td align="right">0.7257</td>
<td align="right">0.8159</td>
<td align="right">0.8849</td>
<td align="right">0.9332</td>
<td align="right">0.9641</td>
<td align="right">0.9821</td>
<td align="right">0.9918</td>
<td align="right">0.9965</td>
</tr>
<tr class="even">
<td align="left">0.1</td>
<td align="right">0.5040</td>
<td align="right">0.6217</td>
<td align="right">0.7291</td>
<td align="right">0.8186</td>
<td align="right">0.8869</td>
<td align="right">0.9345</td>
<td align="right">0.9649</td>
<td align="right">0.9826</td>
<td align="right">0.9920</td>
<td align="right">0.9966</td>
</tr>
<tr class="odd">
<td align="left">0.2</td>
<td align="right">0.5080</td>
<td align="right">0.6255</td>
<td align="right">0.7324</td>
<td align="right">0.8212</td>
<td align="right">0.8888</td>
<td align="right">0.9357</td>
<td align="right">0.9656</td>
<td align="right">0.9830</td>
<td align="right">0.9922</td>
<td align="right">0.9967</td>
</tr>
<tr class="even">
<td align="left">0.3</td>
<td align="right">0.5120</td>
<td align="right">0.6293</td>
<td align="right">0.7357</td>
<td align="right">0.8238</td>
<td align="right">0.8907</td>
<td align="right">0.9370</td>
<td align="right">0.9664</td>
<td align="right">0.9834</td>
<td align="right">0.9925</td>
<td align="right">0.9968</td>
</tr>
<tr class="odd">
<td align="left">0.4</td>
<td align="right">0.5160</td>
<td align="right">0.6331</td>
<td align="right">0.7389</td>
<td align="right">0.8264</td>
<td align="right">0.8925</td>
<td align="right">0.9382</td>
<td align="right">0.9671</td>
<td align="right">0.9838</td>
<td align="right">0.9927</td>
<td align="right">0.9969</td>
</tr>
<tr class="even">
<td align="left">0.5</td>
<td align="right">0.5199</td>
<td align="right">0.6368</td>
<td align="right">0.7422</td>
<td align="right">0.8289</td>
<td align="right">0.8944</td>
<td align="right">0.9394</td>
<td align="right">0.9678</td>
<td align="right">0.9842</td>
<td align="right">0.9929</td>
<td align="right">0.9970</td>
</tr>
<tr class="odd">
<td align="left">0.6</td>
<td align="right">0.5239</td>
<td align="right">0.6406</td>
<td align="right">0.7454</td>
<td align="right">0.8315</td>
<td align="right">0.8962</td>
<td align="right">0.9406</td>
<td align="right">0.9686</td>
<td align="right">0.9846</td>
<td align="right">0.9931</td>
<td align="right">0.9971</td>
</tr>
<tr class="even">
<td align="left">0.7</td>
<td align="right">0.5279</td>
<td align="right">0.6443</td>
<td align="right">0.7486</td>
<td align="right">0.8340</td>
<td align="right">0.8980</td>
<td align="right">0.9418</td>
<td align="right">0.9693</td>
<td align="right">0.9850</td>
<td align="right">0.9932</td>
<td align="right">0.9972</td>
</tr>
<tr class="odd">
<td align="left">0.8</td>
<td align="right">0.5319</td>
<td align="right">0.6480</td>
<td align="right">0.7517</td>
<td align="right">0.8365</td>
<td align="right">0.8997</td>
<td align="right">0.9429</td>
<td align="right">0.9699</td>
<td align="right">0.9854</td>
<td align="right">0.9934</td>
<td align="right">0.9973</td>
</tr>
<tr class="even">
<td align="left">0.9</td>
<td align="right">0.5359</td>
<td align="right">0.6517</td>
<td align="right">0.7549</td>
<td align="right">0.8389</td>
<td align="right">0.9015</td>
<td align="right">0.9441</td>
<td align="right">0.9706</td>
<td align="right">0.9857</td>
<td align="right">0.9936</td>
<td align="right">0.9974</td>
</tr>
<tr class="odd">
<td align="left">1</td>
<td align="right">0.5398</td>
<td align="right">0.6554</td>
<td align="right">0.7580</td>
<td align="right">0.8413</td>
<td align="right">0.9032</td>
<td align="right">0.9452</td>
<td align="right">0.9713</td>
<td align="right">0.9861</td>
<td align="right">0.9938</td>
<td align="right">0.9974</td>
</tr>
<tr class="even">
<td align="left">1.1</td>
<td align="right">0.5438</td>
<td align="right">0.6591</td>
<td align="right">0.7611</td>
<td align="right">0.8438</td>
<td align="right">0.9049</td>
<td align="right">0.9463</td>
<td align="right">0.9719</td>
<td align="right">0.9864</td>
<td align="right">0.9940</td>
<td align="right">0.9975</td>
</tr>
<tr class="odd">
<td align="left">1.2</td>
<td align="right">0.5478</td>
<td align="right">0.6628</td>
<td align="right">0.7642</td>
<td align="right">0.8461</td>
<td align="right">0.9066</td>
<td align="right">0.9474</td>
<td align="right">0.9726</td>
<td align="right">0.9868</td>
<td align="right">0.9941</td>
<td align="right">0.9976</td>
</tr>
<tr class="even">
<td align="left">1.3</td>
<td align="right">0.5517</td>
<td align="right">0.6664</td>
<td align="right">0.7673</td>
<td align="right">0.8485</td>
<td align="right">0.9082</td>
<td align="right">0.9484</td>
<td align="right">0.9732</td>
<td align="right">0.9871</td>
<td align="right">0.9943</td>
<td align="right">0.9977</td>
</tr>
<tr class="odd">
<td align="left">1.4</td>
<td align="right">0.5557</td>
<td align="right">0.6700</td>
<td align="right">0.7704</td>
<td align="right">0.8508</td>
<td align="right">0.9099</td>
<td align="right">0.9495</td>
<td align="right">0.9738</td>
<td align="right">0.9875</td>
<td align="right">0.9945</td>
<td align="right">0.9977</td>
</tr>
<tr class="even">
<td align="left">1.5</td>
<td align="right">0.5596</td>
<td align="right">0.6736</td>
<td align="right">0.7734</td>
<td align="right">0.8531</td>
<td align="right">0.9115</td>
<td align="right">0.9505</td>
<td align="right">0.9744</td>
<td align="right">0.9878</td>
<td align="right">0.9946</td>
<td align="right">0.9978</td>
</tr>
<tr class="odd">
<td align="left">1.6</td>
<td align="right">0.5636</td>
<td align="right">0.6772</td>
<td align="right">0.7764</td>
<td align="right">0.8554</td>
<td align="right">0.9131</td>
<td align="right">0.9515</td>
<td align="right">0.9750</td>
<td align="right">0.9881</td>
<td align="right">0.9948</td>
<td align="right">0.9979</td>
</tr>
<tr class="even">
<td align="left">1.7</td>
<td align="right">0.5675</td>
<td align="right">0.6808</td>
<td align="right">0.7794</td>
<td align="right">0.8577</td>
<td align="right">0.9147</td>
<td align="right">0.9525</td>
<td align="right">0.9756</td>
<td align="right">0.9884</td>
<td align="right">0.9949</td>
<td align="right">0.9979</td>
</tr>
<tr class="odd">
<td align="left">1.8</td>
<td align="right">0.5714</td>
<td align="right">0.6844</td>
<td align="right">0.7823</td>
<td align="right">0.8599</td>
<td align="right">0.9162</td>
<td align="right">0.9535</td>
<td align="right">0.9761</td>
<td align="right">0.9887</td>
<td align="right">0.9951</td>
<td align="right">0.9980</td>
</tr>
<tr class="even">
<td align="left">1.9</td>
<td align="right">0.5753</td>
<td align="right">0.6879</td>
<td align="right">0.7852</td>
<td align="right">0.8621</td>
<td align="right">0.9177</td>
<td align="right">0.9545</td>
<td align="right">0.9767</td>
<td align="right">0.9890</td>
<td align="right">0.9952</td>
<td align="right">0.9981</td>
</tr>
<tr class="odd">
<td align="left">2</td>
<td align="right">0.5793</td>
<td align="right">0.6915</td>
<td align="right">0.7881</td>
<td align="right">0.8643</td>
<td align="right">0.9192</td>
<td align="right">0.9554</td>
<td align="right">0.9772</td>
<td align="right">0.9893</td>
<td align="right">0.9953</td>
<td align="right">0.9981</td>
</tr>
<tr class="even">
<td align="left">2.1</td>
<td align="right">0.5832</td>
<td align="right">0.6950</td>
<td align="right">0.7910</td>
<td align="right">0.8665</td>
<td align="right">0.9207</td>
<td align="right">0.9564</td>
<td align="right">0.9778</td>
<td align="right">0.9896</td>
<td align="right">0.9955</td>
<td align="right">0.9982</td>
</tr>
<tr class="odd">
<td align="left">2.2</td>
<td align="right">0.5871</td>
<td align="right">0.6985</td>
<td align="right">0.7939</td>
<td align="right">0.8686</td>
<td align="right">0.9222</td>
<td align="right">0.9573</td>
<td align="right">0.9783</td>
<td align="right">0.9898</td>
<td align="right">0.9956</td>
<td align="right">0.9982</td>
</tr>
<tr class="even">
<td align="left">2.3</td>
<td align="right">0.5910</td>
<td align="right">0.7019</td>
<td align="right">0.7967</td>
<td align="right">0.8708</td>
<td align="right">0.9236</td>
<td align="right">0.9582</td>
<td align="right">0.9788</td>
<td align="right">0.9901</td>
<td align="right">0.9957</td>
<td align="right">0.9983</td>
</tr>
<tr class="odd">
<td align="left">2.4</td>
<td align="right">0.5948</td>
<td align="right">0.7054</td>
<td align="right">0.7995</td>
<td align="right">0.8729</td>
<td align="right">0.9251</td>
<td align="right">0.9591</td>
<td align="right">0.9793</td>
<td align="right">0.9904</td>
<td align="right">0.9959</td>
<td align="right">0.9984</td>
</tr>
<tr class="even">
<td align="left">2.5</td>
<td align="right">0.5987</td>
<td align="right">0.7088</td>
<td align="right">0.8023</td>
<td align="right">0.8749</td>
<td align="right">0.9265</td>
<td align="right">0.9599</td>
<td align="right">0.9798</td>
<td align="right">0.9906</td>
<td align="right">0.9960</td>
<td align="right">0.9984</td>
</tr>
<tr class="odd">
<td align="left">2.6</td>
<td align="right">0.6026</td>
<td align="right">0.7123</td>
<td align="right">0.8051</td>
<td align="right">0.8770</td>
<td align="right">0.9279</td>
<td align="right">0.9608</td>
<td align="right">0.9803</td>
<td align="right">0.9909</td>
<td align="right">0.9961</td>
<td align="right">0.9985</td>
</tr>
<tr class="even">
<td align="left">2.7</td>
<td align="right">0.6064</td>
<td align="right">0.7157</td>
<td align="right">0.8078</td>
<td align="right">0.8790</td>
<td align="right">0.9292</td>
<td align="right">0.9616</td>
<td align="right">0.9808</td>
<td align="right">0.9911</td>
<td align="right">0.9962</td>
<td align="right">0.9985</td>
</tr>
<tr class="odd">
<td align="left">2.8</td>
<td align="right">0.6103</td>
<td align="right">0.7190</td>
<td align="right">0.8106</td>
<td align="right">0.8810</td>
<td align="right">0.9306</td>
<td align="right">0.9625</td>
<td align="right">0.9812</td>
<td align="right">0.9913</td>
<td align="right">0.9963</td>
<td align="right">0.9986</td>
</tr>
<tr class="even">
<td align="left">2.9</td>
<td align="right">0.6141</td>
<td align="right">0.7224</td>
<td align="right">0.8133</td>
<td align="right">0.8830</td>
<td align="right">0.9319</td>
<td align="right">0.9633</td>
<td align="right">0.9817</td>
<td align="right">0.9916</td>
<td align="right">0.9964</td>
<td align="right">0.9986</td>
</tr>
</tbody>
</table></div>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:Student">Table 9.2: </span>Quantiles of the Student-<span class="math inline">\(t\)</span> distribution. The rows correspond to different degrees of freedom (<span class="math inline">\(\nu\)</span>, say); the columns correspond to different probabilities (<span class="math inline">\(z\)</span>, say). The cell gives <span class="math inline">\(q\)</span> that is s.t. <span class="math inline">\(\mathbb{P}(-q&lt;X&lt;q)=z\)</span>, with <span class="math inline">\(X \sim t(\nu)\)</span>.</caption>
<thead><tr class="header">
<th align="left"></th>
<th align="right">0.05</th>
<th align="right">0.1</th>
<th align="right">0.75</th>
<th align="right">0.9</th>
<th align="right">0.95</th>
<th align="right">0.975</th>
<th align="right">0.99</th>
<th align="right">0.999</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">1</td>
<td align="right">0.079</td>
<td align="right">0.158</td>
<td align="right">2.414</td>
<td align="right">6.314</td>
<td align="right">12.706</td>
<td align="right">25.452</td>
<td align="right">63.657</td>
<td align="right">636.619</td>
</tr>
<tr class="even">
<td align="left">2</td>
<td align="right">0.071</td>
<td align="right">0.142</td>
<td align="right">1.604</td>
<td align="right">2.920</td>
<td align="right">4.303</td>
<td align="right">6.205</td>
<td align="right">9.925</td>
<td align="right">31.599</td>
</tr>
<tr class="odd">
<td align="left">3</td>
<td align="right">0.068</td>
<td align="right">0.137</td>
<td align="right">1.423</td>
<td align="right">2.353</td>
<td align="right">3.182</td>
<td align="right">4.177</td>
<td align="right">5.841</td>
<td align="right">12.924</td>
</tr>
<tr class="even">
<td align="left">4</td>
<td align="right">0.067</td>
<td align="right">0.134</td>
<td align="right">1.344</td>
<td align="right">2.132</td>
<td align="right">2.776</td>
<td align="right">3.495</td>
<td align="right">4.604</td>
<td align="right">8.610</td>
</tr>
<tr class="odd">
<td align="left">5</td>
<td align="right">0.066</td>
<td align="right">0.132</td>
<td align="right">1.301</td>
<td align="right">2.015</td>
<td align="right">2.571</td>
<td align="right">3.163</td>
<td align="right">4.032</td>
<td align="right">6.869</td>
</tr>
<tr class="even">
<td align="left">6</td>
<td align="right">0.065</td>
<td align="right">0.131</td>
<td align="right">1.273</td>
<td align="right">1.943</td>
<td align="right">2.447</td>
<td align="right">2.969</td>
<td align="right">3.707</td>
<td align="right">5.959</td>
</tr>
<tr class="odd">
<td align="left">7</td>
<td align="right">0.065</td>
<td align="right">0.130</td>
<td align="right">1.254</td>
<td align="right">1.895</td>
<td align="right">2.365</td>
<td align="right">2.841</td>
<td align="right">3.499</td>
<td align="right">5.408</td>
</tr>
<tr class="even">
<td align="left">8</td>
<td align="right">0.065</td>
<td align="right">0.130</td>
<td align="right">1.240</td>
<td align="right">1.860</td>
<td align="right">2.306</td>
<td align="right">2.752</td>
<td align="right">3.355</td>
<td align="right">5.041</td>
</tr>
<tr class="odd">
<td align="left">9</td>
<td align="right">0.064</td>
<td align="right">0.129</td>
<td align="right">1.230</td>
<td align="right">1.833</td>
<td align="right">2.262</td>
<td align="right">2.685</td>
<td align="right">3.250</td>
<td align="right">4.781</td>
</tr>
<tr class="even">
<td align="left">10</td>
<td align="right">0.064</td>
<td align="right">0.129</td>
<td align="right">1.221</td>
<td align="right">1.812</td>
<td align="right">2.228</td>
<td align="right">2.634</td>
<td align="right">3.169</td>
<td align="right">4.587</td>
</tr>
<tr class="odd">
<td align="left">20</td>
<td align="right">0.063</td>
<td align="right">0.127</td>
<td align="right">1.185</td>
<td align="right">1.725</td>
<td align="right">2.086</td>
<td align="right">2.423</td>
<td align="right">2.845</td>
<td align="right">3.850</td>
</tr>
<tr class="even">
<td align="left">30</td>
<td align="right">0.063</td>
<td align="right">0.127</td>
<td align="right">1.173</td>
<td align="right">1.697</td>
<td align="right">2.042</td>
<td align="right">2.360</td>
<td align="right">2.750</td>
<td align="right">3.646</td>
</tr>
<tr class="odd">
<td align="left">40</td>
<td align="right">0.063</td>
<td align="right">0.126</td>
<td align="right">1.167</td>
<td align="right">1.684</td>
<td align="right">2.021</td>
<td align="right">2.329</td>
<td align="right">2.704</td>
<td align="right">3.551</td>
</tr>
<tr class="even">
<td align="left">50</td>
<td align="right">0.063</td>
<td align="right">0.126</td>
<td align="right">1.164</td>
<td align="right">1.676</td>
<td align="right">2.009</td>
<td align="right">2.311</td>
<td align="right">2.678</td>
<td align="right">3.496</td>
</tr>
<tr class="odd">
<td align="left">60</td>
<td align="right">0.063</td>
<td align="right">0.126</td>
<td align="right">1.162</td>
<td align="right">1.671</td>
<td align="right">2.000</td>
<td align="right">2.299</td>
<td align="right">2.660</td>
<td align="right">3.460</td>
</tr>
<tr class="even">
<td align="left">70</td>
<td align="right">0.063</td>
<td align="right">0.126</td>
<td align="right">1.160</td>
<td align="right">1.667</td>
<td align="right">1.994</td>
<td align="right">2.291</td>
<td align="right">2.648</td>
<td align="right">3.435</td>
</tr>
<tr class="odd">
<td align="left">80</td>
<td align="right">0.063</td>
<td align="right">0.126</td>
<td align="right">1.159</td>
<td align="right">1.664</td>
<td align="right">1.990</td>
<td align="right">2.284</td>
<td align="right">2.639</td>
<td align="right">3.416</td>
</tr>
<tr class="even">
<td align="left">90</td>
<td align="right">0.063</td>
<td align="right">0.126</td>
<td align="right">1.158</td>
<td align="right">1.662</td>
<td align="right">1.987</td>
<td align="right">2.280</td>
<td align="right">2.632</td>
<td align="right">3.402</td>
</tr>
<tr class="odd">
<td align="left">100</td>
<td align="right">0.063</td>
<td align="right">0.126</td>
<td align="right">1.157</td>
<td align="right">1.660</td>
<td align="right">1.984</td>
<td align="right">2.276</td>
<td align="right">2.626</td>
<td align="right">3.390</td>
</tr>
<tr class="even">
<td align="left">200</td>
<td align="right">0.063</td>
<td align="right">0.126</td>
<td align="right">1.154</td>
<td align="right">1.653</td>
<td align="right">1.972</td>
<td align="right">2.258</td>
<td align="right">2.601</td>
<td align="right">3.340</td>
</tr>
<tr class="odd">
<td align="left">500</td>
<td align="right">0.063</td>
<td align="right">0.126</td>
<td align="right">1.152</td>
<td align="right">1.648</td>
<td align="right">1.965</td>
<td align="right">2.248</td>
<td align="right">2.586</td>
<td align="right">3.310</td>
</tr>
</tbody>
</table></div>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:Chi2">Table 9.3: </span>Quantiles of the <span class="math inline">\(\chi^2\)</span> distribution. The rows correspond to different degrees of freedom; the columns correspond to different probabilities.</caption>
<colgroup>
<col width="5%">
<col width="11%">
<col width="11%">
<col width="11%">
<col width="11%">
<col width="11%">
<col width="11%">
<col width="11%">
<col width="11%">
</colgroup>
<thead><tr class="header">
<th align="left"></th>
<th align="right">0.05</th>
<th align="right">0.1</th>
<th align="right">0.75</th>
<th align="right">0.9</th>
<th align="right">0.95</th>
<th align="right">0.975</th>
<th align="right">0.99</th>
<th align="right">0.999</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">1</td>
<td align="right">0.004</td>
<td align="right">0.016</td>
<td align="right">1.323</td>
<td align="right">2.706</td>
<td align="right">3.841</td>
<td align="right">5.024</td>
<td align="right">6.635</td>
<td align="right">10.828</td>
</tr>
<tr class="even">
<td align="left">2</td>
<td align="right">0.103</td>
<td align="right">0.211</td>
<td align="right">2.773</td>
<td align="right">4.605</td>
<td align="right">5.991</td>
<td align="right">7.378</td>
<td align="right">9.210</td>
<td align="right">13.816</td>
</tr>
<tr class="odd">
<td align="left">3</td>
<td align="right">0.352</td>
<td align="right">0.584</td>
<td align="right">4.108</td>
<td align="right">6.251</td>
<td align="right">7.815</td>
<td align="right">9.348</td>
<td align="right">11.345</td>
<td align="right">16.266</td>
</tr>
<tr class="even">
<td align="left">4</td>
<td align="right">0.711</td>
<td align="right">1.064</td>
<td align="right">5.385</td>
<td align="right">7.779</td>
<td align="right">9.488</td>
<td align="right">11.143</td>
<td align="right">13.277</td>
<td align="right">18.467</td>
</tr>
<tr class="odd">
<td align="left">5</td>
<td align="right">1.145</td>
<td align="right">1.610</td>
<td align="right">6.626</td>
<td align="right">9.236</td>
<td align="right">11.070</td>
<td align="right">12.833</td>
<td align="right">15.086</td>
<td align="right">20.515</td>
</tr>
<tr class="even">
<td align="left">6</td>
<td align="right">1.635</td>
<td align="right">2.204</td>
<td align="right">7.841</td>
<td align="right">10.645</td>
<td align="right">12.592</td>
<td align="right">14.449</td>
<td align="right">16.812</td>
<td align="right">22.458</td>
</tr>
<tr class="odd">
<td align="left">7</td>
<td align="right">2.167</td>
<td align="right">2.833</td>
<td align="right">9.037</td>
<td align="right">12.017</td>
<td align="right">14.067</td>
<td align="right">16.013</td>
<td align="right">18.475</td>
<td align="right">24.322</td>
</tr>
<tr class="even">
<td align="left">8</td>
<td align="right">2.733</td>
<td align="right">3.490</td>
<td align="right">10.219</td>
<td align="right">13.362</td>
<td align="right">15.507</td>
<td align="right">17.535</td>
<td align="right">20.090</td>
<td align="right">26.124</td>
</tr>
<tr class="odd">
<td align="left">9</td>
<td align="right">3.325</td>
<td align="right">4.168</td>
<td align="right">11.389</td>
<td align="right">14.684</td>
<td align="right">16.919</td>
<td align="right">19.023</td>
<td align="right">21.666</td>
<td align="right">27.877</td>
</tr>
<tr class="even">
<td align="left">10</td>
<td align="right">3.940</td>
<td align="right">4.865</td>
<td align="right">12.549</td>
<td align="right">15.987</td>
<td align="right">18.307</td>
<td align="right">20.483</td>
<td align="right">23.209</td>
<td align="right">29.588</td>
</tr>
<tr class="odd">
<td align="left">20</td>
<td align="right">10.851</td>
<td align="right">12.443</td>
<td align="right">23.828</td>
<td align="right">28.412</td>
<td align="right">31.410</td>
<td align="right">34.170</td>
<td align="right">37.566</td>
<td align="right">45.315</td>
</tr>
<tr class="even">
<td align="left">30</td>
<td align="right">18.493</td>
<td align="right">20.599</td>
<td align="right">34.800</td>
<td align="right">40.256</td>
<td align="right">43.773</td>
<td align="right">46.979</td>
<td align="right">50.892</td>
<td align="right">59.703</td>
</tr>
<tr class="odd">
<td align="left">40</td>
<td align="right">26.509</td>
<td align="right">29.051</td>
<td align="right">45.616</td>
<td align="right">51.805</td>
<td align="right">55.758</td>
<td align="right">59.342</td>
<td align="right">63.691</td>
<td align="right">73.402</td>
</tr>
<tr class="even">
<td align="left">50</td>
<td align="right">34.764</td>
<td align="right">37.689</td>
<td align="right">56.334</td>
<td align="right">63.167</td>
<td align="right">67.505</td>
<td align="right">71.420</td>
<td align="right">76.154</td>
<td align="right">86.661</td>
</tr>
<tr class="odd">
<td align="left">60</td>
<td align="right">43.188</td>
<td align="right">46.459</td>
<td align="right">66.981</td>
<td align="right">74.397</td>
<td align="right">79.082</td>
<td align="right">83.298</td>
<td align="right">88.379</td>
<td align="right">99.607</td>
</tr>
<tr class="even">
<td align="left">70</td>
<td align="right">51.739</td>
<td align="right">55.329</td>
<td align="right">77.577</td>
<td align="right">85.527</td>
<td align="right">90.531</td>
<td align="right">95.023</td>
<td align="right">100.425</td>
<td align="right">112.317</td>
</tr>
<tr class="odd">
<td align="left">80</td>
<td align="right">60.391</td>
<td align="right">64.278</td>
<td align="right">88.130</td>
<td align="right">96.578</td>
<td align="right">101.879</td>
<td align="right">106.629</td>
<td align="right">112.329</td>
<td align="right">124.839</td>
</tr>
<tr class="even">
<td align="left">90</td>
<td align="right">69.126</td>
<td align="right">73.291</td>
<td align="right">98.650</td>
<td align="right">107.565</td>
<td align="right">113.145</td>
<td align="right">118.136</td>
<td align="right">124.116</td>
<td align="right">137.208</td>
</tr>
<tr class="odd">
<td align="left">100</td>
<td align="right">77.929</td>
<td align="right">82.358</td>
<td align="right">109.141</td>
<td align="right">118.498</td>
<td align="right">124.342</td>
<td align="right">129.561</td>
<td align="right">135.807</td>
<td align="right">149.449</td>
</tr>
<tr class="even">
<td align="left">200</td>
<td align="right">168.279</td>
<td align="right">174.835</td>
<td align="right">213.102</td>
<td align="right">226.021</td>
<td align="right">233.994</td>
<td align="right">241.058</td>
<td align="right">249.445</td>
<td align="right">267.541</td>
</tr>
<tr class="odd">
<td align="left">500</td>
<td align="right">449.147</td>
<td align="right">459.926</td>
<td align="right">520.950</td>
<td align="right">540.930</td>
<td align="right">553.127</td>
<td align="right">563.852</td>
<td align="right">576.493</td>
<td align="right">603.446</td>
</tr>
</tbody>
</table></div>
<div class="inline-table"><table style="width:100%;" class="table table-sm">
<caption>
<span id="tab:Fstat">Table 9.4: </span>Quantiles of the <span class="math inline">\(\mathcal{F}\)</span> distribution. The columns and rows correspond to different degrees of freedom (resp. <span class="math inline">\(n_1\)</span> and <span class="math inline">\(n_2\)</span>). The different panels correspond to different probabilities (<span class="math inline">\(\alpha\)</span>) The corresponding cell gives <span class="math inline">\(z\)</span> that is s.t. <span class="math inline">\(\mathbb{P}(X \le z)=\alpha\)</span>, with <span class="math inline">\(X \sim \mathcal{F}(n_1,n_2)\)</span>.</caption>
<colgroup>
<col width="15%">
<col width="8%">
<col width="8%">
<col width="8%">
<col width="8%">
<col width="8%">
<col width="8%">
<col width="8%">
<col width="8%">
<col width="8%">
<col width="8%">
</colgroup>
<thead><tr class="header">
<th align="left"></th>
<th align="right">1</th>
<th align="right">2</th>
<th align="right">3</th>
<th align="right">4</th>
<th align="right">5</th>
<th align="right">6</th>
<th align="right">7</th>
<th align="right">8</th>
<th align="right">9</th>
<th align="right">10</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">alpha = 0.9</td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="even">
<td align="left">5</td>
<td align="right">4.060</td>
<td align="right">3.780</td>
<td align="right">3.619</td>
<td align="right">3.520</td>
<td align="right">3.453</td>
<td align="right">3.405</td>
<td align="right">3.368</td>
<td align="right">3.339</td>
<td align="right">3.316</td>
<td align="right">3.297</td>
</tr>
<tr class="odd">
<td align="left">10</td>
<td align="right">3.285</td>
<td align="right">2.924</td>
<td align="right">2.728</td>
<td align="right">2.605</td>
<td align="right">2.522</td>
<td align="right">2.461</td>
<td align="right">2.414</td>
<td align="right">2.377</td>
<td align="right">2.347</td>
<td align="right">2.323</td>
</tr>
<tr class="even">
<td align="left">15</td>
<td align="right">3.073</td>
<td align="right">2.695</td>
<td align="right">2.490</td>
<td align="right">2.361</td>
<td align="right">2.273</td>
<td align="right">2.208</td>
<td align="right">2.158</td>
<td align="right">2.119</td>
<td align="right">2.086</td>
<td align="right">2.059</td>
</tr>
<tr class="odd">
<td align="left">20</td>
<td align="right">2.975</td>
<td align="right">2.589</td>
<td align="right">2.380</td>
<td align="right">2.249</td>
<td align="right">2.158</td>
<td align="right">2.091</td>
<td align="right">2.040</td>
<td align="right">1.999</td>
<td align="right">1.965</td>
<td align="right">1.937</td>
</tr>
<tr class="even">
<td align="left">50</td>
<td align="right">2.809</td>
<td align="right">2.412</td>
<td align="right">2.197</td>
<td align="right">2.061</td>
<td align="right">1.966</td>
<td align="right">1.895</td>
<td align="right">1.840</td>
<td align="right">1.796</td>
<td align="right">1.760</td>
<td align="right">1.729</td>
</tr>
<tr class="odd">
<td align="left">100</td>
<td align="right">2.756</td>
<td align="right">2.356</td>
<td align="right">2.139</td>
<td align="right">2.002</td>
<td align="right">1.906</td>
<td align="right">1.834</td>
<td align="right">1.778</td>
<td align="right">1.732</td>
<td align="right">1.695</td>
<td align="right">1.663</td>
</tr>
<tr class="even">
<td align="left">500</td>
<td align="right">2.716</td>
<td align="right">2.313</td>
<td align="right">2.095</td>
<td align="right">1.956</td>
<td align="right">1.859</td>
<td align="right">1.786</td>
<td align="right">1.729</td>
<td align="right">1.683</td>
<td align="right">1.644</td>
<td align="right">1.612</td>
</tr>
<tr class="odd">
<td align="left">alpha = 0.95</td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="even">
<td align="left">5</td>
<td align="right">6.608</td>
<td align="right">5.786</td>
<td align="right">5.409</td>
<td align="right">5.192</td>
<td align="right">5.050</td>
<td align="right">4.950</td>
<td align="right">4.876</td>
<td align="right">4.818</td>
<td align="right">4.772</td>
<td align="right">4.735</td>
</tr>
<tr class="odd">
<td align="left">10</td>
<td align="right">4.965</td>
<td align="right">4.103</td>
<td align="right">3.708</td>
<td align="right">3.478</td>
<td align="right">3.326</td>
<td align="right">3.217</td>
<td align="right">3.135</td>
<td align="right">3.072</td>
<td align="right">3.020</td>
<td align="right">2.978</td>
</tr>
<tr class="even">
<td align="left">15</td>
<td align="right">4.543</td>
<td align="right">3.682</td>
<td align="right">3.287</td>
<td align="right">3.056</td>
<td align="right">2.901</td>
<td align="right">2.790</td>
<td align="right">2.707</td>
<td align="right">2.641</td>
<td align="right">2.588</td>
<td align="right">2.544</td>
</tr>
<tr class="odd">
<td align="left">20</td>
<td align="right">4.351</td>
<td align="right">3.493</td>
<td align="right">3.098</td>
<td align="right">2.866</td>
<td align="right">2.711</td>
<td align="right">2.599</td>
<td align="right">2.514</td>
<td align="right">2.447</td>
<td align="right">2.393</td>
<td align="right">2.348</td>
</tr>
<tr class="even">
<td align="left">50</td>
<td align="right">4.034</td>
<td align="right">3.183</td>
<td align="right">2.790</td>
<td align="right">2.557</td>
<td align="right">2.400</td>
<td align="right">2.286</td>
<td align="right">2.199</td>
<td align="right">2.130</td>
<td align="right">2.073</td>
<td align="right">2.026</td>
</tr>
<tr class="odd">
<td align="left">100</td>
<td align="right">3.936</td>
<td align="right">3.087</td>
<td align="right">2.696</td>
<td align="right">2.463</td>
<td align="right">2.305</td>
<td align="right">2.191</td>
<td align="right">2.103</td>
<td align="right">2.032</td>
<td align="right">1.975</td>
<td align="right">1.927</td>
</tr>
<tr class="even">
<td align="left">500</td>
<td align="right">3.860</td>
<td align="right">3.014</td>
<td align="right">2.623</td>
<td align="right">2.390</td>
<td align="right">2.232</td>
<td align="right">2.117</td>
<td align="right">2.028</td>
<td align="right">1.957</td>
<td align="right">1.899</td>
<td align="right">1.850</td>
</tr>
<tr class="odd">
<td align="left">alpha = 0.99</td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="even">
<td align="left">5</td>
<td align="right">16.258</td>
<td align="right">13.274</td>
<td align="right">12.060</td>
<td align="right">11.392</td>
<td align="right">10.967</td>
<td align="right">10.672</td>
<td align="right">10.456</td>
<td align="right">10.289</td>
<td align="right">10.158</td>
<td align="right">10.051</td>
</tr>
<tr class="odd">
<td align="left">10</td>
<td align="right">10.044</td>
<td align="right">7.559</td>
<td align="right">6.552</td>
<td align="right">5.994</td>
<td align="right">5.636</td>
<td align="right">5.386</td>
<td align="right">5.200</td>
<td align="right">5.057</td>
<td align="right">4.942</td>
<td align="right">4.849</td>
</tr>
<tr class="even">
<td align="left">15</td>
<td align="right">8.683</td>
<td align="right">6.359</td>
<td align="right">5.417</td>
<td align="right">4.893</td>
<td align="right">4.556</td>
<td align="right">4.318</td>
<td align="right">4.142</td>
<td align="right">4.004</td>
<td align="right">3.895</td>
<td align="right">3.805</td>
</tr>
<tr class="odd">
<td align="left">20</td>
<td align="right">8.096</td>
<td align="right">5.849</td>
<td align="right">4.938</td>
<td align="right">4.431</td>
<td align="right">4.103</td>
<td align="right">3.871</td>
<td align="right">3.699</td>
<td align="right">3.564</td>
<td align="right">3.457</td>
<td align="right">3.368</td>
</tr>
<tr class="even">
<td align="left">50</td>
<td align="right">7.171</td>
<td align="right">5.057</td>
<td align="right">4.199</td>
<td align="right">3.720</td>
<td align="right">3.408</td>
<td align="right">3.186</td>
<td align="right">3.020</td>
<td align="right">2.890</td>
<td align="right">2.785</td>
<td align="right">2.698</td>
</tr>
<tr class="odd">
<td align="left">100</td>
<td align="right">6.895</td>
<td align="right">4.824</td>
<td align="right">3.984</td>
<td align="right">3.513</td>
<td align="right">3.206</td>
<td align="right">2.988</td>
<td align="right">2.823</td>
<td align="right">2.694</td>
<td align="right">2.590</td>
<td align="right">2.503</td>
</tr>
<tr class="even">
<td align="left">500</td>
<td align="right">6.686</td>
<td align="right">4.648</td>
<td align="right">3.821</td>
<td align="right">3.357</td>
<td align="right">3.054</td>
<td align="right">2.838</td>
<td align="right">2.675</td>
<td align="right">2.547</td>
<td align="right">2.443</td>
<td align="right">2.356</td>
</tr>
</tbody>
</table></div>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="FAVAR.html"><span class="header-section-number">8</span> Factor-Augmented VAR</a></div>
<div class="next"><a href="references.html">References</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#append"><span class="header-section-number">9</span> Appendix</a></li>
<li><a class="nav-link" href="#definitions-and-statistical-results"><span class="header-section-number">9.1</span> Definitions and statistical results</a></li>
<li><a class="nav-link" href="#AppendixProof"><span class="header-section-number">9.2</span> Proofs</a></li>
<li><a class="nav-link" href="#statistical-tables"><span class="header-section-number">9.3</span> Statistical Tables</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>The Identification of Dynamic Structural Shocks</strong>" was written by Kenza Benhima and Jean-Paul Renne. It was last built on 2023-01-17.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
