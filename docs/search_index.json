[["index.html", "The Identification of Dynamic Structural Shocks The Identification of Dynamic Structural Shocks", " The Identification of Dynamic Structural Shocks Kenza Benhima and Jean-Paul Renne 2024-12-05 The Identification of Dynamic Structural Shocks The identification and estimation of dynamic responses to structural shocks is one of the principal goals of macroeconometrics. These responses correspond to the effect, over time, of an exogenous intervention that propagates through the economy, as modeled by a system of simultaneous equations. Over the last decades, several methodologies have been proposed so as to estimate these responses. The objective of this course, developed by Kenza Benhima and Jean-Paul Renne, is to provide an exhaustive view of these methodologies and to provide students with tools enabling them to implement them in various contexts. Codes associated with this course are part of the IdSS package (Identification of Structural Shocks), which is available on GitHub. To load a package from GitHub, you need to use function install_github from the devtools package: install.packages(&quot;devtools&quot;) # devtools allows to use &quot;install_github&quot; library(devtools) install_github(&quot;jrenne/IdSS&quot;) library(IdSS) Useful (R) links: Download R: R software: https://cran.r-project.org (the basic R software) RStudio: https://www.rstudio.com (a convenient R editor) Tutorials: Rstudio: https://dss.princeton.edu/training/RStudio101.pdf (by Oscar Torres-Reyna) R: https://cran.r-project.org/doc/contrib/Paradis-rdebuts_en.pdf (by Emmanuel Paradis) My own tutorial: https://jrenne.shinyapps.io/Rtuto_publiShiny/ "],["basics.html", "Chapter 1 VARs and IRFs: the basics 1.1 Definition of VARs (and SVARMA) models 1.2 IRFs in SVARMA 1.3 Covariance-stationary VARMA models 1.4 VAR estimation 1.5 Block exogeneity and Granger causality", " Chapter 1 VARs and IRFs: the basics Often, impulse response functions (IRFs) are generated in the context of vectorial autoregressive (VAR) models. This section presents these models and show how they can be used to compute IRFs. 1.1 Definition of VARs (and SVARMA) models Definition 1.1 ((S)VAR model) Let \\(y_{t}\\) denote a \\(n \\times1\\) vector of (endogenous) random variables. Process \\(y_{t}\\) follows a \\(p^{th}\\)-order (S)VAR if, for all \\(t\\), we have \\[\\begin{eqnarray} \\begin{array}{rllll} VAR:&amp; y_t &amp;=&amp; c + \\Phi_1 y_{t-1} + \\dots + \\Phi_p y_{t-p} + \\varepsilon_t,\\\\ SVAR:&amp; y_t &amp;=&amp; c + \\Phi_1 y_{t-1} + \\dots + \\Phi_p y_{t-p} + B \\eta_t, \\end{array}\\tag{1.1} \\end{eqnarray}\\] with \\(\\varepsilon_t = B\\eta_t\\), where \\(\\{\\eta_{t}\\}\\) is a white noise sequence whose components are mutually and serially independent. The first line of Eq. (1.1) corresponds to the reduced-form of the VAR model (structural form for the second line). While the structural shocks (the components of \\(\\eta_t\\)) are mutually uncorrelated, this is not the case of the innovations, that are the components of \\(\\varepsilon_t\\). However, in boths cases, vectors \\(\\eta_t\\) and \\(\\varepsilon_t\\) are serially correlated (through time). As is the case for univariate models, VARs can be extended with MA terms in \\(\\eta_t\\), giving rise to VARMA models: Definition 1.2 ((S)VARMA model) Let \\(y_{t}\\) denote a \\(n \\times1\\) vector of random variables. Process \\(y_{t}\\) follows a VARMA model of order (p,q) if, for all \\(t\\), we have \\[\\begin{eqnarray} \\begin{array}{rllll} VARMA:&amp; y_t &amp;=&amp; c + \\Phi_1 y_{t-1} + \\dots + \\Phi_p y_{t-p} + \\\\ &amp;&amp;&amp;\\varepsilon_t + \\Theta_1\\varepsilon_{t-1} + \\dots + \\Theta_q \\varepsilon_{t-q},\\\\ SVARMA:&amp; y_t &amp;=&amp; c + \\Phi_1 y_{t-1} + \\dots + \\Phi_p y_{t-p} + \\\\ &amp;&amp;&amp; B_0 \\eta_t+ B_1 \\eta_{t-1} + \\dots + B_q \\eta_{t-q}, \\end{array}\\tag{1.2} \\end{eqnarray}\\] with \\(\\varepsilon_t = B_0\\eta_t\\), and \\(B_j = \\Theta_j B_0\\), for \\(j \\ge 0\\) (with \\(\\Theta_0=Id\\)), where \\(\\{\\eta_{t}\\}\\) is a white noise sequence whose components are are mutually and serially independent. 1.2 IRFs in SVARMA One of the main objectives of macro-econometrics is to derive IRFs, that represent the dynamic effects of structural shocks (components of \\(\\eta_t\\)) though the system of variables \\(y_t\\). Formally, an IRF is a difference in conditional expectations: \\[\\begin{equation} \\boxed{\\Psi_{i,j,h} = \\mathbb{E}(y_{i,t+h}|\\eta_{j,t}=1) - \\mathbb{E}(y_{i,t+h})}\\tag{1.3} \\end{equation}\\] (effect on \\(y_{i,t+h}\\) of a one-unit shock on \\(\\eta_{j,t}\\)). IRFs closely relate to the Wold decomposition of \\(y_t\\). Indeed, if the dynamics of process \\(y_t\\) can be described as a VARMA model, and if \\(y_t\\) is covariance stationary (see Def. 11.1), then \\(y_t\\) admits the following infinite MA representation (or Wold decomposition): \\[\\begin{equation} y_t = \\mu + \\sum_{h=0}^\\infty \\Psi_{h} \\eta_{t-h}.\\tag{1.4} \\end{equation}\\] With these notations, we get \\(\\mathbb{E}(y_{i,t+h}|\\eta_{j,t}=1) = \\mu_i + \\Psi_{i,j,h}\\), where \\(\\Psi_{i,j,h}\\) is the component \\((i,j)\\) of matrix \\(\\Psi_h\\) and \\(\\mu_i\\) is the \\(i^{th}\\) entry of vector \\(\\mu\\). Since we also have \\(\\mathbb{E}(y_{i,t+h})=\\mu_i\\), we obtain Eq. (1.3). Hence, estimating IRFs amounts to estimating the \\(\\Psi_{h}\\)’s. In general, there exist three main approaches for that: Calibrate and solve a (purely structural) Dynamic Stochastic General Equilibrium (DSGE) model at the first order (linearization). The solution takes the form of Eq. (1.4). Directly estimate the \\(\\Psi_{h}\\) based on projection approaches (see Section 8). Approximate the infinite MA representation by estimating a parsimonious type of model, e.g. VAR(MA) models (see Section 1.4). Once a (Structural) VARMA representation is obtained, Eq. (1.4) is easily deduced using the following proposition: Proposition 1.1 (IRF of an ARMA(p,q) process) If \\(y_t\\) follows the VARMA model described in Def. 1.2, then the matrices \\(\\Psi_h\\) appearing in Eq. (1.4) can be computed recursively as follows: Set \\(\\Psi_{-1}=\\dots=\\Psi_{-p}=0\\). For \\(h \\ge 0\\), (recursively) apply: \\[ \\Psi_h = \\Phi_1 \\Psi_{h-1} + \\dots + \\Phi_p \\Psi_{h-p} + \\Theta_h B_0, \\] with \\(\\Theta_0 = Id\\) and \\(\\Theta_h = 0\\) for \\(h&gt;q\\). Proof. This is obtained by applying the operator \\(\\frac{\\partial}{\\partial \\eta_{t}}\\) on both sides of Eq. (1.2). Typically, consider the VAR(2) case. The first steps of the algorithm mentioned in the last bullet point are as follows: \\[\\begin{eqnarray*} y_t &amp;=&amp; \\Phi_1 {\\color{blue}y_{t-1}} + \\Phi_2 y_{t-2} + B \\eta_t \\\\ &amp;=&amp; \\Phi_1 \\color{blue}{(\\Phi_1 y_{t-2} + \\Phi_2 y_{t-3} + B \\eta_{t-1})} + \\Phi_2 y_{t-2} + B \\eta_t \\\\ &amp;=&amp; B \\eta_t + \\Phi_1 B \\eta_{t-1} + (\\Phi_2 + \\Phi_1^2) \\color{red}{y_{t-2}} + \\Phi_1\\Phi_2 y_{t-3} \\\\ &amp;=&amp; B \\eta_t + \\Phi_1 B \\eta_{t-1} + (\\Phi_2 + \\Phi_1^2) \\color{red}{(\\Phi_1 y_{t-3} + \\Phi_2 y_{t-4} + B \\eta_{t-2})} + \\Phi_1\\Phi_2 y_{t-3} \\\\ &amp;=&amp; \\underbrace{B}_{=\\Psi_0} \\eta_t + \\underbrace{\\Phi_1 B}_{=\\Psi_1} \\eta_{t-1} + \\underbrace{(\\Phi_2 + \\Phi_1^2)B}_{=\\Psi_2} \\eta_{t-2} + f(y_{t-3},y_{t-4}). \\end{eqnarray*}\\] In particular, we have \\(B = \\Psi_0\\). Matrix \\(B\\) indeed captures the contemporaneous impact of \\(\\eta_t\\) on \\(y_t\\). That is why matrix \\(B\\) is sometimes called impulse matrix. Example 1.1 (IRFs of an SVARMA model) Consider the following VARMA(1,1) model: \\[\\begin{eqnarray} \\quad y_t &amp;=&amp; \\underbrace{\\left[\\begin{array}{cc} 0.5 &amp; 0.3 \\\\ -0.4 &amp; 0.7 \\end{array}\\right]}_{\\Phi_1} y_{t-1} + \\underbrace{\\left[\\begin{array}{cc} 1 &amp; 2 \\\\ -1 &amp; 1 \\end{array}\\right]}_{B}\\eta_t \\\\ &amp;&amp; - \\underbrace{\\left[\\begin{array}{cc} -0.4 &amp; 0 \\\\ 1 &amp; 0.5 \\end{array}\\right]}_{\\Theta_1} \\underbrace{\\left[\\begin{array}{cc} 1 &amp; 2 \\\\ -1 &amp; 1 \\end{array}\\right]}_{B}\\eta_{t-1}.\\tag{1.5} \\end{eqnarray}\\] We can use function simul.VARMA of package IdSS to produce IRFs (using indic.IRF=1 in the list of arguments): library(IdSS) distri &lt;- list(type=c(&quot;gaussian&quot;,&quot;gaussian&quot;),df=c(4,4)) n &lt;- length(distri$type) # dimension of y_t nb.sim &lt;- 30 eps &lt;- simul.distri(distri,nb.sim) Phi &lt;- array(NaN,c(n,n,1)) Phi[,,1] &lt;- matrix(c(.5,-.4,.3,.7),2,2) p &lt;- dim(Phi)[3] Theta &lt;- array(NaN,c(n,n,1)) Theta[,,1] &lt;- matrix(c(-.4,1,0,.5),2,2) q &lt;- dim(Theta)[3] Mu &lt;- rep(0,n) C &lt;- matrix(c(1,-1,2,1),2,2) Model &lt;- list( Mu = Mu,Phi = Phi,Theta = Theta,C = C,distri = distri) Y0 &lt;- rep(0,n) eta0 &lt;- c(1,0) res.sim.1 &lt;- simul.VARMA(Model,nb.sim,Y0,eta0,indic.IRF=1) eta0 &lt;- c(0,1) res.sim.2 &lt;- simul.VARMA(Model,nb.sim,Y0,eta0,indic.IRF=1) par(plt=c(.15,.95,.25,.8)) par(mfrow=c(2,2)) for(i in 1:2){ if(i == 1){res.sim &lt;- res.sim.1 }else{res.sim &lt;- res.sim.2} for(j in 1:2){ plot(res.sim$Y[j,],las=1, type=&quot;l&quot;,lwd=3,xlab=&quot;&quot;,ylab=&quot;&quot;, main=paste(&quot;Resp. of y&quot;,j, &quot; to a 1-unit increase in eta&quot;,i,sep=&quot;&quot;)) abline(h=0,col=&quot;grey&quot;,lty=3) }} Figure 1.1: Impulse response functions (SVARMA(1,1) specified above). 1.3 Covariance-stationary VARMA models Let’s come back to the infinite MA case (Eq. (1.4)): \\[ y_t = \\mu + \\sum_{h=0}^\\infty \\Psi_{h} \\eta_{t-h}. \\] For \\(y_t\\) to be covariance-stationary (and ergodic for the mean), it has to be the case that \\[\\begin{equation} \\sum_{i=0}^\\infty \\|\\Psi_i\\| &lt; \\infty,\\tag{1.6} \\end{equation}\\] where \\(\\|A\\|\\) denotes a norm of the matrix \\(A\\) (e.g. \\(\\|A\\|=\\sqrt{tr(AA&#39;)}\\)). This notably implies that if \\(y_t\\) is stationary (and ergodic for the mean), then \\(\\|\\Psi_h\\|\\rightarrow 0\\) when \\(h\\) gets large. What should be satisfied by \\(\\Phi_k\\)’s and \\(\\Theta_k\\)’s for a VARMA-based process (Eq. (1.2)) to be stationary? The conditions will be similar to that we have in the univariate case. Let us introduce the following notations: \\[\\begin{eqnarray} y_t &amp;=&amp; c + \\underbrace{\\Phi_1 y_{t-1} + \\dots +\\Phi_p y_{t-p}}_{\\color{blue}{\\mbox{AR component}}} + \\tag{1.7}\\\\ &amp;&amp;\\underbrace{B \\eta_t - \\Theta_1 B \\eta_{t-1} - \\dots - \\Theta_q B \\eta_{t-q}}_{\\color{red}{\\mbox{MA component}}} \\nonumber\\\\ &amp;\\Leftrightarrow&amp; \\underbrace{ \\color{blue}{(I - \\Phi_1 L - \\dots - \\Phi_p L^p)}}_{= \\color{blue}{\\Phi(L)}}y_t = c + \\underbrace{ \\color{red}{(I + \\Theta_1 L + \\ldots + \\Theta_q L^q)}}_{=\\color{red}{\\Theta(L)}} B \\eta_{t}. \\nonumber \\end{eqnarray}\\] Process \\(y_t\\) is stationary iff the roots of \\(\\det(\\Phi(z))=0\\) are strictly outside the unit circle or, equivalently, iff the eigenvalues of \\[\\begin{equation} \\Phi = \\left[\\begin{array}{cccc} \\Phi_{1} &amp; \\Phi_{2} &amp; \\cdots &amp; \\Phi_{p}\\\\ I &amp; 0 &amp; \\cdots &amp; 0\\\\ 0 &amp; \\ddots &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; I &amp; 0\\end{array}\\right]\\tag{1.8} \\end{equation}\\] lie strictly within the unit circle. Hence, as is the case for univariate processes, the covariance-stationarity of a VARMA model depends only on the specification of its AR part. Let’s derive the first two unconditional moments of a (covariance-stationary) VARMA process. Eq. (1.7) gives \\(\\mathbb{E}(\\Phi(L)y_t)=c\\), therefore \\(\\Phi(1)\\mathbb{E}(y_t)=c\\), or \\[ \\mathbb{E}(y_t) = (I - \\Phi_1 - \\dots - \\Phi_p)^{-1}c. \\] The autocovariances of \\(y_t\\) can be deduced from the infinite MA representation (Eq. (1.4)). We have: \\[ \\gamma_j \\equiv \\mathbb{C}ov(y_t,y_{t-j}) = \\sum_{i=j}^\\infty \\Psi_i \\Psi_{i-j}&#39;. \\] (This infinite sum exists as soon as Eq. (1.6) is satisfied.) Conditional means and autocovariances can also be deduced from Eq. (1.4). For \\(0 \\le h\\) and \\(0 \\le h_1 \\le h_2\\): \\[\\begin{eqnarray*} \\mathbb{E}_t(y_{t+h}) &amp;=&amp; \\mu + \\sum_{k=0}^\\infty \\Psi_{k+h} \\eta_{t-k} \\\\ \\mathbb{C}ov_t(y_{t+1+h_1},y_{t+1+h_2}) &amp;=&amp; \\sum_{k=0}^{h_1} \\Psi_{k}\\Psi_{k+h_2-h_1}&#39;. \\end{eqnarray*}\\] The previous formula implies in particular that the forecasting error \\(y_{t+h} - \\mathbb{E}_t(y_{t+h})\\) has a variance equal to: \\[ \\mathbb{V}ar_t(y_{t+1+h}) = \\sum_{k=0}^{h} \\Psi_{k}\\Psi_{k}&#39;. \\] Because the \\(\\eta_t\\) are mutually and serially independent (and therefore uncorrelated), we have: \\[ \\mathbb{V}ar(\\Psi_k \\eta_{t-k}) = \\mathbb{V}ar\\left(\\sum_{i=1}^n \\psi_{k,i} \\eta_{i,t-k}\\right) = \\sum_{i=1}^n \\psi_{k,i}\\psi_{k,i}&#39;, \\] where \\(\\psi_{k,i}\\) denotes the \\(i^{th}\\) column of \\(\\Psi_k\\). This suggests the following decomposition of the variance of the forecast error (called variance decomposition): \\[ \\mathbb{V}ar_t(y_{t+1+h}) = \\sum_{i=1}^n \\underbrace{\\sum_{k=0}^{h}\\psi_{k,i}\\psi_{k,i}&#39;.}_{\\mbox{Contribution of $\\eta_{i,t}$}} \\] Let us now turn to the estimation of VAR models. Note that if there is an MA component (i.e., if we consider a VARMA model), then OLS regressions yield biased estimates (even for asymptotically large samples). Assume for instance that \\(y_t\\) follows a VARMA(1,1) model: \\[ y_{i,t} = \\phi_i y_{t-1} + \\varepsilon_{i,t}, \\] where \\(\\phi_i\\) is the \\(i^{th}\\) row of \\(\\Phi_1\\), and where \\(\\varepsilon_{i,t}\\) is a linear combination of \\(\\eta_t\\) and \\(\\eta_{t-1}\\). Since \\(y_{t-1}\\) (the regressor) is correlated to \\(\\eta_{t-1}\\), it is also correlated to \\(\\varepsilon_{i,t}\\). The OLS regression of \\(y_{i,t}\\) on \\(y_{t-1}\\) yields a biased estimator of \\(\\phi_i\\) (see Figure 1.2). Hence, SVARMA models cannot be consistently estimated by simple OLS regressions (contrary to VAR models, as we will see in the next section); instrumental-variable approaches can be employed to estimate SVARMA models (using past values of \\(y_t\\) as instruments, see, e.g., Gouriéroux, Monfort, and Renne (2020)). N &lt;- 1000 # number of replications T &lt;- 100 # sample length phi &lt;- .8 # autoregressive parameter sigma &lt;- 1 par(mfrow=c(1,2)) for(theta in c(0,-0.4)){ all.y &lt;- matrix(0,1,N) y &lt;- all.y eta_1 &lt;- rnorm(N) for(t in 1:(T+1)){ eta &lt;- rnorm(N) y &lt;- phi * y + sigma * eta + theta * sigma * eta_1 all.y &lt;- rbind(all.y,y) eta_1 &lt;- eta } all.y_1 &lt;- all.y[1:T,] all.y &lt;- all.y[2:(T+1),] XX_1 &lt;- 1/apply(all.y_1 * all.y_1,2,sum) XY &lt;- apply(all.y_1 * all.y,2,sum) phi.est.OLS &lt;- XX_1 * XY plot(density(phi.est.OLS),xlab=&quot;OLS estimate of phi&quot;,ylab=&quot;&quot;, main=paste(&quot;theta = &quot;,theta,sep=&quot;&quot;)) abline(v=phi,col=&quot;red&quot;,lwd=2)} Figure 1.2: Illustration of the bias obtained when estimating the auto-regressive parameters of an ARMA process by (standard) OLS. 1.4 VAR estimation This section discusses the estimation of VAR models. Eq. (1.1) can be written: \\[ y_{t}=c+\\Phi(L)y_{t-1}+\\varepsilon_{t}, \\] with \\(\\Phi(L) = \\Phi_1 + \\Phi_2 L + \\dots + \\Phi_p L^{p-1}\\). Consequently: \\[ y_{t}\\mid y_{t-1},y_{t-2},\\ldots,y_{-p+1}\\sim \\mathcal{N}(c+\\Phi_{1}y_{t-1}+\\ldots\\Phi_{p}y_{t-p},\\Omega). \\] Using Hamilton (1994)’s notations, denote with \\(\\Pi\\) the matrix \\(\\left[\\begin{array}{ccccc} c &amp; \\Phi_{1} &amp; \\Phi_{2} &amp; \\ldots &amp; \\Phi_{p}\\end{array}\\right]&#39;\\) and with \\(x_{t}\\) the vector \\(\\left[\\begin{array}{ccccc} 1 &amp; y&#39;_{t-1} &amp; y&#39;_{t-2} &amp; \\ldots &amp; y&#39;_{t-p}\\end{array}\\right]&#39;\\), we have: \\[\\begin{equation} y_{t}= \\Pi&#39;x_{t} + \\varepsilon_{t}. \\tag{1.9} \\end{equation}\\] The previous representation is convenient to discuss the estimation of the VAR model, as parameters are gathered in two matrices only: \\(\\Pi\\) and \\(\\Omega\\). Let us start with the case where the shocks are Gaussian. Proposition 1.2 (MLE of a Gaussian VAR) If \\(y_t\\) follows a VAR(p) (see Definition 1.1), and if \\(\\varepsilon_t \\sim \\,i.i.d.\\,\\mathcal{N}(0,\\Omega)\\), then the ML estimate of \\(\\Pi\\), denoted by \\(\\hat{\\Pi}\\) (see Eq. (1.9)), is given by \\[\\begin{equation} \\hat{\\Pi}=\\left[\\sum_{t=1}^{T}x_{t}x&#39;_{t}\\right]^{-1}\\left[\\sum_{t=1}^{T}y_{t}&#39;x_{t}\\right]= (\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39;\\mathbf{y},\\tag{1.10} \\end{equation}\\] where \\(\\mathbf{X}\\) is the \\(T \\times (1+np)\\) matrix whose \\(t^{th}\\) row is \\(x_t\\) and where \\(\\mathbf{y}\\) is the \\(T \\times n\\) matrix whose \\(t^{th}\\) row is \\(y_{t}&#39;\\). That is, the \\(i^{th}\\) column of \\(\\hat{\\Pi}\\) (\\(b_i\\), say) is the OLS estimate of \\(\\beta_i\\), where: \\[\\begin{equation} y_{i,t} = \\beta_i&#39;x_t + \\varepsilon_{i,t},\\tag{1.11} \\end{equation}\\] (i.e., \\(\\beta_i&#39; = [c_i,\\phi_{i,1}&#39;,\\dots,\\phi_{i,p}&#39;]&#39;\\)). The ML estimate of \\(\\Omega\\), denoted by \\(\\hat{\\Omega}\\), coincides with the sample covariance matrix of the \\(n\\) series of the OLS residuals in Eq. (1.11), i.e.: \\[\\begin{equation} \\hat{\\Omega} = \\frac{1}{T} \\sum_{i=1}^T \\hat{\\varepsilon}_t\\hat{\\varepsilon}_t&#39;,\\quad\\mbox{with } \\hat{\\varepsilon}_t= y_t - \\hat{\\Pi}&#39;x_t. \\end{equation}\\] The asymptotic distributions of these estimators are the ones resulting from standard OLS formula. Proof. See Appendix 11.2. As stated by Proposition 1.3, when the shocks are not Gaussian, then the OLS regressions still provide consistent estimates of the model parameters. However, since \\(x_t\\) correlates to \\(\\varepsilon_s\\) for \\(s&lt;t\\), the OLS estimator \\(\\mathbf{b}_i\\) of \\(\\boldsymbol\\beta_i\\) is biased in small sample. (That is also the case for the ML estimator.) Indeed, denoting by \\(\\boldsymbol\\varepsilon_i\\) the \\(T \\times 1\\) vector of \\(\\varepsilon_{i,t}\\)’s, and using the notations of \\(b_i\\) and \\(\\beta_i\\) introduced in Proposition 1.2, we have: \\[\\begin{equation} \\mathbf{b}_i = \\beta_i + (\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39;\\boldsymbol\\varepsilon_i.\\tag{1.12} \\end{equation}\\] We have non-zero correlation between \\(x_t\\) and \\(\\varepsilon_{i,s}\\) for \\(s&lt;t\\) and, therefore, \\(\\mathbb{E}[(\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39;\\boldsymbol\\varepsilon_i] \\ne 0\\). However, when \\(y_t\\) is covariance stationary, then \\(\\frac{1}{n}\\mathbf{X}&#39;\\mathbf{X}\\) converges to a positive definite matrix \\(\\mathbf{Q}\\), and \\(\\frac{1}{n}X&#39;\\boldsymbol\\varepsilon_i\\) converges to 0. Hence \\(\\mathbf{b}_i \\overset{p}{\\rightarrow} \\beta_i\\). More precisely: Proposition 1.3 (Asymptotic distribution of the OLS estimate of VAR coefficients (for one variable)) If \\(y_t\\) follows a VAR model, as defined in Definition 1.1, we have: \\[ \\sqrt{T}(\\mathbf{b}_i-\\beta_i) = \\underbrace{\\left[\\frac{1}{T}\\sum_{t=p}^T x_t x_t&#39; \\right]^{-1}}_{\\overset{p}{\\rightarrow} \\mathbf{Q}^{-1}} \\underbrace{\\sqrt{T} \\left[\\frac{1}{T}\\sum_{t=1}^T x_t\\varepsilon_{i,t} \\right]}_{\\overset{d}{\\rightarrow} \\mathcal{N}(0,\\sigma_i^2\\mathbf{Q})}, \\] where \\(\\sigma_i = \\mathbb{V}ar(\\varepsilon_{i,t})\\) and where \\(\\mathbf{Q} = \\mbox{plim }\\frac{1}{T}\\sum_{t=p}^T x_t x_t&#39;\\) is given by: \\[\\begin{equation} \\mathbf{Q} = \\left[ \\begin{array}{ccccc} 1 &amp; \\mu&#39; &amp;\\mu&#39; &amp; \\dots &amp; \\mu&#39; \\\\ \\mu &amp; \\gamma_0 + \\mu\\mu&#39; &amp; \\gamma_1 + \\mu\\mu&#39; &amp; \\dots &amp; \\gamma_{p-1} + \\mu\\mu&#39;\\\\ \\mu &amp; \\gamma_1 + \\mu\\mu&#39; &amp; \\gamma_0 + \\mu\\mu&#39; &amp; \\dots &amp; \\gamma_{p-2} + \\mu\\mu&#39;\\\\ \\vdots &amp;\\vdots &amp;\\vdots &amp;\\dots &amp;\\vdots \\\\ \\mu &amp; \\gamma_{p-1} + \\mu\\mu&#39; &amp; \\gamma_{p-2} + \\mu\\mu&#39; &amp; \\dots &amp; \\gamma_{0} + \\mu\\mu&#39; \\end{array} \\right],\\tag{1.13} \\end{equation}\\] where \\(\\gamma_j\\) is the unconditional autocovariance matrix of order \\(j\\) of \\(y_t\\), i.e., \\(\\gamma_i = \\mathbb{C}ov(y_{t},y_{t-j})\\). Proof. See Appendix 11.2. The following proposition extends the previous proposition and includes covariances between different \\(\\beta_i\\)’s as well as the asymptotic distribution of the ML estimates of \\(\\Omega\\). Proposition 1.4 (Asymptotic distribution of the OLS estimates) If \\(y_t\\) follows a VAR model, as defined in Definition 1.1, we have: \\[\\begin{equation} \\sqrt{T}\\left[ \\begin{array}{c} vec(\\hat\\Pi - \\Pi)\\\\ vec(\\hat\\Omega - \\Omega) \\end{array} \\right] \\sim \\mathcal{N}\\left(0, \\left[ \\begin{array}{cc} \\Omega \\otimes \\mathbf{Q}^{-1} &amp; 0\\\\ 0 &amp; \\Sigma_{22} \\end{array} \\right]\\right),\\tag{1.14} \\end{equation}\\] where the component of \\(\\Sigma_{22}\\) corresponding to the covariance between \\(\\hat\\sigma_{i,j}\\) and \\(\\hat\\sigma_{k,l}\\) (for \\(i,j,l,m \\in \\{1,\\dots,n\\}^4\\)) is equal to \\(\\sigma_{i,l}\\sigma_{j,m}+\\sigma_{i,m}\\sigma_{j,l}\\). Proof. See Hamilton (1994), Appendix of Chapter 11. In practice, to use the previous proposition (for instance to implement Monte-Carlo simulations, see Section 3.1), \\(\\Omega\\) is replaced with \\(\\hat{\\Omega}\\), \\(\\mathbf{Q}\\) is replaced with \\(\\hat{\\mathbf{Q}} = \\frac{1}{T}\\sum_{t=p}^T x_t x_t&#39;\\) and \\(\\Sigma\\) with the matrix whose components are of the form \\(\\hat\\sigma_{i,l}\\hat\\sigma_{j,m}+\\hat\\sigma_{i,m}\\hat\\sigma_{j,l}\\), where the \\(\\hat\\sigma_{i,l}\\)’s are the components of \\(\\hat\\Omega\\). The simplicity of the VAR framework and the tractability of its MLE open the way to convenient econometric testing. Let’s illustrate this with the likelihood ratio test (see Def. 11.2). The maximum value achieved by the MLE is \\[ \\log\\mathcal{L}(Y_{T};\\hat{\\Pi},\\hat{\\Omega}) = -\\frac{Tn}{2}\\log(2\\pi)+\\frac{T}{2}\\log\\left|\\hat{\\Omega}^{-1}\\right| -\\frac{1}{2}\\sum_{t=1}^{T}\\left[\\hat{\\varepsilon}_{t}&#39;\\hat{\\Omega}^{-1}\\hat{\\varepsilon}_{t}\\right]. \\] The last term is: \\[\\begin{eqnarray*} \\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}&#39;\\hat{\\Omega}^{-1}\\hat{\\varepsilon}_{t} &amp;=&amp; \\mbox{Tr}\\left[\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}&#39;\\hat{\\Omega}^{-1}\\hat{\\varepsilon}_{t}\\right] = \\mbox{Tr}\\left[\\sum_{t=1}^{T}\\hat{\\Omega}^{-1}\\hat{\\varepsilon}_{t}\\hat{\\varepsilon}_{t}&#39;\\right]\\\\ &amp;=&amp;\\mbox{Tr}\\left[\\hat{\\Omega}^{-1}\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}\\hat{\\varepsilon}_{t}&#39;\\right] = \\mbox{Tr}\\left[\\hat{\\Omega}^{-1}\\left(T\\hat{\\Omega}\\right)\\right]=Tn. \\end{eqnarray*}\\] Therefore, the optimized log-likelihood is simply obtained by: \\[\\begin{equation} \\log\\mathcal{L}(Y_{T};\\hat{\\Pi},\\hat{\\Omega})=-(Tn/2)\\log(2\\pi)+(T/2)\\log\\left|\\hat{\\Omega}^{-1}\\right|-Tn/2.\\tag{1.15} \\end{equation}\\] Assume that we want to test the null hypothesis that a set of variables follows a VAR(\\(p_{0}\\)) against the alternative specification of \\(p_{1}\\) (\\(&gt;p_{0}\\)). Let us denote by \\(\\hat{L}_{0}\\) and \\(\\hat{L}_{1}\\) the maximum log-likelihoods obtained with \\(p_{0}\\) and \\(p_{1}\\) lags, respectively. Under the null hypothesis (\\(H_0\\): \\(p=p_0\\)), we have: \\[\\begin{eqnarray*} 2\\left(\\hat{L}_{1}-\\hat{L}_{0}\\right)&amp;=&amp;T\\left(\\log\\left|\\hat{\\Omega}_{1}^{-1}\\right|-\\log\\left|\\hat{\\Omega}_{0}^{-1}\\right|\\right) \\sim \\chi^2(n^{2}(p_{1}-p_{0})). \\end{eqnarray*}\\] What precedes can be used to help determine the appropriate number of lags to use in the specification. In a VAR, using too many lags consumes numerous degrees of freedom: with \\(p\\) lags, each of the \\(n\\) equations in the VAR contains \\(n\\times p\\) coefficients plus the intercept term. Adding lags improve in-sample fit, but is likely to result in over-parameterization and affect the out-of-sample prediction performance. To select appropriate lag length, selection criteria are often used. In the context of VAR models, using Eq. (1.15) (Gaussian case), we have for instance: \\[\\begin{eqnarray*} AIC &amp; = &amp; cst + \\log\\left|\\hat{\\Omega}\\right|+\\frac{2}{T}N\\\\ BIC &amp; = &amp; cst + \\log\\left|\\hat{\\Omega}\\right|+\\frac{\\log T}{T}N, \\end{eqnarray*}\\] where \\(N=p \\times n^{2}\\). library(vars);library(IdSS) ## Warning: package &#39;vars&#39; was built under R version 4.3.2 data &lt;- US3var[,c(&quot;y.gdp.gap&quot;,&quot;infl&quot;)] VARselect(data,lag.max = 6) ## $selection ## AIC(n) HQ(n) SC(n) FPE(n) ## 3 3 2 3 ## ## $criteria ## 1 2 3 4 5 6 ## AIC(n) -0.3394120 -0.4835525 -0.5328327 -0.5210835 -0.5141079 -0.49112812 ## HQ(n) -0.3017869 -0.4208439 -0.4450407 -0.4082080 -0.3761491 -0.32808581 ## SC(n) -0.2462608 -0.3283005 -0.3154798 -0.2416298 -0.1725534 -0.08747275 ## FPE(n) 0.7121914 0.6165990 0.5869659 0.5939325 0.5981364 0.61210908 estimated.var &lt;- VAR(data,p=3) #print(estimated.var$varresult) Phi &lt;- Acoef(estimated.var) PHI &lt;- make.PHI(Phi) # autoregressive matrix of companion form. print(abs(eigen(PHI)$values)) # check stationarity ## [1] 0.9114892 0.9114892 0.6319554 0.4759403 0.4759403 0.3246995 1.5 Block exogeneity and Granger causality 1.5.1 Block exogeneity Let’s decompose \\(y_t\\) into two subvectors \\(y^{(1)}_{t}\\) (\\(n_1 \\times 1\\)) and \\(y^{(2)}_{t}\\) (\\(n_2 \\times 1\\)), with \\(y_t&#39; = [{y^{(1)}_{t}}&#39;,{y^{(2)}_{t}}&#39;]\\) (and therefore \\(n=n_1 +n_2\\)), such that: \\[ \\left[ \\begin{array}{c} y^{(1)}_{t}\\\\ y^{(2)}_{t} \\end{array} \\right] = \\left[ \\begin{array}{cc} \\Phi^{(1,1)} &amp; \\Phi^{(1,2)}\\\\ \\Phi^{(2,1)} &amp; \\Phi^{(2,2)} \\end{array} \\right] \\left[ \\begin{array}{c} y^{(1)}_{t-1}\\\\ y^{(2)}_{t-1} \\end{array} \\right] + \\varepsilon_t. \\] Using, e.g., a likelihood ratio test (see Def. 11.2), one can easily test for block exogeneity of \\(y_t^{(2)}\\) (say). The null assumption can be expressed as \\(\\Phi^{(2,1)}=0\\). 1.5.2 Granger Causality Granger (1969) developed a method to explore causal relationships among variables. The approach consists in determining whether the past values of \\(y_{1,t}\\) can help explain the current \\(y_{2,t}\\) (beyond the information already included in the past values of \\(y_{2,t}\\)). Formally, let us denote three information sets: \\[\\begin{eqnarray*} \\mathcal{I}_{1,t} &amp; = &amp; \\left\\{ y_{1,t},y_{1,t-1},\\ldots\\right\\} \\\\ \\mathcal{I}_{2,t} &amp; = &amp; \\left\\{ y_{2,t},y_{2,t-1},\\ldots\\right\\} \\\\ \\mathcal{I}_{t} &amp; = &amp; \\left\\{ y_{1,t},y_{1,t-1},\\ldots y_{2,t},y_{2,t-1},\\ldots\\right\\}. \\end{eqnarray*}\\] We say that \\(y_{1,t}\\) Granger-causes \\(y_{2,t}\\) if \\[ \\mathbb{E}\\left[y_{2,t}\\mid \\mathcal{I}_{2,t-1}\\right]\\neq \\mathbb{E}\\left[y_{2,t}\\mid \\mathcal{I}_{t-1}\\right]. \\] To get the intuition behind the testing procedure, consider the following bivariate VAR(\\(p\\)) process: \\[\\begin{eqnarray*} y_{1,t} &amp; = &amp; c_1+\\Sigma_{i=1}^{p}\\Phi_i^{(11)}y_{1,t-i}+\\Sigma_{i=1}^{p}\\Phi_i^{(12)}y_{2,t-i}+\\varepsilon_{1,t}\\\\ y_{2,t} &amp; = &amp; c_2+\\Sigma_{i=1}^{p}\\Phi_i^{(21)}y_{1,t-i}+\\Sigma_{i=1}^{p}\\Phi_i^{(22)}y_{2,t-i}+\\varepsilon_{2,t}, \\end{eqnarray*}\\] where \\(\\Phi_k^{(ij)}\\) denotes the element \\((i,j)\\) of \\(\\Phi_k\\). Then, \\(y_{1,t}\\) is said not to Granger-cause \\(y_{2,t}\\) if \\[ \\Phi_1^{(21)}=\\Phi_2^{(21)}=\\ldots=\\Phi_p^{(21)}=0. \\] The null and alternative hypotheses therefore are: \\[ \\begin{cases} H_{0}: &amp; \\Phi_1^{(21)}=\\Phi_2^{(21)}=\\ldots=\\Phi_p^{(21)}=0\\\\ H_{1}: &amp; \\Phi_1^{(21)}\\neq0\\mbox{ or }\\Phi_2^{(21)}\\neq0\\mbox{ or}\\ldots\\Phi_p^{(21)}\\neq0.\\end{cases} \\] Loosely speaking, we reject \\(H_{0}\\) if some of the coefficients on the lagged \\(y_{1,t}\\)’s are statistically significant. Formally, this can be tested using the \\(F\\)-test or asymptotic chi-square test. The \\(F\\)-statistic is \\[ F=\\frac{(RSS-USS)/p}{USS/(T-2p-1)}, \\] where RSS is the Restricted sum of squared residuals and USS is the Unrestricted sum of squared residuals. Under \\(H_{0}\\), the \\(F\\)-statistic is distributed as \\(\\mathcal{F}(p,T-2p-1)\\) (See Table 11.4).1 According to the following lines of code, the output gap Granger-causes inflation, but the reverse is not true: grangertest(US3var[,c(&quot;y.gdp.gap&quot;,&quot;infl&quot;)],order=3) ## Granger causality test ## ## Model 1: infl ~ Lags(infl, 1:3) + Lags(y.gdp.gap, 1:3) ## Model 2: infl ~ Lags(infl, 1:3) ## Res.Df Df F Pr(&gt;F) ## 1 214 ## 2 217 -3 3.9761 0.008745 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 grangertest(US3var[,c(&quot;infl&quot;,&quot;y.gdp.gap&quot;)],order=3) ## Granger causality test ## ## Model 1: y.gdp.gap ~ Lags(y.gdp.gap, 1:3) + Lags(infl, 1:3) ## Model 2: y.gdp.gap ~ Lags(y.gdp.gap, 1:3) ## Res.Df Df F Pr(&gt;F) ## 1 214 ## 2 217 -3 1.5451 0.2038 References "],["identifStruct.html", "Chapter 2 Identification problem and standard identification techniques 2.1 The identification problem 2.2 A stylized example motivating short-run restrictions 2.3 Cholesky: a specific short-run-restriction situation 2.4 Long-run restrictions", " Chapter 2 Identification problem and standard identification techniques 2.1 The identification problem In Section 1.4, we have seen how to estimate \\(\\mathbb{V}ar(\\varepsilon_t) =\\Omega\\) and the \\(\\Phi_k\\) matrices in the context of a VAR model. But the IRFs are functions of \\(B\\) and of the \\(\\Phi_k\\)’s, not of \\(\\Omega\\) the \\(\\Phi_k\\)’s (see Section 1.2). We have \\(\\Omega = BB&#39;\\), which provides some restrictions on the components of \\(B\\), but this is not sufficient to fully identify \\(B\\). Indeed, seen a system of equations whose unknowns are the \\(b_{i,j}\\)’s (components of \\(B\\)), the system \\(\\Omega = BB&#39;\\) contains only \\(n(n+1)/2\\) linearly independent equations. For instance, for \\(n=2\\): \\[\\begin{eqnarray*} &amp;&amp;\\left[ \\begin{array}{cc} \\omega_{11} &amp; \\omega_{12} \\\\ \\omega_{12} &amp; \\omega_{22} \\end{array} \\right] = \\left[ \\begin{array}{cc} b_{11} &amp; b_{12} \\\\ b_{21} &amp; b_{22} \\end{array} \\right]\\left[ \\begin{array}{cc} b_{11} &amp; b_{21} \\\\ b_{12} &amp; b_{22} \\end{array} \\right]\\\\ &amp;\\Leftrightarrow&amp;\\left[ \\begin{array}{cc} \\omega_{11} &amp; \\omega_{12} \\\\ \\omega_{12} &amp; \\omega_{22} \\end{array} \\right] = \\left[ \\begin{array}{cc} b_{11}^2+b_{12}^2 &amp; \\color{red}{b_{11}b_{21}+b_{12}b_{22}} \\\\ \\color{red}{b_{11}b_{21}+b_{12}b_{22}} &amp; b_{22}^2 + b_{21}^2 \\end{array} \\right]. \\end{eqnarray*}\\] We then have 3 linearly independent equations but 4 unknowns. Therefore, \\(B\\) is not identified based on second-order moments. Additional restrictions are required to identify \\(B\\). This section covers two standard identification schemes: short-run and long-run restrictions. A short-run restriction (SRR) prevents a structural shock from affecting an endogenous variable contemporaneously. It is easy to implement: the appropriate entries of \\(B\\) are set to 0. A particular (popular) case is that of the Cholesky, or recursive approach. Examples include Bernanke (1986), Sims (1986), Galí (1992), Ruibio-Ramírez, Waggoner, and Zha (2010). A long-run restriction (LRR) prevents a structural shock from having a cumulative impact on one of the endogenous variables. Additional computations are required to implement this. One needs to compute the cumulative effect of one of the structural shocks \\(u_{t}\\) on one of the endogenous variable. Examples include Blanchard and Quah (1989), Faust and Leeper (1997), Galí (1999), Erceg, Guerrieri, and Gust (2005), Christiano, Eichenbaum, and Vigfusson (2007). The two approaches can be combined (see, e.g., Gerlach and Smets (1995)). 2.2 A stylized example motivating short-run restrictions Let us consider a simple example that could motivate short-run restrictions. Consider the following stylized macro model: \\[\\begin{equation} \\begin{array}{clll} g_{t}&amp;=&amp; \\bar{g}-\\lambda(i_{t-1}-\\mathbb{E}_{t-1}\\pi_{t})+ \\underbrace{{\\color{blue}\\sigma_d \\eta_{d,t}}}_{\\mbox{demand shock}}&amp; (\\mbox{IS curve})\\\\ \\Delta \\pi_{t} &amp; = &amp; \\beta (g_{t} - \\bar{g})+ \\underbrace{{\\color{blue}\\sigma_{\\pi} \\eta_{\\pi,t}}}_{\\mbox{cost push shock}} &amp; (\\mbox{Phillips curve})\\\\ i_{t} &amp; = &amp; \\rho i_{t-1} + \\left[ \\gamma_\\pi \\mathbb{E}_{t}\\pi_{t+1} + \\gamma_g (g_{t} - \\bar{g}) \\right]\\\\ &amp;&amp; \\qquad \\qquad+\\underbrace{{\\color{blue}\\sigma_{mp} \\eta_{mp,t}}}_{\\mbox{Mon. Pol. shock}} &amp; (\\mbox{Taylor rule}), \\end{array}\\tag{2.1} \\end{equation}\\] where: \\[\\begin{equation} \\eta_t = \\left[ \\begin{array}{c} \\eta_{\\pi,t}\\\\ \\eta_{d,t}\\\\ \\eta_{mp,t} \\end{array} \\right] \\sim i.i.d.\\,\\mathcal{N}(0,I).\\tag{2.2} \\end{equation}\\] Vector \\(\\eta_t\\) is assumed to be a vector of structural shocks, mutually and serially independent. On date \\(t\\): \\(g_t\\) is contemporaneously affected by \\(\\eta_{d,t}\\) only; \\(\\pi_t\\) is contemporaneously affected by \\(\\eta_{\\pi,t}\\) and \\(\\eta_{d,t}\\); \\(i_t\\) is contemporaneously affected by \\(\\eta_{mp,t}\\), \\(\\eta_{\\pi,t}\\) and \\(\\eta_{d,t}\\). System (2.1) could be rewritten as follows: \\[\\begin{equation} \\left[\\begin{array}{c} g_t\\\\ \\pi_t\\\\ i_t \\end{array}\\right] = \\Phi(L) \\left[\\begin{array}{c} g_{t-1}\\\\ \\pi_{t-1}\\\\ i_{t-1} + \\end{array}\\right] +\\underbrace{\\underbrace{ \\left[ \\begin{array}{ccc} 0 &amp; \\bullet &amp; 0 \\\\ \\bullet &amp; \\bullet &amp; 0 \\\\ \\bullet &amp; \\bullet &amp; \\bullet \\end{array} \\right]}_{=B} \\eta_t.}_{=\\varepsilon_t}\\tag{2.3} \\end{equation}\\] This is the reduced-form of the model. This representation suggests three additional restrictions on the entries of \\(B\\); the latter matrix is therefore identified as soon as \\(\\Omega = BB&#39;\\) is known (up to the signs of its columns). 2.3 Cholesky: a specific short-run-restriction situation There are particular cases in which some well-known matrix decomposition of \\(\\Omega=\\mathbb{V}ar(\\varepsilon_t)\\) can be used to easily estimate some specific SVAR. This is the case for the so-called Cholesky decomposition. Consider the following context: A first shock (say, \\(\\eta_{n_1,t}\\)) can affect instantaneously (i.e., on date \\(t\\)) only one of the endogenous variable (say, \\(y_{n_1,t}\\)); A second shock (say, \\(\\eta_{n_2,t}\\)) can affect instantaneously (i.e., on date \\(t\\)) two endogenous variables, \\(y_{n_1,t}\\) (the same as before) and \\(y_{n_2,t}\\); \\(\\dots\\) This implies that column \\(n_1\\) of \\(B\\) has only 1 non-zero entry (this is the \\(n_1^{th}\\) entry), that column \\(n_2\\) of \\(B\\) has 2 non-zero entries (the \\(n_1^{th}\\) and the \\(n_2^{th}\\) ones), etc. Without loss of generality, we can set \\(n_1=n\\), \\(n_2=n-1\\), etc. In this context, matrix \\(B\\) is lower triangular. The Cholesky decomposition of \\(\\Omega_{\\varepsilon}\\) then provides an appropriate estimate of \\(B\\), since this matrix decomposition yields to a lower triangular matrix satisfying: \\[ \\Omega_\\varepsilon = BB&#39;. \\] For instance, Dedola and Lippi (2005) estimate 5 structural VAR models for the US, the UK, Germany, France and Italy to analyse the monetary-policy transmission mechanisms. They estimate SVAR(5) models over the period 1975-1997. The shock-identification scheme is based on Cholesky decompositions, the ordering of the endogenous variables being: the industrial production, the consumer price index, a commodity price index, the short-term rate, monetary aggregate and the effective exchange rate (except for the US). This ordering implies that monetary policy reacts to the shocks affecting the first three variables but that the latter react to monetary policy shocks with a one-period lag only. The Cholesky approach can be employed when one is interested in one specific structural shock. This is the case, e.g., of Christiano, Eichenbaum, and Evans (1996). Their identification is based on the following relationship between \\(\\varepsilon_t\\) and \\(\\eta_t\\): \\[ \\left[\\begin{array}{c} \\boldsymbol\\varepsilon_{S,t}\\\\ \\varepsilon_{r,t}\\\\ \\boldsymbol\\varepsilon_{F,t} \\end{array}\\right] = \\left[\\begin{array}{ccc} B_{SS} &amp; 0 &amp; 0 \\\\ B_{rS} &amp; B_{rr} &amp; 0 \\\\ B_{FS} &amp; B_{Fr} &amp; B_{FF} \\end{array}\\right] \\left[\\begin{array}{c} \\boldsymbol\\eta_{S,t}\\\\ \\eta_{r,t}\\\\ \\boldsymbol\\eta_{F,t} \\end{array}\\right], \\] where \\(S\\), \\(r\\) and \\(F\\) respectively correspond to slow-moving variables, the policy variable (short-term rate) and fast-moving variables. While \\(\\eta_{r,t}\\) is scalar, \\(\\boldsymbol\\eta_{S,t}\\) and \\(\\boldsymbol\\eta_{F,t}\\) may be vectors. The space spanned by \\(\\boldsymbol\\varepsilon_{S,t}\\) is the same as that spanned by \\(\\boldsymbol\\eta_{S,t}\\). As a result, because \\(\\varepsilon_{r,t}\\) is a linear combination of \\(\\eta_{r,t}\\) and \\(\\boldsymbol\\eta_{S,t}\\) (which are \\(\\perp\\)), it comes that the \\(B_{rr}\\eta_{r,t}\\)’s are the (population) residuals in the regression of \\(\\varepsilon_{r,t}\\) on \\(\\boldsymbol\\varepsilon_{S,t}\\). Because \\(\\mathbb{V}ar(\\eta_{r,t})=1\\), \\(B_{rr}\\) is given by the square root of the variance of \\(B_{rr}\\eta_{r,t}\\). \\(B_{F,r}\\) is finally obtained by regressing the components of \\(\\boldsymbol\\varepsilon_{F,t}\\) on the estimates of \\(\\eta_{r,t}\\). An equivalent approach consists in computing the Cholesky decomposition of \\(BB&#39;\\) and the contemporaneous impacts of the monetary policy shock (on the \\(n\\) endogenous variables) are the components of the column of \\(B\\) corresponding to the policy variable. library(IdSS) library(vars) data(&quot;USmonthly&quot;) # Select sample period: First.date &lt;- &quot;1965-01-01&quot;;Last.date &lt;- &quot;1995-06-01&quot; indic.first &lt;- which(USmonthly$DATES==First.date) indic.last &lt;- which(USmonthly$DATES==Last.date) USmonthly &lt;- USmonthly[indic.first:indic.last,] considered.variables &lt;- c(&quot;LIP&quot;,&quot;UNEMP&quot;,&quot;LCPI&quot;,&quot;LPCOM&quot;,&quot;FFR&quot;,&quot;NBR&quot;,&quot;TTR&quot;,&quot;M1&quot;) y &lt;- as.matrix(USmonthly[considered.variables]) res.svar.ordering &lt;- svar.ordering(y,p=3, posit.of.shock = 5, nb.periods.IRF = 20, nb.bootstrap.replications = 100, confidence.interval = 0.90, # expressed in pp. indic.plot = 1 # Plots are displayed if = 1. ) Figure 2.1: Response to a monetary-policy shock. Identification approach of Christiano, Eichenbaum and Evans (1996). Confidence intervals are obtained by boostrapping the estimated VAR model (see inference section). 2.4 Long-run restrictions Let us now turn to Long-run restrictions. Such a restriction concerns the long-run influence of a shock on an endogenous variable. Let us consider for instance a structural shock that is assumed to have no “long-run influence” on GDP. How to express this? The long-run change in GDP can be expressed as \\(GDP_{t+h} - GDP_t\\), with \\(h\\) large. Note further that: \\[ GDP_{t+h} - GDP_t = \\Delta GDP_{t+h} +\\Delta GDP_{t+h-1} + \\dots + \\Delta GDP_{t+1}. \\] Hence, the fact that a given structural shock (\\(\\eta_{i,t}\\), say) has no long-run influence on GDP means that \\[ \\lim_{h\\rightarrow\\infty}\\frac{\\partial GDP_{t+h}}{\\partial \\eta_{i,t}} = \\lim_{h\\rightarrow\\infty} \\frac{\\partial}{\\partial \\eta_{i,t}}\\left(\\sum_{k=1}^h \\Delta GDP_{t+k}\\right)= 0. \\] This long-run effect can be formulated as a function of \\(B\\) and of the matrices \\(\\Phi_i\\) when \\(y_t\\) (including \\(\\Delta GDP_t\\)) follows a VAR process. Without loss of generality, we will only consider the VAR(1) case. Indeed, one can always write a VAR(\\(p\\)) as a VAR(1): for that, stack the last \\(p\\) values of vector \\(y_t\\) in vector \\(y_{t}^{*}=[y_t&#39;,\\dots,y_{t-p+1}&#39;]&#39;\\); Eq. (1.1) can then be rewritten in its companion form: \\[\\begin{equation} y_{t}^{*} = \\underbrace{\\left[\\begin{array}{c} c\\\\ 0\\\\ \\vdots\\\\ 0\\end{array}\\right]}_{=c^*}+ \\underbrace{\\left[\\begin{array}{cccc} \\Phi_{1} &amp; \\Phi_{2} &amp; \\cdots &amp; \\Phi_{p}\\\\ I &amp; 0 &amp; \\cdots &amp; 0\\\\ 0 &amp; \\ddots &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; I &amp; 0\\end{array}\\right]}_{=\\Phi} y_{t-1}^{*}+ \\underbrace{\\left[\\begin{array}{c} \\varepsilon_{t}\\\\ 0\\\\ \\vdots\\\\ 0\\end{array}\\right]}_{\\varepsilon_t^*},\\tag{2.4} \\end{equation}\\] where matrices \\(\\Phi\\) and \\(\\Omega^* = \\mathbb{V}ar(\\varepsilon_t^*)\\) are of dimension \\(np \\times np\\); \\(\\Omega^*\\) is filled with zeros, except the \\(n\\times n\\) upper-left block that is equal to \\(\\Omega = \\mathbb{V}ar(\\varepsilon_t)\\). (Matrix \\(\\Phi\\) had been introduced in Eq. (1.8).) Let us then focus on the VAR(1) case: \\[\\begin{eqnarray*} y_{t} &amp;=&amp; c+\\Phi y_{t-1}+\\varepsilon_{t}\\\\ &amp; = &amp; c+\\varepsilon_{t}+\\Phi(c+\\varepsilon_{t-1})+\\ldots+\\Phi^{k}(c+\\varepsilon_{t-k})+\\ldots \\\\ &amp; = &amp; \\mu +\\varepsilon_{t}+\\Phi\\varepsilon_{t-1}+\\ldots+\\Phi^{k}\\varepsilon_{t-k}+\\ldots \\\\ &amp; = &amp; \\mu +B\\eta_{t}+\\Phi B\\eta_{t-1}+\\ldots+\\Phi^{k}B\\eta_{t-k}+\\ldots, \\end{eqnarray*}\\] which is the Wold representation of \\(y_t\\). The sequence of shocks \\(\\{\\eta_t\\}\\) determines the sequence \\(\\{y_t\\}\\). What if \\(\\{\\eta_t\\}\\) is replaced with \\(\\{\\tilde{\\eta}_t\\}\\), where \\(\\tilde{\\eta}_t=\\eta_t\\) if \\(t \\ne s\\) and \\(\\tilde{\\eta}_s=\\eta_s + \\gamma\\)? Assume \\(\\{\\tilde{y}_t\\}\\) is the associated “perturbated” sequence. We have \\(\\tilde{y}_t = y_t\\) if \\(t&lt;s\\). For \\(t \\ge s\\), the Wold decomposition of \\(\\{\\tilde{y}_t\\}\\) implies: \\[ \\tilde{y}_t = y_t + \\Phi^{t-s} B \\gamma. \\] Therefore, the cumulative impact of \\(\\gamma\\) on \\(\\tilde{y}_t\\) will be (for \\(t \\ge s\\)): \\[\\begin{eqnarray} (\\tilde{y}_t - y_t) + (\\tilde{y}_{t-1} - y_{t-1}) + \\dots + (\\tilde{y}_s - y_s) &amp;=&amp; \\nonumber \\\\ (Id + \\Phi + \\Phi^2 + \\dots + \\Phi^{t-s}) B \\gamma.&amp;&amp; \\tag{2.5} \\end{eqnarray}\\] Consider a shock on \\(\\eta_{1,t}\\), with a magnitude of \\(1\\). This shock corresponds to \\(\\gamma = [1,0,\\dots,0]&#39;\\). Given Eq. (2.5), the long-run cumulative effect of this shock on the endogenous variables is given by: \\[ \\underbrace{(Id+\\Phi+\\ldots+\\Phi^{k}+\\ldots)}_{=(Id - \\Phi)^{-1}}B\\left[\\begin{array}{c} 1\\\\ 0\\\\ \\vdots\\\\ 0\\end{array}\\right], \\] that is the first column of the \\(n \\times n\\) matrix \\(\\Theta \\equiv (Id - \\Phi)^{-1}B\\). In this context, consider the following long-run restriction: “the \\(j^{th}\\) structural shock has no cumulative impact on the \\(i^{th}\\) endogenous variable”. It is equivalent to \\[ \\Theta_{ij}=0, \\] where \\(\\Theta_{ij}\\) is the element \\((i,j)\\) of \\(\\Theta\\). Blanchard and Quah (1989) have implemented such long-run restrictions in a small-scale VAR. Two variables are considered: GDP and unemployment. Consequently, the VAR is affected by two types of shocks. Specifically, authors want to identify supply shocks (that can have a permanent effect on output) and demand shocks (that cannot have a permanent effect on output).2 Blanchard and Quah (1989)’s dataset is quarterly, spanning the period from 1950:2 to 1987:4. Their VAR features 8 lags. Here are the data they use: library(IdSS) data(BQ) par(mfrow=c(1,2)) plot(BQ$Date,BQ$Dgdp,type=&quot;l&quot;,main=&quot;GDP quarterly growth rate&quot;, xlab=&quot;&quot;,ylab=&quot;&quot;,lwd=2) plot(BQ$Date,BQ$unemp,type=&quot;l&quot;,ylim=c(-3,6),main=&quot;Unemployment rate (gap)&quot;, xlab=&quot;&quot;,ylab=&quot;&quot;,lwd=2) Estimate a reduced-form VAR(8) model: library(vars) y &lt;- BQ[,2:3] est.VAR &lt;- VAR(y,p=8) Omega &lt;- var(residuals(est.VAR)) Now, let us define a loss function (loss) that is equal to zero if (a) \\(BB&#39;=\\Omega\\) and (b) the element (1,1) of \\(\\Theta = (Id - \\Phi)^{-1} B\\) is equal to zero: # Compute (Id - Phi)^{-1}: Phi &lt;- Acoef(est.VAR) PHI &lt;- make.PHI(Phi) sum.PHI.k &lt;- solve(diag(dim(PHI)[1]) - PHI)[1:2,1:2] loss &lt;- function(param){ B &lt;- matrix(param,2,2) X &lt;- Omega - B %*% t(B) Theta &lt;- sum.PHI.k[1:2,1:2] %*% B loss &lt;- 10000 * ( X[1,1]^2 + X[2,1]^2 + X[2,2]^2 + Theta[1,1]^2 ) return(loss) } res.opt &lt;- optim(c(1,0,0,1),loss,method=&quot;BFGS&quot;,hessian=FALSE) print(res.opt$par) ## [1] 0.8570358 -0.2396345 0.1541395 0.1921221 (Note: one can use that type of approach, based on a loss function, to mix short- and long-run restrictions.) Figure 2.2 displays the resulting IRFs. Note that, for GDP, we cumulate the GDP growth IRF, so as to have the response of the GDP in level. B.hat &lt;- matrix(res.opt$par,2,2) print(cbind(Omega,B.hat %*% t(B.hat))) ## Dgdp unemp ## Dgdp 0.7582704 -0.17576173 0.7582694 -0.17576173 ## unemp -0.1757617 0.09433658 -0.1757617 0.09433558 nb.sim &lt;- 40 par(mfrow=c(2,2));par(plt=c(.15,.95,.15,.8)) Y &lt;- simul.VAR(c=matrix(0,2,1),Phi,B.hat,nb.sim,y0.star=rep(0,2*8), indic.IRF = 1,u.shock = c(1,0)) plot(cumsum(Y[,1]),type=&quot;l&quot;,lwd=2,xlab=&quot;&quot;,ylab=&quot;&quot;,main=&quot;Demand shock on GDP&quot;) plot(Y[,2],type=&quot;l&quot;,lwd=2,xlab=&quot;&quot;,ylab=&quot;&quot;,main=&quot;Demand shock on UNEMP&quot;) Y &lt;- simul.VAR(c=matrix(0,2,1),Phi,B.hat,nb.sim,y0.star=rep(0,2*8), indic.IRF = 1,u.shock = c(0,1)) plot(cumsum(Y[,1]),type=&quot;l&quot;,lwd=2,xlab=&quot;&quot;,ylab=&quot;&quot;,main=&quot;Supply shock on GDP&quot;) plot(Y[,2],type=&quot;l&quot;,lwd=2,xlab=&quot;&quot;,ylab=&quot;&quot;,main=&quot;Supply shock on UNEMP&quot;) Figure 2.2: IRF of GDP and unemployment to demand and supply shocks. References "],["Inference.html", "Chapter 3 Inference 3.1 Monte Carlo method 3.2 Delta method 3.3 Bootstrap 3.4 Bootstrap-after-bootstrap", " Chapter 3 Inference Consider the following SVAR model: \\[y_t = \\Phi_1 y_{t-1} + \\dots + \\Phi_p y_{t-p} + \\varepsilon_t\\] with \\(\\varepsilon_t=B\\eta_t\\), \\(\\Omega_\\varepsilon=BB&#39;\\). The corresponding infinite MA representation (Eq. (1.4), or Wold representation, is: \\[ y_t = \\sum_{h=0}^\\infty\\Psi_h \\eta_{t-h}, \\] where \\(\\Psi_0=B\\) and for \\(h=1,2,\\dots\\): \\[ \\Psi_h = \\sum_{j=1}^h\\Psi_{h-j}\\Phi_j, \\] with \\(\\Phi_j=0\\) for \\(j&gt;p\\) (see Prop. 1.1 for this recursive computation of the \\(\\Psi_j\\)’s). Inference on the VAR coefficients \\(\\{\\Phi_j\\}_{j=1,...,p}\\) is straightforward (standard OLS inference). But inference is more complicated regarding IRFs. Indeed, as shown by the previous equation, the (infinite) MA coefficients \\(\\{\\Psi_j\\}_{j=1,...}\\) are non-linear functions of the \\(\\{\\Phi_j\\}_{j=1,...,p}\\) and of \\(\\Omega_\\varepsilon\\). An other issue pertain to small sample bias: typically, for persistent processes, auto-regressive parameters are known to be downward biased. The main inference methods are the following: Monte Carlo method (Hamilton (1994)) Asymptotic normal approximation (Lütkepohl (1990)), or Delta method Bootstrap method (Kilian (1998)) 3.1 Monte Carlo method We use Monte Carlo when we need to approximate the distribution of a variable whose distribution is unknown (here: the \\(\\Psi_j\\)’s) but which is a function of another variable whose distribution is known (here, the \\(\\Phi_j\\)’s). For instance, suppose we know the distribution of a random variable \\(X\\), which takes values in \\(\\mathbb{R}\\), with density function \\(p\\). Assume we want to compute the mean of \\(\\varphi(X)\\). We have: \\[ \\mathbb{E}(\\varphi(X))=\\int_{-\\infty}^{+\\infty}\\varphi(x)p(x)dx \\] Suppose that the above integral does not have a simple expression. We cannot compute \\(\\mathbb{E}(\\varphi(X))\\) but, by virtue of the law of large numbers, we can approximate it as follows: \\[ \\mathbb{E}(\\varphi(X))\\approx\\frac{1}{N}\\sum_{i=1}^N\\varphi(X^{(i)}), \\] where \\(\\{X^{(i)}\\}_{i=1,...,N}\\) are \\(N\\) independent draws of \\(X\\). More generally, the distribution of \\(\\varphi(X)\\) can be approximated by the empirical distribution of the \\(\\varphi(X^{(i)})\\)’s. Typically, if 10’000 values of \\(\\varphi(X^{(i)})\\) are drawn, the \\(5^{th}\\) percentile of the p.d.f. of \\(\\varphi(X)\\) can be approximated by the \\(500^{th}\\) value of the 10’000 draws of \\(\\varphi(X^{(i)})\\) (after arranging these values in ascending order). As regards the computation of confidence intervals around IRFs, one has to think of \\(\\{\\widehat{\\Phi}_j\\}_{j=1,...,p}\\), and of \\(\\widehat{\\Omega}\\) as \\(X\\) and \\(\\{\\widehat{\\Psi}_j\\}_{j=1,...}\\) as \\(\\varphi(X)\\). (Proposition 1.4 provides us with the asymptotic distribution of the “\\(X\\).”) To summarize, here are the steps one can implement to derive confidence intervals for the IRFs using the Monte-Carlo approach: For each iteration \\(k\\), Draw \\(\\{\\widehat{\\Phi}_j^{(k)}\\}_{j=1,...,p}\\) and \\(\\widehat{\\Omega}^{(k)}\\) from their asymptotic distribution (using Proposition 1.4). Compute the matrix \\(B^{(k)}\\) so that \\(\\widehat{\\Omega}^{(k)}=B^{(k)}B^{(k)&#39;}\\), according to your identification strategy. Compute the associated IRFs \\(\\{\\widehat{\\Psi}_j\\}^{(k)}\\). Perform \\(N\\) replications and report the median impulse response (and its confidence intervals). The following code implements the Monte Carlo method. library(IdSS);library(vars);library(Matrix) data(&quot;USmonthly&quot;) First.date &lt;- &quot;1965-01-01&quot; Last.date &lt;- &quot;1995-06-01&quot; indic.first &lt;- which(USmonthly$DATES==First.date) indic.last &lt;- which(USmonthly$DATES==Last.date) USmonthly &lt;- USmonthly[indic.first:indic.last,] considered.variables&lt;-c(&quot;LIP&quot;,&quot;UNEMP&quot;,&quot;LCPI&quot;,&quot;LPCOM&quot;,&quot;FFR&quot;,&quot;NBR&quot;,&quot;TTR&quot;,&quot;M1&quot;) y &lt;- as.matrix(USmonthly[considered.variables]) # =================================== # CEE with Monte Carlo # =================================== res.svar.ordering &lt;- svar.ordering.2(y,p=3, posit.of.shock = 5, nb.periods.IRF = 20, inference = 3,# 0 -&gt; no inference, 1 -&gt; parametric bootst., # 2 &lt;- non-parametric bootstrap, 3 &lt;- Monte Carlo, # 4 &lt;- bootstrap-after-bootstrap nb.draws = 200, confidence.interval = 0.90, # expressed in pp. indic.plot = 1 # Plots are displayed if = 1. ) Figure 3.1: IRF associated with a monetary policy shock; Monte Carlo method. IRFs.ordering &lt;- res.svar.ordering$IRFs median.IRFs.ordering &lt;- res.svar.ordering$all.CI.median simulated.IRFs.ordering &lt;- res.svar.ordering$simulated.IRFs Below we plot the estimated IRFs to a mom^netary policy shock, together with a sample of 50 IRFs simulated with the Monte Carlo method, and the median of all simulated IRFs. par(mfrow=c(1,1)) plot(IRFs.ordering[,5],type=&quot;l&quot;) for(i in 1:50){ lines(simulated.IRFs.ordering[,5,i],col=&quot;red&quot;,type=&quot;l&quot;) } lines(IRFs.ordering[,5],col=&quot;black&quot;,type=&quot;l&quot;) lines(median.IRFs.ordering[,5],col=&quot;blue&quot;,type=&quot;l&quot;) Figure 3.2: Estimated IRF associated with a monetary policy shock (black line) with simulated IRFs (red lines) and median IRF (blue line); Monte Carlo method. 3.2 Delta method Suppose \\(\\beta\\) is a vector of parameters and \\(\\beta\\) is an estimator such that \\[ \\sqrt{T}(\\hat\\beta-\\beta)\\overset{d}{\\rightarrow}\\mathcal{N}(0,\\Sigma_\\beta), \\] where \\(d\\) denotes convergence in distribution, \\(N(0,\\Sigma_\\beta)\\) denotes the multivariate normal distribution with mean vector 0 and covariance matrix \\(\\Sigma_\\beta\\) and \\(T\\) is the size of the sample used for estimation. Let \\(\\varphi(\\beta) = (\\varphi_l(\\beta),..., \\varphi_m(\\beta))&#39;\\) be a continuously differentiable function with values in \\(\\mathbb{R}^m\\), and assume that \\(\\partial \\varphi_i/\\partial \\beta&#39; = (\\partial \\varphi_i/\\partial \\beta_j)\\) is nonzero at \\(\\beta\\) for \\(i = 1,\\dots, m\\). Then \\[ \\sqrt{T}(\\varphi(\\hat\\beta)-\\varphi(\\beta))\\overset{d}{\\rightarrow}\\mathcal{N}\\left(0,\\frac{\\partial \\varphi}{\\partial \\beta&#39;}\\Sigma_\\beta\\frac{\\partial \\varphi&#39;}{\\partial \\beta}\\right). \\] Using this property, Lütkepohl (1990) provides the asymptotic distributions of the \\(\\Psi_j\\)’s. A limit of the last two approaches (Monte Carlo and the Delta method) is that they rely on asymptotic results and the normality assumption. Boostrapping approaches are more robust in small-sample and non-normal situations. 3.3 Bootstrap IRFs’ confidence intervals are intervals where 90% (or 95%, 75%, …) of the IRFs would lie, if we were to repeat the estimation a large number of times in similar conditions (\\(T\\) observations). We obviously cannot do this, because we have only one sample: \\(\\{y_t\\}_{t=1,..,T}\\). But we can try to construct such samples. Bootstrapping consists in: re-sampling \\(N\\) times, i.e., constructing \\(N\\) samples of \\(T\\) observations, using the estimated VAR coefficients and a sample of residuals from the distribution \\(N(0,BB&#39;)\\) (parametric approach), or a sample of residuals drawn randomly from the set of the actual estimated residuals \\(\\{\\hat\\varepsilon_t\\}_{t=1,..,T}\\). (non-parametric approach). re-estimating the SVAR \\(N\\) times. Here is the algorithm for the non-parametric approach: Construct a sample \\[ y_t^{(k)}=\\widehat{\\Phi}_1 y_{t-1}^{(k)} + \\dots + \\widehat{\\Phi}_p y_{t-p}^{(k)} + \\hat\\varepsilon_t^{(k)}, \\] with \\(\\hat\\varepsilon_{t}^{(k)}=\\hat\\varepsilon_{s_t^{(k)}}\\), where \\(\\{s_1^{(k)},..,s_T^{(k)}\\}\\) is a random set from \\(\\{1,..,T\\}^T\\). (Note: in the parametric approach, we would draw \\(\\hat\\varepsilon_{t}^{(k)}\\) from the \\(N(0,BB&#39;)\\) distribution) Re-estimate the SVAR and compute the IRFs \\(\\{\\widehat{\\Psi}_j\\}^{(k)}\\). Perform \\(N\\) replications and report the median impulse response (and its confidence intervals). The following code implements the bootstrap method. library(IdSS);library(vars);library(Matrix) data(&quot;USmonthly&quot;) First.date &lt;- &quot;1965-01-01&quot; Last.date &lt;- &quot;1995-06-01&quot; indic.first &lt;- which(USmonthly$DATES==First.date) indic.last &lt;- which(USmonthly$DATES==Last.date) USmonthly &lt;- USmonthly[indic.first:indic.last,] considered.variables&lt;-c(&quot;LIP&quot;,&quot;UNEMP&quot;,&quot;LCPI&quot;,&quot;LPCOM&quot;,&quot;FFR&quot;,&quot;NBR&quot;,&quot;TTR&quot;,&quot;M1&quot;) y &lt;- as.matrix(USmonthly[considered.variables]) # =================================== # CEE with bootstrap # =================================== res.svar.ordering &lt;- svar.ordering.2(y,p=3, posit.of.shock = 5, nb.periods.IRF = 20, inference = 2,# 0 -&gt; no inference, 1 -&gt; parametric bootstr., # 2 &lt;- non-parametric bootstrap, 3 &lt;- monte carlo, # 4 &lt;- bootstrap-after-bootstrap nb.draws = 200, confidence.interval = 0.90, # expressed in pp. indic.plot = 1 # Plots are displayed if = 1. ) Figure 3.3: IRF associated with a monetary policy shock; bootstrap method. IRFs.ordering.bootstrap &lt;- res.svar.ordering$IRFs median.IRFs.ordering.bootstrap &lt;- res.svar.ordering$all.CI.median simulated.IRFs.ordering.bootstrap &lt;- res.svar.ordering$simulated.IRFs 3.4 Bootstrap-after-bootstrap The previous simple bootstrapping procedure deals with non-normality and small sample distribution, since we use the actual residuals. However, it does not deal with the small sample bias, stemming, in particular, from small-sample bias associated with OLS coefficient estimates \\(\\{\\widehat{\\Phi}_j\\}_{j=1,..,p}\\). The code below illustrates the small sample bias. # Distribution of coefficients stemming from non-parametric bootstrap n &lt;- length(considered.variables) h &lt;- 5 par(mfrow=c(2,ifelse(round(n/2)==n/2,n/2,(n+1)/2))) for (i in 1:n){ hist(simulated.IRFs.ordering.bootstrap[h,i,],xlab=&quot;&quot;,ylab=&quot;&quot;, main=paste(&quot;Effect at h = &quot;,h,&quot; on &quot;, considered.variables[i],sep=&quot;&quot;),cex.main=.9) lines(array(c(IRFs.ordering.bootstrap[h,i], IRFs.ordering.bootstrap[h,i],0,100),c(2,2)),col=&quot;red&quot;) lines(array(c(median.IRFs.ordering.bootstrap[h,i], median.IRFs.ordering.bootstrap[h,i],0,100),c(2,2)),col=&quot;blue&quot;) text(IRFs.ordering.bootstrap[h,i],25,label=&quot;Estimated coef.&quot;,col=&quot;red&quot;) } Figure 3.4: Estimated and bootstrapped coefficients. The distribution of the bootstrapped coefficients is not centered around the estimated coefficients. In the following code, we perform the VAR estimation and bootstrap inference after generating artificial data. We can then compare the IRFs and confidence intervals to the ``true’’ parameters used to generate the data. # Simulate a small sample est.VAR &lt;- VAR(y,p=3) Phi &lt;- Acoef(est.VAR) cst &lt;- Bcoef(est.VAR)[,3*n+1] resids &lt;- residuals(est.VAR) Omega &lt;- var(resids) B.hat &lt;- t(chol(Omega)) y0.star &lt;- NULL for(k in 3:1){ y0.star &lt;- c(y0.star,y[k,]) } small.sample &lt;- simul.VAR(c=rep(0,dim(y)[2]), Phi, B.hat, nb.sim = 100, y0.star, indic.IRF = 0) colnames(small.sample) &lt;- considered.variables # Estimate the VAR with the small sample res.svar.small.sample &lt;- svar.ordering.2(small.sample,p=3, posit.of.shock = 5, nb.periods.IRF = 20, inference = 2,# 0 -&gt; no inference, 1 -&gt; parametric bootstr., # 2 &lt;- non-parametric bootstrap, 3 &lt;- monte carlo nb.draws = 200, confidence.interval = 0.90, # expressed in pp. indic.plot = 1 # Plots are displayed if = 1. ) Figure 3.5: Simulated IRF associated with a monetary policy shock. IRFs.small.sample &lt;- res.svar.small.sample$IRFs median.IRFs.small.sample &lt;- res.svar.small.sample$all.CI.median simulated.IRFs.small.sample &lt;- res.svar.small.sample$simulated.IRFs # True IRFs res.svar.ordering &lt;- svar.ordering.2(y,p=3, posit.of.shock = 5, nb.periods.IRF = 20, inference = 0,# 0 -&gt; no inference, 1 -&gt; parametric bootstr., # 2 &lt;- non-parametric bootstrap, 3 &lt;- monte carlo, # 4 &lt;- bootstrap-after-bootstrap indic.plot = 0 # Plots are displayed if = 1. ) IRFs.ordering.true &lt;- res.svar.ordering$IRFs # Distribution of coefficients resulting from the small sample VAR h &lt;- 5 par(mfrow=c(2,ifelse(round(n/2)==n/2,n/2,(n+1)/2))) for (i in 1:n){ hist(simulated.IRFs.small.sample[h,i,],xlab=&quot;&quot;,ylab=&quot;&quot;, main=paste(&quot;Effect at h = &quot;,h,&quot; on &quot;, considered.variables[i],sep=&quot;&quot;),cex.main=.9) lines(array(c(IRFs.small.sample[h,i], IRFs.small.sample[h,i],0,100),c(2,2)),col=&quot;red&quot;) lines(array(c(median.IRFs.small.sample[h,i], median.IRFs.small.sample[h,i],0,100),c(2,2)),col=&quot;blue&quot;) lines(array(c(IRFs.ordering.true[h,i], IRFs.ordering.true[h,i],0,100),c(2,2)),col=&quot;black&quot;) text(IRFs.small.sample[h,i],25,label=&quot;Estimated coef.&quot;,col=&quot;red&quot;) text(IRFs.ordering.true[h,i],30,label=&quot;True coef.&quot;,col=&quot;black&quot;) } Figure 3.6: IRF associated with a monetary policy shock; sign-restriction approach. The main idea of the bootstrap-after-bootstrap of Kilian (1998) is to run two consecutive boostraps: the objective of the first is to compute the bias, which can further be used to correct the initial estimates of the \\(\\Phi_i\\)’s. Further, these corrected estimates are used —in the second boostrap— to compute a set of IRFs (as in the standard boostrap). More formally, the algorithm is as follows: Estimate the SVAR coefficients \\(\\{\\widehat{\\Phi}_j\\}_{j=1,..,p}\\) and \\(\\widehat{\\Omega}\\) First bootstrap. For each iteration \\(k\\): Construct a sample \\[ y_t^{(k)}=\\widehat{\\Phi}_1 y_{t-1}^{(k)} + \\dots + \\widehat{\\Phi}_p y_{t-p}^{(k)} + \\hat\\varepsilon_t^{(k)}, \\] with \\(\\hat\\varepsilon_{t}^{(k)}=\\hat\\varepsilon_{s_t^{(k)}}\\), where \\(\\{s_1^{(k)},..,s_T^{(k)}\\}\\) is a random set from \\(\\{1,..,T\\}^T\\). Re-estimate the VAR and compute the coefficients \\(\\{\\widehat{\\Phi}_j\\}_{j=1,..,p}^{(k)}\\). Perform \\(N\\) replications and compute the median coefficients \\(\\{\\widehat{\\Phi}_j\\}_{j=1,..,p}^*\\). Approximate the bias terms by \\(\\widehat{\\Theta}_j=\\widehat{\\Phi}_j^*-\\widehat{\\Phi}_j\\). Construct the bias-corrected terms \\(\\widetilde{\\Phi}_j=\\widehat{\\Phi}_j-\\widehat{\\Theta}_j\\). Second bootstrap. For each iteration \\(k\\): Construct a sample now from \\[ y_t^{(k)}=\\widetilde{\\Phi}_1 y_{t-1}^{(k)} + \\dots + \\widetilde{\\Phi}_p y_{t-p}^{(k)} + \\hat\\varepsilon_t^{(k)}. \\] Re-estimate the VAR and compute the coefficients \\(\\{\\widehat{\\Phi}^*_j\\}_{j=1,..,p}^{(k)}\\). Construct the bias-corrected estimates \\(\\widetilde{\\Phi}_j^{*(k)}=\\widehat{\\Phi}_j^{*(k)}-\\widehat{\\Theta}_j\\). Compute the associated IRFs \\(\\{\\widetilde{\\Psi}_j^{*(k)}\\}_{j\\ge 1}\\). Perform \\(N\\) replications and compute the median and the confidence interval of the set of IRFs. It should be noted that correcting for the bias can generate non-stationary results (\\(\\tilde \\Phi\\) with eigenvalue with modulus \\(&gt;1\\)). Solution (Kilian (1998)): In step 5, check if the largest eigenvalue of \\(\\tilde\\Phi\\) is of modulus &lt;1. If not, shrink the bias: for all \\(j\\)s, set \\(\\widehat{\\Theta}_j^{(i+1)}=\\delta_{i+1}\\widehat{\\Theta}_j^{(i)}\\), with \\(\\delta_{i+1}=\\delta_i-0.01\\), starting with \\(\\delta_1=1\\) and \\(\\widehat{\\Theta}_j^{(1)} =\\widehat{\\Theta}_j\\), and compute \\(\\widetilde{\\Phi}_j^{(i+1)}=\\widehat{\\Phi}_j-\\widehat{\\Theta}_j^{(i+1)}\\) until the largest eigenvalue of \\(\\tilde\\Phi^{(i+1)}\\) has modulus &lt;1. The following code implements the bootstrap-after-bootrap method. library(IdSS);library(vars);library(Matrix) data(&quot;USmonthly&quot;) First.date &lt;- &quot;1965-01-01&quot; Last.date &lt;- &quot;1995-06-01&quot; indic.first &lt;- which(USmonthly$DATES==First.date) indic.last &lt;- which(USmonthly$DATES==Last.date) USmonthly &lt;- USmonthly[indic.first:indic.last,] considered.variables&lt;-c(&quot;LIP&quot;,&quot;UNEMP&quot;,&quot;LCPI&quot;,&quot;LPCOM&quot;,&quot;FFR&quot;,&quot;NBR&quot;,&quot;TTR&quot;,&quot;M1&quot;) y &lt;- as.matrix(USmonthly[considered.variables]) # =================================== # CEE with bootstrap-after-bootstrap # =================================== res.svar.ordering &lt;- svar.ordering.2(y,p=3, posit.of.shock = 5, nb.periods.IRF = 20, inference = 4,# 0 -&gt; no inference, 1 -&gt; parametric bootstr., # 2 &lt;- non-parametric bootstrap, 3 &lt;- monte carlo, # 4 &lt;- bootstrap-after-bootstrap nb.draws = 200, confidence.interval = 0.90, # expressed in pp. indic.plot = 1 # Plots are displayed if = 1. ) Figure 3.7: IRF associated with a monetary policy shock; sign-restriction approach. IRFs.ordering &lt;- res.svar.ordering$IRFs median.IRFs.ordering &lt;- res.svar.ordering$all.CI.median simulated.IRFs.ordering &lt;- res.svar.ordering$simulated.IRFs As an alternative, function VAR.Boot of package VAR.etp (Kim (2022)) can be used to operate the bias-correction approach of Kilian (1998): library(VAR.etp) library(vars) #standard VAR models data(dat) # part of VAR.etp package corrected &lt;- VAR.Boot(dat,p=2,nb=200,type=&quot;const&quot;) noncorrec &lt;- VAR(dat,p=2) rbind(corrected$coef[1,], (corrected$coef+corrected$Bias)[1,], noncorrec$varresult$inv$coefficients) ## inv(-1) inc(-1) con(-1) inv(-2) inc(-2) con(-2) const ## [1,] -0.3270068 0.0908887 1.107804 -0.1370713 0.168105 0.9770239 -0.02028674 ## [2,] -0.3196310 0.1459888 0.961219 -0.1605511 0.114605 0.9343938 -0.01672199 ## [3,] -0.3196310 0.1459888 0.961219 -0.1605511 0.114605 0.9343938 -0.01672199 References "],["Signs.html", "Chapter 4 Sign restrictions 4.1 The approach 4.2 An example 4.3 The penalty-function approach (PFA) 4.4 Narrative sign restrictions", " Chapter 4 Sign restrictions To identifiy the structural shocks, we need to find a matrix \\(B\\) that satisfies \\(\\Omega = BB&#39;\\) (with \\(\\Omega = \\mathbb{V}ar(\\varepsilon_t)\\)) and other restrictions. Indeed, as explained above, \\(\\Omega = BB&#39;\\) is not sufficient to identify \\(B\\) since, if we take any orthogonal matrix \\(Q\\) (see Def. 4.1), then \\(\\mathcal{P}=BQ\\) also satisfies \\(\\Omega = \\mathcal{P}\\mathcal{P}&#39;\\). Definition 4.1 (Orthogonal matrix) An orthogonal matrix \\(Q\\) is a matrix such that \\(QQ&#39; = I,\\) i.e., all columns (rows) of \\(Q\\) are are orthogonal and unit vectors: \\[q_i&#39;q_j=0\\text{ if }i\\neq j\\text{ and }q_i&#39;q_j=1\\text{ if }i= j,\\] where \\(q_i\\) is the \\(i^{th}\\) column of \\(Q\\). 4.1 The approach The idea behind the sign-restriction approach is to “draw” random matrices \\(\\mathcal{P}\\) that satisfy \\(\\Omega = \\mathcal{P}\\mathcal{P}&#39;\\), and then to constitute a set of admissible matrices, keeping in this set only the simulated \\(\\mathcal{P}\\) matrices that satisfy some predefined sign-based restriction. An example of restriction is “after one year, a contractionary monetary-policy shocks has a negative impact on inflation”. As suggested above, if \\(B\\) is any matrix that satisfies \\(\\Omega = BB&#39;\\) (for instance, \\(B\\) can be based on the Cholesky decomposition of \\(\\Omega\\)), then we also have \\(\\Omega = \\mathcal{P}\\mathcal{P}&#39;\\) as soon as \\(\\mathcal{P}=BQ\\), where \\(Q\\) is an orthogonal matrix. Therefore, to draw \\(\\mathcal{P}\\) matrices, it suffices to draw in the set of orthogonal matrices. To fix ideas, consider dimension 2. In that case, the orthogonal matrices are rotation matrices, and the set of orthogonal matrices can be parameterized by the angle \\(x\\), with: \\[ Q_x=\\begin{pmatrix}\\cos(x)&amp;\\cos\\left(x+\\frac{\\pi}{2}\\right)\\\\ \\sin(x)&amp;\\sin\\left(x+\\frac{\\pi}{2}\\right)\\end{pmatrix}=\\begin{pmatrix}\\cos(x)&amp;-\\sin(x)\\\\ \\sin(x)&amp;\\cos(x)\\end{pmatrix}. \\] (This is an angle-\\(x\\) counter-clockwise rotation.) Hence, in that case, by drawing \\(x\\) randomly from \\([0,2\\pi]\\), we draw randomly from the set of \\(2\\times2\\) rotation matrices. For high-dimensional VAR, we lose this simple geometrical representation, though. It is not always possible to parametrize a rotation matrix (high-dimensional VARs). How to proceed, then? Arias, Rubio-Ramírez, and Waggoner (2018) provide a procedure. Their approach is based on the so-called \\(QR\\) decomposition: any square matrix \\(X\\) may be decomposed as \\(X=QR\\) where \\(Q\\) is an orthogonal matrix and \\(R\\) is an upper diagonal matrix. With this in mind, they propose a two-step approach: Draw a random matrix \\(X\\) by drawing each element from independent standard normal distribution. Let \\(X = QR\\) be the \\(QR\\) decomposition of \\(X\\) with the diagonal of \\(R\\) normalized to be positive. The random matrix \\(Q\\) is orthogonal and is a draw from the uniform distribution over the set of orthogonal matrices. Equipped with this procedure, the sign-restriction is based on the following algorithm: Draw a random orthogonal matrix \\(Q\\) (using step i. and ii. described above). Compute \\(B = PQ\\) where \\(P\\) is the Cholesky decomposition of the reduced form residuals \\(\\Omega_{\\varepsilon}\\). Compute the impulse response associated with \\(B\\) \\(y_{t,t+k}=\\Phi^kB\\) or the cumulated response \\(\\bar y_{t,t+k}=\\sum_{j=0}^{k}\\Phi^jB\\). Are the sign restrictions satisfied? Yes. Store the impulse response in the set of admissible response. No. Discard the impulse response. Perform \\(N\\) replications and report the median impulse response (and its “confidence” intervals). Note: to take into account the uncertainty in \\(B\\) and \\(\\Phi\\), you can draw \\(B\\) and \\(\\Phi\\) in Steps 2 and 3 using an inference method (see Section 3). The sign-restriction approach method has the advantage of being relatively agnostic. Moreover, it is fairly flexible, as one can impose sign restrictions on any variable, at any horizon. 4.2 An example A prominent example is Uhlig (2005). Using US monthly data from 1965.I to 2003.XII, he employs sign restrictions to estimate the effect of monetary policy shocks. According to conventional wisdom, monetary contractions should:3 Raise the federal funds rate, Lower prices, Decrease non-borrowed reserves, Reduce real output. The restrictions considered by Uhlig (2005) are as follows: an expansionary monetary policy shock leads to: Increases in prices Increase in nonborrowed reserves Decreases in the federal funds rate What about output? Since is the response of interest, we leave it un-restricted. library(IdSS);library(vars);library(Matrix) data(&quot;USmonthly&quot;) First.date &lt;- &quot;1965-01-01&quot; Last.date &lt;- &quot;1995-06-01&quot; indic.first &lt;- which(USmonthly$DATES==First.date) indic.last &lt;- which(USmonthly$DATES==Last.date) USmonthly &lt;- USmonthly[indic.first:indic.last,] considered.variables&lt;-c(&quot;LIP&quot;,&quot;UNEMP&quot;,&quot;LCPI&quot;,&quot;LPCOM&quot;,&quot;FFR&quot;,&quot;NBR&quot;,&quot;TTR&quot;,&quot;M1&quot;) n &lt;- length(considered.variables) y &lt;- as.matrix(USmonthly[considered.variables]) sign.restrictions &lt;- list() horizon &lt;- list() #Define sign restrictions and horizon for restrictions for(i in 1:n){ sign.restrictions[[i]] &lt;- matrix(0,n,n) horizon[[i]] &lt;- 1 } # Sign restrictions on shock 1 (monetary shock) sign.restrictions[[1]][1,3] &lt;- 1 # positive impact on price level sign.restrictions[[1]][2,5] &lt;- -1 # negative impact on interest rate sign.restrictions[[1]][3,6] &lt;- 1 # positive impact on non-borrowed reserves horizon[[1]] &lt;- 1:5 # from horizon 1 to 5 res.svar.signs &lt;- svar.signs(y,p=3, nb.shocks = 1, #number of identified shocks nb.periods.IRF = 20, bootstrap.replications = 1, # = 1 if no bootstrap, = N if bootstrap confidence.interval = 0.90, # expressed in pp. indic.plot = 1, # Plots are displayed if = 1. nb.draws = 10000, # number of draws sign.restrictions, horizon, recursive =1 # =0 &lt;- draw Q directly, =1 &lt;- draw q recursively ) Figure 4.1: IRF associated with a monetary policy shock; sign-restriction approach. # Output IRFs.signs &lt;- res.svar.signs$IRFs.signs # all the simulated IRFs nb.rotations &lt;- res.svar.signs$xx # total number of rotations all.CI.median &lt;- res.svar.signs$all.CI.median # median IRFs for the selected shocks all.CI.lower.bounds &lt;- res.svar.signs$all.CI.lower.bounds # lower-bound IRFs for the selected shocks all.CI.upper.bounds &lt;- res.svar.signs$all.CI.upper.bounds # upper-bound IRFs for the selected shocks It has to be stressed that the sign restriction approach does not lead to a unique IRF, but to a set of admissible IRFs. Accordingly, we say that this approach is set-identified, not point-identified. 4.3 The penalty-function approach (PFA) An alternative approach is the so-called penalty-function approach (PFA, Uhlig (2005), present in Danne (2015)’s package). This approach relies on a penalty function: \\[ \\begin{array}{llll}f(x)&amp;=&amp;x&amp;\\text{ if }x\\le0\\\\ &amp;&amp;100.x&amp;\\text{ if }x&gt;0\\end{array} \\] which penalizes positive responses and rewards negative responses. Let \\(\\psi_k^j(q)\\) be the impulse response of variable \\(j\\). The \\(\\psi_k^j(q)\\)’s are the elements of \\(\\psi_k(q)=\\Psi_kq\\). Let \\(\\sigma_j\\) be the standard deviation of variable \\(j\\). Let \\(\\iota_{j,k}=1\\) if we restrict the response of variable \\(j\\) at the \\(k^th\\) horizon to be negative, \\(\\iota_{j,k}=-1\\) if we restrict it to be positive, and \\(\\iota_{j,k}=0\\) if there is no restriction. The total penalty is given by \\[ \\mathbf{P}(q)=\\sum_{j=1}^m\\sum_{k=0}^Kf\\left(\\iota_{j,k}\\frac{\\psi_k^j(q)}{\\sigma_j}\\right). \\] We are looking for a solution to \\[\\begin{array}{ll} &amp;\\min_q \\mathbf{P}(q)\\\\ \\text{s.t. }&amp;q&#39;q=1.\\end{array}\\] The problem is solved numerically. 4.4 Narrative sign restrictions A related approach, introduced by Antolín-Díaz and Rubio-Ramírez (2018), consists in imposing that, on some specific dates (based on narrative evidence), the signs of some shocks are positive (or negative).4 For instance, Antolín-Díaz and Rubio-Ramírez (2018) argue that one should rule out structural parameters that disagree with the view that “a negative oil supply shock occurred at the outbreak of the Gulf War in August 1990.” Suppose we want to impose the restriction that, at dates \\(\\{t_1,\\dots,t_J\\}\\), the signs of the \\(j^{th}\\) shock are all positive. Then, the narrative sign restrictions are simply imposed by: \\[ \\hat{\\eta}_{j,t}(B) = e_j&#39;\\hat\\eta_{t}(B) &gt; 0, \\] where \\(\\hat\\eta_{t}(B)\\) is the vector of structural shock associated with a given matrix \\(B\\) (and where \\(e_j\\) is the \\(j^{th}\\) column of the \\(n \\times n\\) identity matrix). References "],["SignsZeros.html", "Chapter 5 Combining sign and zero restrictions", " Chapter 5 Combining sign and zero restrictions Sometimes we need to combine different types of restrictions. For instance: One shock satisfies both zero and sign restrictions. Some shocks can be identified with zero restrictions (SR or LR), others with sign restrictions. Some shocks satisfy the same zero restrictions (e.g. no LR effect on output) but can be distinguished from each other through sign restrictions. In such instances, we must make independent draws from the set of all structural parameters satisfying the zero restrictions. How to do that? Arias, Rubio-Ramírez, and Waggoner (2018) propose to impose the zero restrictions on \\(B\\), and then check signs. Remember, \\(\\mathcal{P}=BQ\\) is a candidate impact IRF. For each structural shock \\(j\\), define the \\(m\\)-column matrices \\(Z_j\\) (zero restrictions) and \\(S_j\\) (sign restrictions). Each row of \\(Z_j\\) (resp. \\(S_j\\)) defines a zero (resp. sign) restriction. \\(Z_j\\) has \\(m-j\\) rows at most (i.e., \\(m-j\\) zero restriction at most). Example 5.1 In a 4-variable VAR, we want to impose that the first structural shock has no effect on variable 1, affects positively variable 2 and negatively variable 3 on impact: \\[Z_1 = \\begin{pmatrix}1 &amp; 0 &amp; 0 &amp; 0\\end{pmatrix}, \\] \\[S_1 = \\begin{pmatrix}0 &amp; 1 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; -1 &amp; 0\\end{pmatrix}. \\] For both zero and sign restrictions to be satisfied, we must have \\[ Z_jb_j=0 \\quad \\mbox{and} \\quad S_jb_j&gt;0, \\] where \\(b_j\\) is the \\(j^{th}\\) column of \\(B\\), i.e. the impact effect of the \\(j^{th}\\) structural shock. The algorithm is as follows: For \\(1\\le j\\le m\\), draw \\(u_j\\in \\mathbb{R}^{m+1-j-z_j}\\) from a standard normal distribution (\\(z_j\\) is the number of zero restrictions imposed on the \\(j^{th}\\) shock) and set \\(w_j = u_j/||u_j||\\). Define \\(Q= \\begin{pmatrix}q_1&amp;...&amp;q_m\\end{pmatrix}\\) recursively by \\(q_j = K_jw_j\\) for any matrix \\(K_j\\) whose columns form an orthogonal basis for the null space of the matrix \\[M_j = \\begin{pmatrix} q_1&amp;...&amp;q_{j-1}&amp;\\color{blue}{(Z_jP)&#39;}\\end{pmatrix}&#39;.\\] (Vector \\(q_j\\) will then be orthogonal to \\(\\begin{pmatrix} q_1&amp;...&amp;q_{j-1}\\end{pmatrix}\\) and satisfy the zero restriction.) Set \\(B=PQ\\). Check sign restrictions (\\(S_jb_j&gt;0\\) for all \\(j\\)?). Perform \\(N\\) replications and report the median impulse response (and its confidence intervals). Function svar.signs can run this algorithm. It is called as follows:5 library(IdSS);library(vars);library(Matrix) data(&quot;USmonthly&quot;) First.date &lt;- &quot;1965-01-01&quot; Last.date &lt;- &quot;1995-06-01&quot; indic.first &lt;- which(USmonthly$DATES==First.date) indic.last &lt;- which(USmonthly$DATES==Last.date) USmonthly &lt;- USmonthly[indic.first:indic.last,] considered.variables&lt;-c(&quot;LIP&quot;,&quot;UNEMP&quot;,&quot;LCPI&quot;,&quot;LPCOM&quot;,&quot;FFR&quot;,&quot;NBR&quot;,&quot;TTR&quot;,&quot;M1&quot;) n &lt;- length(considered.variables) y &lt;- as.matrix(USmonthly[considered.variables]) sign.restrictions &lt;- list() SR.restrictions &lt;- list() horizon &lt;- list() #Define sign restrictions and horizon for restrictions for(i in 1:n){ sign.restrictions[[i]] &lt;- matrix(0,n,n) horizon[[i]] &lt;- 1 } # 2 shocks on the demand for reserves sign.restrictions[[1]][1,6] &lt;- 1 sign.restrictions[[2]][1,7] &lt;- 1 # 3 shocks that drive an endogenous response of the interest rate sign.restrictions[[3]][1,1] &lt;- 1 sign.restrictions[[3]][2,5] &lt;- 1 sign.restrictions[[4]][1,2] &lt;- -1 sign.restrictions[[4]][2,5] &lt;- 1 sign.restrictions[[5]][1,3] &lt;- 1 sign.restrictions[[5]][2,5] &lt;- 1 # monetary policy shock sign.restrictions[[6]][1,5] &lt;- -1 sign.restrictions[[6]][2,3] &lt;- 1 sign.restrictions[[6]][3,6] &lt;- 1 # horizon for sign restrictions on monetary policy shock horizon[[6]] &lt;- 1:5 #Define zero restrictions # 2 shocks on the demand for reserves SR.restrictions[[1]] &lt;- array(0,c(1,n)) SR.restrictions[[1]][1,5] &lt;- 1 # no impact on the interest rate SR.restrictions[[2]] &lt;- array(0,c(1,n)) SR.restrictions[[2]][1,5] &lt;- 1 # no impact on the interest rate for(i in 3:n){ SR.restrictions[[i]] &lt;- array(0,c(0,n)) } res.svar.signs.zeros &lt;- svar.signs(y,p=3, nb.shocks = 6, #number of identified shocks nb.periods.IRF = 20, bootstrap.replications = 100, # = 1 if no bootstrap, = N if bootstrap confidence.interval = 0.90, # expressed in pp. indic.plot = 1, # Plots are displayed if = 1. nb.draws = 10000, # number of draws sign.restrictions, horizon, recursive =0, SR.restrictions ) Figure 5.1: IRFs; sign-restriction approach. Figure 5.2: IRFs; sign-restriction approach. Figure 5.3: IRFs; sign-restriction approach. Figure 5.4: IRFs; sign-restriction approach. Figure 5.5: IRFs; sign-restriction approach. Figure 5.6: IRFs; sign-restriction approach. # Output IRFs.signs &lt;- res.svar.signs.zeros$IRFs.signs # all the simulated IRFs nb.rotations &lt;- res.svar.signs.zeros$xx # total number of rotations all.CI.median &lt;- res.svar.signs.zeros$all.CI.median # median IRFs for the selected shocks all.CI.lower.bounds &lt;- res.svar.signs.zeros$all.CI.lower.bounds # lower-bound IRFs for the selected shocks all.CI.upper.bounds &lt;- res.svar.signs.zeros$all.CI.upper.bounds # upper-bound IRFs for the selected shocks References "],["forecast-error-variance-maximization.html", "Chapter 6 Forecast error variance maximization 6.1 The main (unconditional) approach 6.2 Restrictions based on narrative historical decomposition", " Chapter 6 Forecast error variance maximization 6.1 The main (unconditional) approach The approach presented in this section exploits the derivations of Uhlig (2004). Consider a process \\(\\{y_t\\}\\) that admits the infinite MA representation of Eq. (1.4). Let \\(Q\\) be an orthogonal matrix, an alternative decomposition of \\(y_t\\) is: \\[\\begin{eqnarray} y_t&amp;=&amp;\\sum_{h=0}^{+\\infty}\\Psi_h\\underbrace{\\eta_{t-h}}_{Q\\tilde \\eta_{t-h}} = \\sum_{h=0}^{+\\infty}\\underbrace{\\Psi_hQ}_{\\tilde\\Psi_h}\\tilde \\eta_{t-h} = \\sum_{h=0}^{+\\infty}\\tilde\\Psi_h\\tilde \\eta_{t-h}, \\end{eqnarray}\\] where \\(\\tilde \\eta_{t-h}=Q&#39;\\eta_{t-h}\\) are the white-noise shocks associated with the new MA representation, \\(Q\\) being an orthgonal matrix. (They also satisfy \\(\\mathbb{V}ar(\\tilde\\eta_t)=Id\\).) The \\(h\\)-step ahead prediction error of \\(y_{t+h}\\), given all the data up to, and including, \\(t-1\\) is given by \\[ e_{t+h}(h)=y_{t+h}-\\mathbb{E}_{t-1}(y_{t+h})=\\sum_{j=0}^h\\tilde \\Psi_h\\tilde \\eta_{t+h-j}. \\] The variance-covariance matrix of \\(e_{t+h}(h)\\) is \\[ \\Omega^{(h)}=\\sum_{j=0}^h\\tilde \\Psi_j\\tilde \\Psi_j&#39;=\\sum_{j=0}^h \\Psi_j \\Psi_j&#39;. \\] We can decompose \\(\\Omega^{(h)}\\) into the contribution of each shock \\(l\\) (\\(l^{th}\\) component of \\(\\tilde{\\eta}_t\\)): \\[ \\Omega^{(h)}=\\sum_{l=1}^n\\Omega_l^{(h)}(Q) \\] with \\[ \\Omega_l^{(h)}(Q) =\\sum_{j=0}^h(\\Psi_jq_l)(\\Psi_jq_l)&#39;, \\] where \\(q_l\\) is the \\(l^{th}\\) column of \\(Q\\). This decomposition can be used with the objective of finding the impulse vector \\(b\\) that is s.t. that it explains as much as possible of the sum of the \\(h\\)-step ahead prediction error variance of some variable \\(i\\), say, for prediction horizons \\(h \\in [\\underline{h} , \\overline{h}]\\). Formally, the task is to explain as much as possible of the variance \\[ \\sigma^2(\\underline{h},\\overline{h},q_1)=\\sum_{h=\\underline{h}}^{\\overline{h}} \\sum_{j=0}^h\\left[(\\Psi_jq_1)(\\Psi_jq_1)&#39;\\right]_{i,i} \\] with a single impulse vector \\(q_1\\). Denote by \\(E_{ii}\\) the matrix that is filled with zeros, except for its (\\(i,i\\)) entry, set to 1. We have: \\[\\begin{eqnarray*} \\sigma^2(\\underline{h},\\overline{h},q_1)&amp;=&amp;\\sum_{h=\\underline{h}}^{\\overline{h}} \\sum_{j=0}^h\\left[(\\Psi_jq_1)(\\Psi_jq_1)&#39;\\right]_{i,i}=\\sum_{h=\\underline{h}}^{\\overline{h}} \\sum_{j=0}^h Tr\\left[E_{ii}(\\Psi_jq_1)(\\Psi_jq_1)&#39;\\right]\\\\ &amp;=&amp;\\sum_{h=\\underline{h}}^{\\overline{h}} \\sum_{j=0}^h Tr\\left[q_1&#39;\\Psi_j&#39;E_{ii}\\Psi_j q_1\\right]\\\\ &amp;=&amp; q_1&#39;Sq_1, \\end{eqnarray*}\\] where \\[\\begin{eqnarray*} \\begin{array}{lll}S&amp;=&amp;\\sum_{h=\\underline{h}}^{\\overline{h}}\\sum_{j=0}^{h}\\Psi_j&#39;E_{ii}\\Psi_j\\\\ &amp;=&amp;\\sum_{j=0}^{\\overline{h}}(\\overline{h}+1-max(\\underline{h},j))\\Psi_j&#39;E_{ii}\\Psi_j\\\\ &amp;=&amp;\\sum_{j=0}^{\\overline{h}}(\\overline{h}+1-max(\\underline{h},j))\\Psi_{j,i}&#39;\\Psi_{j,i}\\\\ \\end{array} \\end{eqnarray*}\\] where \\(\\Psi_{j,i}\\) denotes row \\(i\\) of \\(\\Psi_{j}\\), i.e., the response of variable \\(i\\) at horizon \\(j\\) (when \\(Q=Id\\)). The maximization problem subject to the side constraint \\(q_1&#39;q_1=1\\) can be written as a Lagrangian: \\[ L=q_1&#39;Sq_1-\\lambda(q_1&#39;q_1-1), \\] with the first-order condition \\(Sq_1=\\lambda q_1\\) (the side constraint is \\(q_1&#39;q_1=1\\)). From this equation, we see that the solution \\(q_1\\) is an eigenvector of \\(S\\), the one associated with eigenvalue \\(\\lambda\\). We also see that \\(\\sigma^2(\\underline{h},\\overline{h},q_1)=\\lambda\\). Thus, to maximize this variance, we need to find the eigenvector of \\(S\\) that is associated with the maximal eigenvalue \\(\\lambda\\). That defines the first principal component (see Section 10.1). That is, if \\(S\\) admits the following spectral decomposition: \\[ S = \\mathcal{P}D\\mathcal{P}&#39;, \\] where \\(D\\) is diagonal matrix whose entries are the (ordered) eigenvalues: \\(\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_n \\ge 0\\), then \\(\\sigma^2(\\underline{h},\\overline{h},q_1)\\) is maximized for \\(q_1 = p_1\\), where \\(p_1\\) is the first column of \\(\\mathcal{P}\\). The following code identifies a ``main GDP shock’’ using Uhlig’s method. library(IdSS) library(readxl) library(vars) library(Matrix) # Declare data: TFP &lt;- levpan$tfp_lev GDP &lt;- levpan$lngdpcap E12 &lt;- levpan$e12m CONS &lt;- levpan$lnconcap HOURS &lt;- levpan$lnhrscap y &lt;- cbind(TFP,GDP,E12,CONS,HOURS) names.of.variables &lt;- c(&quot;TFP&quot;,&quot;GDP&quot;,&quot;E12&quot;,&quot;Consumption&quot;,&quot;Hours&quot;) colnames(y) &lt;- names.of.variables names.of.shocks &lt;- c(&quot;Main GDP shock&quot;) #define horizons for FEVM H1 &lt;- 1 H2 &lt;- 20 variable &lt;- 2 # variables for which we want to maximize FEVD norm &lt;- 15 # horizon at which the impact of the shock # is normalized to be positive res.svar.fevmax &lt;- svar.fevmax(y,p=2, nb.shocks=1, names.of.shocks, H1, H2, variable, norm, nb.periods.IRF = 20, bootstrap.replications = 100, # This is used in #the parametric bootstrap only confidence.interval = 0.90, # expressed in pp. indic.plot = 1 # Plots are displayed if = 1. ) # Compute variance decomposition: Variance.decomp &lt;- variance.decomp(res.svar.fevmax$simulated.IRFs) vardecomp &lt;- Variance.decomp$vardecomp mean(vardecomp[2,2,20,,1]) # mean contribution (across all simulated IRFs) ## [1] 0.8214392 # of 1st shock to variance of second variable, horizon 20. mean(vardecomp[1,2,10,,1]) # mean contribution of 1st shock ## [1] 0.6171658 # to covariance between 1st and 2nd variable, horizon 10. Figure 6.1: Main GDP shock. The ``main GDP shock’’ explains 82% of the variance of GDP at horizon 20. Barsky and Sims (2011) exploit this approach to identify a “TFP news shock”, that they define as the shock (a) that is orthogonal to the innovation in current utilization-adjusted TFP and (b) that best explains variation in future TFP. Levchenko and Pandalai-Nayar (2018) add a “sentiment shock” that they define as the shock (a) that is orthogonal to both the innovation in current utilization-adjusted TFP and to the TFP news shock, and (b) that best explains variation in consumer sentiment. The following code replicates Levchenko and Pandalai-Nayar (2018). They use a mix of zero and FEVD to identify TFP surprises, TFP news, and sentiment shocks. We adopt a different approach by using FEVD to capture zero restrictions. names.of.shocks &lt;- c(&quot;TFP surprise&quot;,&quot;TFP news&quot;,&quot;Sentiment&quot;) #define horizons for FEVM H1 &lt;- array(0,c(1,3)) H2 &lt;- array(0,c(1,3)) H1[1,1] &lt;- 1 H1[1,2] &lt;- 1 H1[1,3] &lt;- 1 H2[1,1] &lt;- 1 H2[1,2] &lt;- 40 H2[1,3] &lt;- 2 variable &lt;- c(1,1,3) # variables for which we want to maximize FEVD norm &lt;- c(1,20,2) # horizon at which the impact of the # shock is normalized to be positive res.svar.fevmax &lt;- svar.fevmax(y,p=2, nb.shocks=3, names.of.shocks, H1, H2, variable, norm, nb.periods.IRF = 40, bootstrap.replications = 100, # This is used in the #parametric bootstrap only confidence.interval = 0.90, # expressed in pp. indic.plot = 1 # Plots are displayed if = 1. ) Figure 6.2: Replication of Levchenko and Pandalai-Nayar (2020). FEVD and zero restrictions. Figure 6.3: Replication of Levchenko and Pandalai-Nayar (2020). FEVD and zero restrictions. Variance.decomp &lt;- variance.decomp(res.svar.fevmax$simulated.IRFs) vardecomp &lt;- Variance.decomp$vardecomp mean(vardecomp[2,2,40,,3])# mean contribution (across all simulated IRFs) ## [1] 0.1216597 # of 3rd shock to variance of second variable, horizon 40. Figure 6.4: Replication of Levchenko and Pandalai-Nayar (2020). FEVD and zero restrictions. Sentiment shocks explain 12% of the variance of GDP, against 74% for TFP shocks (including 62% for TFP news shocks). 6.2 Restrictions based on narrative historical decomposition A related approach, introduced by Antolín-Díaz and Rubio-Ramírez (2018), consists in imposing that, on some specific dates (based on narrative information), a particular shock was the most important contributor to the unexpected movement of some variable during a particular period.6 This can be formalized in two different ways (respectively called Type A and Type B by Antolín-Díaz and Rubio-Ramírez (2018)): Type A: A given shock is the most important (least important) driver of the unexpected change in a variable during some periods. For these periods, the absolute value of its contribution to the unexpected change in a variable is larger (smaller) than the absolute value of the contribution of any other structural shock. Type B: A given shock is the overwhelming (negligible) driver of the unexpected change in a given variable during the period. For these periods, the absolute value of its contribution to the unexpected change in a variable is larger (smaller) than the sum of the absolute value of the contributions of all other structural shocks. References "],["NonGaussian.html", "Chapter 7 Identification based on non-normality of the shocks 7.1 Intuition 7.2 Independent Component Analysis (ICA) 7.3 Pseudo-Maximum Likelihood (PML) approach 7.4 Relation with the Heteroskedasticity Identification", " Chapter 7 Identification based on non-normality of the shocks 7.1 Intuition In this section, we show that the non-identification of the structural shocks (\\(\\eta_t\\)) is specific to the Gaussian case. We propose consistent estimation approaches for SVAR in the context of non-Gaussian shocks. We have seen in what precedes that we cannot identify \\(B\\) based on first and second moments only. Since a Gaussian distribution is perfectly determined by the first two moments, it comes that one cannot achieve identification when the structural shocks are Gaussian. That is, even if we observe an infinite number of i.i.d. \\(B \\eta_t\\), we cannot recover \\(B\\) is the \\(\\eta_t\\)’s are Gaussian. Indeed, if \\(\\eta_t \\sim \\mathcal{N}(0,Id)\\), then the distribution of \\(\\varepsilon_t \\equiv B \\eta_t\\) is \\(\\mathcal{N}(0,BB&#39;)\\). Hence \\(\\Omega = B B&#39;\\) is observed (in the population), but for any orthogonal matrix \\(Q\\) (i.e. \\(QQ&#39;=Id\\)), we also have \\(BQ \\eta_t \\sim \\mathcal{N}(0,\\Omega)\\). To illustrate, consider the following bivariate Gaussian situations, with \\(\\Theta_1=0\\)): \\(\\left[\\begin{array}{c}\\eta_{1,t}\\\\ \\eta_{2,t}\\end{array}\\right]\\sim \\mathcal{N}(0,Id)\\), with \\(B = \\left[\\begin{array}{cc} 1 &amp; 2 \\\\ -1 &amp; 1 \\end{array}\\right]\\) and \\(Q = \\left[\\begin{array}{cc} \\cos(\\pi/3) &amp; -\\sin(\\pi/3) \\\\ \\sin(\\pi/3) &amp; \\cos(\\pi/3) \\end{array}\\right]\\) (rotation). Figure 7.1 shows that the distributions of \\(B \\eta_t\\) and of \\(BQ\\eta_t\\) are identical. However, the impulse response functions associated with one of the other impulse matrix (\\(B\\) or \\(BQ\\)) are different. This is illustrated by Figure 7.2, that shows the IRFs associated with two identical models (defined by Eq. (1.5)), the only difference being the impulse matrix (\\(B\\) or \\(BQ\\)). Figure 7.1: This figure compares the distributions of two Gaussian bivariate vectors, \\(B \\eta_t\\) and \\(BQ\\eta_t\\), where \\(\\eta_{t} \\sim \\mathcal{N}(0,Id)\\) (therefore \\(\\eta_{1,t}\\) and \\(\\eta_{2,t}\\) are independent), and \\(Q\\) is an orthogonal matrix. Figure 7.2: This figure shows that the impulse response functions associated with an impulse matrix equal to \\(B\\) (black line) or \\(BQ\\) (red line) are different (even if \\(BB&#39;=BQ(BQ)&#39;\\)). Hence, in the Gaussian case, external restrictions (economic hypotheses) are needed to identify \\(B\\) (see previous sections). But such restrictions may not be necessary if the structural shocks are not Gaussian. That is, the identification problem is very specific to normally-distributed \\(\\eta_t\\)’s (Rigobon (2003), Normandin and Phaneuf (2004), Lanne and Lütkepohl (2008)). To better see why this can be the case, consider again a bivariate vector of independent structural shocks (\\(\\eta_{1,t}\\) and \\(\\eta_{2,t}\\)) but, now, assume that one of them is not Gaussian any more. Specifically, assume that \\(\\eta_{2,t}\\) is drawn from a Student distribution with 5 degrees of freedom: \\(\\eta_{1,t} \\sim \\mathcal{N}(0,1)\\), \\(\\eta_{2,t} \\sim t(5)\\), \\(B = \\left[\\begin{array}{cc} 1 &amp; 2 \\\\ -1 &amp; 1 \\end{array}\\right]\\) and \\(Q = \\left[\\begin{array}{cc} \\cos(\\pi/3) &amp; -\\sin(\\pi/3) \\\\ \\sin(\\pi/3) &amp; \\cos(\\pi/3) \\end{array}\\right]\\). Figure 7.3 shows that, in this case, \\(B \\eta_t\\) and \\(BQ\\eta_t\\) do not have the same distribution any more (in spite of the fact that, in both cases, we have \\(\\mathbb{V}ar(\\varepsilon_t)=BB&#39;\\)). This opens the door to the identification of the impulse matrix (\\(BQ\\)) in the non-Gaussian case. Figure 7.3: This figure compares the distributions of two Gaussian bivariate vectors, \\(B \\eta_t\\) and \\(BQ\\eta_t\\), where \\(\\eta_t{1,t} \\sim \\mathcal{N}(0,1)\\), \\(\\eta_t{2,t} \\sim t(5)\\), and \\(Q\\) is an orthogonal matrix. 7.2 Independent Component Analysis (ICA) The exercise that consists in identifying non-Gaussian independent shocks out of linear combinations of these shocks is a well-known problem of the signal-processing literature, called independent component analysis (ICA). Let us denote by \\(C\\) the matrix that is such that \\(C = \\Omega^{-1/2}B\\), where \\(\\Omega^{1/2}\\) results from the Cholesky decomposition of \\(\\Omega = BB&#39;\\) (implying, \\(\\Omega^{1/2}{\\Omega^{1/2}}&#39;=\\Omega\\)). It is easy to check that \\(C\\) is an orthogonal matrix (and we have \\(B = \\Omega^{1/2}C\\)). The classical ICA problem is as follows: Find \\(C\\) such that \\(\\varepsilon_t = C \\eta_t\\) (or \\(\\eta_t = C&#39;\\varepsilon_t\\)) given that:7 We observe the \\(\\varepsilon_t\\)’s, The components of \\(\\eta_t\\) are independent, \\(CC&#39;=Id\\) (i.e., \\(C\\) is orthogonal). Figure 7.4 represents again some bivariate distributions. The black (red) lines correspond to the distributions of \\(\\eta_t\\) (\\(C\\eta_t\\)). It is important to note that the two components of vector \\(C \\eta_t\\) are not independent (contrary to those of \\(\\eta_t\\)). Figure 7.4: The three plots represent the bivariate distributions of \\(\\eta_t\\) (black) and of \\(C\\eta_t\\) (red), where the two components of \\(\\eta_t\\) are independent, of unit variance, and \\(C\\) is orthogonal. Hence, for each of the three plots, \\(\\mathbb{V}ar(C\\eta_t)=Id\\). In all cases, we have \\(\\mathbb{V}ar(\\varepsilon_t)=\\mathbb{V}ar(\\eta_t)=Id\\). But the two components of \\(\\varepsilon_t\\) are not independent. For instance, in the last two cases, we have \\(\\mathbb{E}(\\varepsilon_{2,t}|\\varepsilon_{1,t}&gt;4)&lt;0\\) (whereas \\(\\mathbb{E}(\\eta_{2,t}|\\eta_{1,t}&gt;4)=0\\)). The objective of ICA is to rotate \\(\\varepsilon_t\\) to retrieve independent components (\\(\\eta_t\\)). Hypothesis 7.1 Process \\(\\eta_t\\) satisfies: The \\(\\eta_t\\)’s are i.i.d. (across time) with \\(\\mathbb{E}(\\eta_t) = 0\\) and \\(\\mathbb{V}ar(\\eta_t) = Id.\\) The components \\(\\eta_{1,t}, \\ldots, \\eta_{n,t}\\) are mutually independent. iii We have \\[ \\varepsilon_t = C_0 \\eta_t, \\] with \\(\\mathbb{V}ar(\\varepsilon_t) = Id\\) (i.e., \\(C_0\\) is orthogonal). Theorem 7.1 (Eriksson, Koivunen (2004)) If Hypothesis 7.1 is satisfied and if at most one of the components of \\(\\eta\\) is Gaussian, then matrix \\(C_0\\) is identifiable up to the post multiplication by \\(DP\\), where \\(P\\) is a permutation matrix and \\(D\\) is a diagonal matrix whose diagonal entries are 1 or \\(-1\\).} 7.3 Pseudo-Maximum Likelihood (PML) approach Hence, non-normal structural shocks are identifiable. But how to estimate them based on observations of the \\(\\varepsilon_t\\)’s? Gouriéroux, Monfort, and Renne (2017) have proposed a Pseudo-Maximum Likelihood (PML) approach. This approach consists in maximizing a so-called pseudo log-likelihood function, based on a set of p.d.f. \\(g_i (\\eta_i), i=1,\\ldots,n\\) (that may be different from the true p.d.f. of the \\(\\eta_{i,t}\\)’s): \\[\\begin{equation} \\log \\mathcal{L}_T (C) = \\sum^T_{t=1} \\sum^n_{i=1} \\log g_i (c&#39;_i \\varepsilon_t),\\tag{7.1} \\end{equation}\\] where \\(c_i\\) is the \\(i^{th}\\) column of matrix \\(C\\) (or \\(c&#39;_i\\) is the \\(i^{th}\\) row of \\(C^{-1}\\) since \\(C^{-1}=C&#39;\\)). The log-likelihood function (7.1) is computed as if the errors \\(\\eta_{i,t}\\) had the pdf \\(g_i (\\eta_i)\\). The PML estimator of matrix \\(C\\) maximizes the pseudo log-likelihood function: \\[\\begin{equation} \\widehat{C_T} = \\arg \\max_C \\sum^T_{t=1} \\sum^n_{i=1} \\log g_i (c&#39;_i \\varepsilon_t),\\tag{7.2} \\end{equation}\\] The restrictions \\(C&#39;C = Id\\) can be eliminated by parameterizing \\(C\\) in such a way that, whatever the considered parameters, \\(C\\) is orthogonal.8 Gouriéroux, Monfort, and Renne (2017) propose to use, for that, the Cayley’s representation: any orthogonal matrix with no eigenvalue equal to \\(-1\\) can be written as \\[\\begin{equation} C(A) = (Id+A) (Id-A)^{-1}, \\end{equation}\\] where \\(A\\) is a skew symmetric (or antisymmetric) matrix, such that \\(A&#39;=-A\\). There is a one-to-one relationship with \\(A\\), since: \\[\\begin{equation} A = (C(A)+Id)^{-1} (C(A)-Id). \\end{equation}\\] Hence, the PML estimator of matrix \\(C\\) is obtained as \\(\\widehat{C_T} = C(\\hat{A}_T),\\) where: \\[\\begin{equation} \\hat{A}_T = \\arg \\max_{a_{i,j}, i&gt;j} \\sum^T_{t=1} \\sum^n_{i=1} \\log g_i [c_i (A)&#39; \\varepsilon_t].\\tag{7.3} \\end{equation}\\] Under assumptions on the \\(g_i\\) functions (excluding the Gaussian distributions), Gouriéroux, Monfort, and Renne (2017) derive the asymptotic properties of the PML estimator. Specifically, the PML estimator \\(\\widehat{C_T}\\) of \\(C_0\\) is consistent (in \\(\\mathcal{P}_0\\), the set of matrices obtained by permutation and sign change of the columns of \\(C_0\\)) and asymptotically normal, with speed of convergence \\(1/\\sqrt{T}\\). The asymptotic variance-covariance matrix of \\(vec \\sqrt{T} (\\widehat{C_T} - C_0)\\) is \\(A^{-1} \\left[\\begin{array}{cc} \\Gamma &amp; 0 \\\\ 0 &amp; 0 \\end{array} \\right] (A&#39;)^{-1}\\), where matrices \\(A\\) and \\(\\Gamma\\) are detailed in Gouriéroux, Monfort, and Renne (2017). Note that the potential misspecification of pseudo-distributions \\(g_i\\) has no effect on the consistency of these specific PML estimators. Table 7.1 reports usual p.d.f. and their derivatives. (The latter are needed to compute the asymptotic variance-covariance matrix of \\(vec \\sqrt{T} (\\widehat{C_T} - C_0)\\).) Table 7.1: This table reports usual p.d.f. and their derivatives. \\(\\log g(x)\\) \\(\\dfrac{d \\log g(x)}{d x}\\) \\(\\dfrac{d^2 \\log g(x)}{d x^2}\\) Gaussian \\(cst - x^2/2\\) \\(-x\\) \\(-1\\) Student \\(t(\\nu&gt;4)\\) \\(-\\dfrac{1-\\nu}{2}\\log\\left( 1 +\\dfrac{x^2}{\\nu-2} \\right)\\) \\(-\\dfrac{x(1+\\nu)}{\\nu - 2 + x^2}\\) \\(- (1+\\nu) \\dfrac{\\nu - 2 - x^2}{\\nu - 2 + x^2}\\) Hyperbolic secant \\(cst - \\log\\left( \\cosh\\left\\{\\dfrac{\\pi}{2}x\\right\\} \\right)\\) \\(-\\dfrac{\\pi}{2} anh\\left(\\dfrac{\\pi}{2}x\\right)\\) \\(-\\left(\\dfrac{\\pi}{2}\\dfrac{1}{\\cosh\\left(\\dfrac{\\pi}{2}x\\right)}\\right)^2\\) Subgaussian \\(cst + \\pi x^2 + \\log \\left(\\cosh\\left\\{\\dfrac{\\pi}{2}x\\right\\}\\right)\\) \\(2\\pi x+\\dfrac{\\pi}{2}\\tanh\\left(x \\dfrac{\\pi}{2}\\right)\\) \\(2\\pi +\\left(\\dfrac{\\pi}{2}\\dfrac{1}{\\cosh\\left(\\dfrac{\\pi}{2}x\\right)}\\right)^2\\) Example 7.1 (Non-Gaussian monetary-policy shocks) We apply the PML-ICA approach on U.S. data coerving the period 1959:IV to 2015:I at the quarterly frequency (\\(T=224\\)). We consider three dependent variables: inflation (\\(\\pi_t\\)), economic activity (\\(z_t\\), the output gap) and the nominal short-term interest rate (\\(r_t\\)). Changes in the log of oil prices are added as an exogenous variable (\\(x_t\\)). library(IdSS) First.date &lt;- &quot;1959-04-01&quot; Last.date &lt;- &quot;2015-01-01&quot; data &lt;- US3var data &lt;- data[(data$Date&gt;=First.date)&amp;(data$Date&lt;=Last.date),] Y &lt;- as.matrix(data[c(&quot;infl&quot;,&quot;y.gdp.gap&quot;,&quot;r&quot;)]) names.var &lt;- c(&quot;inflation&quot;,&quot;real activity&quot;,&quot;short-term rate&quot;) T &lt;- dim(Y)[1] n &lt;- dim(Y)[2] Let us denote by \\(W_t\\) the set of information made of the past values of \\(y_t= [\\pi_t,z_t,r_t]\\), that is \\(\\{y_{t-1},y_{t-2},\\dots\\}\\), and of exogenous variables \\(\\{x_{t},x_{t-1},\\dots\\}\\). The reduced-form VAR model reads: \\[ y_t = \\underbrace{\\mu + \\sum_{i=1}^{p} \\Phi_i y_{t-i} + \\Theta x_t}_{a(W_t;\\theta)} + u_t \\] where the \\(u_t\\)’s are assumed to be serially independent, with zero mean and variance-covariance matrix \\(\\Omega\\). Matrices \\(\\mu\\), \\(\\Phi_i\\), \\(\\Theta\\) and \\(\\Omega\\) are consistently estimated by OLS. Jarque-Bera tests support the hypothesis of non-normality for all residuals. nb.lags &lt;- 6 # number of lags used in the VAR model X &lt;- NULL for(i in 1:nb.lags){ lagged.Y &lt;- rbind(matrix(NaN,i,n),Y[1:(T-i),]) X &lt;- cbind(X,lagged.Y)} X &lt;- cbind(X,data$commo) # add exogenous variables Phi &lt;- matrix(0,n,n*nb.lags);mu &lt;- rep(0,n) effect.commo &lt;- rep(0,n) U &lt;- NULL # Eta is the matrix of OLS residuals for(i in 1:n){ eq &lt;- lm(Y[,i] ~ X) Phi[i,] &lt;- eq$coef[2:(dim(Phi)[2]+1)] mu[i] &lt;- eq$coef[1] U &lt;- cbind(U,eq$residuals) effect.commo[i] &lt;- eq$coef[length(eq$coef)] } Omega &lt;- var(U) # Covariance matrix of the OLS residuals. Omeg12 &lt;- t(chol(Omega)) # Cholesky matrix associated with Omega (lower triang.) Eps &lt;- U %*% t(solve(Omeg12)) # Recover associated structural shocks We want to estimate the orthogonal matrix \\(C\\) such that \\(u_t=\\Omega^{1/2}C \\eta_t\\), where \\(\\Omega^{1/2}\\) results from the Cholesky decomposition of \\(\\Omega\\) and the components of \\(\\eta_t\\) are independent, zero-mean with unit variance. The PML approach is applied on standardized VAR residuals given by: \\[ \\hat\\varepsilon_t = \\hat\\Omega^{-1/2}_T\\underbrace{[y_t - a(W_t;\\hat\\theta_T)]}_{\\mbox{VAR residuals}}. \\] By construction of \\(\\hat\\Omega^{-1/2}_T\\), it comes that the covariance matrix of these residuals is \\(Id\\). The pseudo density functions are distinct and asymmetric mixtures of Gaussian distributions. distri &lt;- list( type=c(&quot;mixt.gaussian&quot;,&quot;mixt.gaussian&quot;,&quot;mixt.gaussian&quot;), df=c(NaN,NaN,NaN), p=c(0.5,.5,.5),mu=c(.1,.1,.1),sigma=c(.5,.7,1.3)) AA.0 &lt;- c(0,0,0) res.optim &lt;- optim(AA.0,func.2.minimize, Y = Eps, distri = distri, gr = d.func.2.minimize, method=&quot;Nelder-Mead&quot;, control=list(trace=FALSE,maxit=1000)) AA.0 &lt;- res.optim$par res.optim &lt;- optim(AA.0,func.2.minimize,d.func.2.minimize, Y = Eps, distri = distri, method=&quot;BFGS&quot;, control=list(trace=FALSE)) AA.est &lt;- res.optim$par n &lt;- ncol(Y) M &lt;- make.M(n) A.est &lt;- matrix(M %*% AA.est,n,n) C.PML &lt;- (diag(n) + A.est) %*% solve(diag(n) - A.est) eta.PML &lt;- Eps %*% C.PML # eta.PML are the ICA-estimated structural shocks # Compute asymptotic covariance matrix of C.PML: V &lt;- make.Asympt.Cov.delta(eta.PML,distri,C.PML) param &lt;- c(C.PML) st.dev &lt;- sqrt(diag(V)) t.stat &lt;- c(C.PML)/sqrt(diag(V)) cbind(param,st.dev,t.stat) # print results of PML estimation ## param st.dev t.stat ## [1,] 0.94417705 0.040848382 23.1141845 ## [2,] -0.32711569 0.118802653 -2.7534376 ## [3,] 0.03905164 0.074172945 0.5264944 ## [4,] 0.32070293 0.119270893 2.6888616 ## [5,] 0.93977707 0.041629110 22.5749976 ## [6,] 0.11818924 0.060821400 1.9432179 ## [7,] -0.07536139 0.071980455 -1.0469702 ## [8,] -0.09906759 0.062185577 -1.5930959 ## [9,] 0.99222290 0.007785691 127.4418551 (Note: it is always useful to combine two optimization algorithms, such as Nelder-Mead and BFGS.) We would obtain close results by neglecting commodity prices. In that case, one can simply use the function estim.SVAR.ICA of the IdSS package. Let us compare the \\(C\\) matrix obtained in the two cases (with or without commodity prices): ICA.res.no.commo &lt;- estim.SVAR.ICA(Y,distri = distri,p=6) round(cbind(ICA.res.no.commo$C.PML,NaN,C.PML),3) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] ## [1,] 0.956 0.287 -0.059 NaN 0.944 0.321 -0.075 ## [2,] -0.292 0.950 -0.108 NaN -0.327 0.940 -0.099 ## [3,] 0.025 0.121 0.992 NaN 0.039 0.118 0.992 Once \\(C\\) has been estimated, it remains to label the resulting structural shocks (components of \\(\\eta_{t}\\)). Postulated shocks are monetary-policy, supply, and demand shocks. This labelling can be based on the following considerations: Contractionary monetary-policy shocks have a negative impact on real activity and on inflation. Supply shock have influences of opposite signs on economic activity and on inflation. Demand shock have influences of same signs on economic activity and on inflation. Let us compute the IRFs associated with the three structural shocks. (For the sake of comparison, the first line of plots shows the IRFs to a monetary-policy shock obtained from a Cholesky-based approach where the short-term rate is ordered last.) IRF.Chol &lt;- array(NaN,c(n,41,n)) IRF.ICA &lt;- array(NaN,c(n,41,n)) PHI &lt;- list();for(i in 1:nb.lags){PHI[[i]]&lt;-array(Phi,c(3,3,nb.lags))[,,i]} for(jjjj in 1:n){ u.shock &lt;- rep(0,n) u.shock[jjjj] &lt;- 1 IRF.Chol[,,jjjj] &lt;- t(simul.VAR(c=rep(0,3),Phi=PHI,B=Omeg12,nb.sim=41, y0.star=rep(0,3*nb.lags),indic.IRF = 1,u.shock = u.shock)) IRF.ICA[,,jjjj] &lt;- t(simul.VAR(c=rep(0,3),Phi=PHI,B=Omeg12%*%C.PML,nb.sim=41, y0.star=rep(0,3*nb.lags),indic.IRF = 1,u.shock = u.shock)) } Figure 7.5: The first row of plots shows the responses of the three endogenous variables to the monetary policy shock in the context of a Cholesky-idendtified SVAR (ordering: inflation, output gap, interest rate). The next three rows of plots show the repsonses of the endogenous variables to the three structural shocks identified by ICA. The last one (Shock 3) is close to the Cholesky-identified monetary policy shock. According to Figure 7.5, Shock 1 is a supply shock, Shock 2 is a demand shock, and Shock 3 is a monetary-policy shock. Note that Shock 3 is close to the one resulting from the Cholesky approach. 7.4 Relation with the Heteroskedasticity Identification In some cases, where the \\(\\varepsilon_t\\)’s are heteroskedastic, the \\(B\\) matrix can be identified (Rigobon (2003), Lanne, Lütkepohl, and Maciejowska (2010)). Consider the case where we still have \\(\\varepsilon_t = B \\eta_t\\) but where \\(\\eta_t\\)’s variance conditionally depends on a regime \\(s_t \\in \\{1,\\dots,M\\}\\). That is: \\[ \\mathbb{V}ar(\\eta_{k,t}|s_t) = \\lambda_{s_t,k} \\quad \\mbox{for } k \\in \\{1,\\dots,n\\} \\] Denoting by \\(\\Lambda_i\\) the diagonal matrix whose diagonal entries are the \\(\\lambda_{i,k}\\)’s, it comes that: \\[ \\mathbb{V}ar(\\eta_{t}|s_t) = \\Lambda_{s_t},\\quad \\mbox{and}\\quad \\mathbb{V}ar(\\varepsilon_{t}|s_t) = B\\Lambda_{s_t}B&#39;. \\] Without loss of generality, it can be assumed that \\(\\Lambda_1=Id\\). In this context, \\(B\\) is identified, apart from sign reversal of its columns if for all \\(k \\ne j \\in \\{1,\\dots,n\\}\\), there is a regime \\(i\\) s.t. \\(\\lambda_{i,k} \\ne \\lambda_{i,j}\\). (Prop.1 in Lanne, Lütkepohl, and Maciejowska (2010)). Bivariate regime case (\\(M=2\\)): \\(B\\) identified if the \\(\\lambda_{2,k}\\)’s are all different. That is, identification is ensured if “there is sufficient heterogeneity in the volatility changes” (Lütkepohl and Netšunajev (2017)). If the regimes \\(s_t\\) are exogenous and serially independent, then this situation is consistent with the “non-Gaussian” situation described above. References "],["Projections.html", "Chapter 8 Local projection methods 8.1 Overview of the approach 8.2 Situation A: Without IV 8.3 Situation B: IV approach", " Chapter 8 Local projection methods 8.1 Overview of the approach Consider the infinite MA representation of \\(y_t\\) (Eq. (1.4)): \\[ y_t = \\mu + \\sum_{h=0}^\\infty \\Psi_{h} \\eta_{t-h}. \\] As seen in Section 1.2, the entries \\((i,j)\\) of the sequence of the \\(\\Psi_h\\) matrices define the IRF of \\(\\eta_{j,t}\\) on \\(y_{i,t}\\). Assume that you observe \\(\\eta_{j,t}\\), then a consistent estimate of \\(\\Psi_{i,j,h}\\) is simply obtained by the OLS regression of \\(y_{i,t+h}\\) on \\(\\eta_{j,t}\\):9 \\[\\begin{equation} y_{i,t+h} = \\mu_i + \\Psi_{i,j,h}\\eta_{j,t} + u_{i,j,t+h}.\\tag{8.1} \\end{equation}\\] Running that kind of regression (using instruments for \\(\\eta_{j,t}\\)) is the core idea of the local projection (LP) approach proposed by Jordà (2005). Now, how to proceed in the (usual) case where \\(\\eta_{j,t}\\) is not observed? We consider two situations. While the second requires some instruments, the first approach does not. This first approach (Section 8.2) is the original Jordà (2005)’s approach. 8.2 Situation A: Without IV Assume that the structural shock of interest (\\(\\eta_{1,t}\\), say) can be consistently obtained as the residual of a regression of a variable \\(x_t\\) on a set of control variables \\(w_t\\) independent from \\(\\eta_{1,t}\\): \\[\\begin{equation} \\eta_{1,t} = x_t - \\mathbb{E}(x_t|w_t),\\tag{8.2} \\end{equation}\\] where \\(\\mathbb{E}(x_t|w_t)\\) is affine in \\(w_t\\) and where \\(w_t\\) is an affine transformation of \\(\\eta_{2:n,t}\\) and of past shocks \\(\\eta_{t-1},\\eta_{t-2},\\dots\\). Eq. (8.2) implies that, conditional on \\(w_t\\), the additional knowledge of \\(x_t\\) is useful only when it comes to forecast something that depends on \\(\\eta_{1,t}\\). Hence, given that \\(u_{i,1,t+h}\\) (see Eq. (8.1)) is independent from \\(\\eta_{1,t}\\) (it depends on \\(\\eta_{t+h},\\dots,\\eta_{t+1},\\color{blue}{\\eta_{2:n,t}},\\eta_{t-1},\\eta_{t-2},\\dots\\)), it comes that \\[\\begin{equation} \\mathbb{E}(u_{i,1,t+h}|x_t,w_t)= \\mathbb{E}(u_{i,1,t+h}|w_t).\\tag{8.3} \\end{equation}\\] This is the conditional mean independence case. Using (8.2), one can rewrite Eq. (8.1) as follows: \\[\\begin{eqnarray*} y_{i,t+h} &amp;=&amp; \\mu_i + \\Psi_{i,1,h}\\eta_{1,t} + u_{i,1,t+h}\\\\ &amp;=&amp; \\mu_i + \\Psi_{i,1,h}x_t \\color{blue}{-\\Psi_{i,1,h}\\mathbb{E}(x_t|w_t) + u_{i,1,t+h}}, \\end{eqnarray*}\\] Given Eq. (8.3), it comes that, conditional on \\(x_t\\) and \\(w_t\\), the expectation of the blue term is a function of \\(w_t\\). Assuming this expectation is linear, standard results in the conditional mean independence case imply that the OLS estimator in the regression of \\(y_{i,t+h}\\) on \\(x_t\\), controlling for \\(w_t\\), provides a consistent estimate of \\(\\Psi_{i,1,h}\\): \\[\\begin{equation} y_{i,t+h} = \\alpha_i + \\Psi_{i,1,h}x_t + \\beta&#39;w_t + v_{i,t+h}. \\end{equation}\\] This is for instance consistent with the case where \\([\\Delta GDP_t, \\pi_t,i_t]&#39;\\) follows a VAR(1) and the monetary-policy shock does not contemporaneously affect \\(\\Delta GDP_t\\) and \\(\\pi_t\\). The IRFs can then be estimated by LP, taking \\(x_t = i_t\\) and \\(w_t = [\\Delta GDP_t,\\pi_t,\\Delta GDP_{t-1}, \\pi_{t-1},i_{t-1}]&#39;\\). This approach closely relates to the SVAR Cholesky-based identification approach. Specifically, if \\(w_t = [\\color{blue}{y_{1,t},\\dots,y_{k-1,t}}, y_{t-1}&#39;,\\dots,y_{t-p}&#39;]&#39;\\), with \\(k\\le n\\), and \\(x_t = y_{k,t}\\), then this approach corresponds, for \\(h=0\\), to the SVAR(\\(p\\)) Cholesky-based IRF (focusing on the responses to the \\(k^{th}\\) structural shock). However, the two approaches differ for \\(h&gt;0\\), because the LP methodology does not assumes a VAR dynamics for \\(y_t\\).10 In the following lines of code, we employ the Jordà (2005)’s approach on the same dataset as the one used in Section 2.3. (We were then illustrating Christiano, Eichenbaum, and Evans (1996)’s methodology.) library(IdSS);library(vars) data(&quot;USmonthly&quot;) # Select sample period: First.date &lt;- &quot;1965-01-01&quot;;Last.date &lt;- &quot;1995-06-01&quot; indic.first &lt;- which(USmonthly$DATES==First.date) indic.last &lt;- which(USmonthly$DATES==Last.date) USmonthly &lt;- USmonthly[indic.first:indic.last,] considered.variables &lt;- c(&quot;LIP&quot;,&quot;UNEMP&quot;,&quot;LCPI&quot;,&quot;LPCOM&quot;,&quot;FFR&quot;,&quot;NBR&quot;,&quot;TTR&quot;,&quot;M1&quot;) y &lt;- as.matrix(USmonthly[considered.variables]) res.jorda &lt;- make.jorda.irf(y,posit.of.shock = 5, nb.periods.IRF = 12, nb.lags.endog.var.4.control=3, indic.plot = 1, # Plots are displayed if = 1. confidence.interval = 0.90) Figure 8.1: Response to a monetary-policy shock. Identification approach of Jorda (2005). 8.3 Situation B: IV approach 8.3.1 Instruments (proxies for structural shocks) Consider now that we have a valid instrument \\(z_t\\) for \\(\\eta_{1,t}\\) (with \\(\\mathbb{E}(z_t)=0\\)). That is: \\[\\begin{equation} \\left\\{ \\begin{array}{llll} (IV.i) &amp; \\mathbb{E}(z_t \\eta_{1,t}) &amp;\\ne 0 &amp; \\mbox{(relevance condition)} \\\\ (IV.ii) &amp; \\mathbb{E}(z_t \\eta_{j,t}) &amp;= 0 \\quad \\mbox{for } j&gt;1 &amp; \\mbox{(exogeneity condition).} \\end{array}\\right.\\tag{8.4} \\end{equation}\\] The instrument \\(z_t\\) can be used to identify the structural shock. Eq. (8.4) implies that there exist \\(\\rho \\ne 0\\) and a mean-zero variable \\(\\xi_t\\) such that: \\[ \\eta_{1,t} = \\rho z_t + \\xi_t, \\] where \\(\\xi_t\\) is correlated neither to \\(z_t\\), nor to \\(\\eta_{j,t}\\), \\(j\\ge2\\). Proof. Define \\(\\rho = \\frac{\\mathbb{E}(\\eta_{1,t}z_t)}{\\mathbb{V}ar(z_t)}\\) and \\(\\xi_t = \\eta_{1,t} - \\rho z_t\\). It is easily seen that \\(\\xi_t\\) satisfies the moment restrictions given above. Ramey (2016) reviews the different approaches employed to construct monetary policy-shocks (the two main approaches are presented in 8.1 and 8.2 below). She has also collected time series of such shocks, see her website. Several of these shocks are included in the Ramey dataset of package IdSS. Example 8.1 (Identification of Monetary-Policy Shocks Based on High-Frequency Data) Instruments for monetary-policy shocks can be extracted from high-frequency market data associated with interest-rate products. The quotes of all interest-rate-related financial products are sensitive to monetary-policy announcements. That is because these quotes mainly depends on investors’ expectations regarding future short-term rates: \\(\\mathbb{E}_t(i_{t+s})\\). Typically, if agents were risk-neutral, the maturity-\\(h\\) interest rate would approximatively be given by: \\[ i_{t,h} \\approx \\mathbb{E}_t\\left(\\frac{1}{h}\\int_{0}^{h} i_{t+s} ds\\right) = \\frac{1}{h}\\int_{0}^{h} \\mathbb{E}_t\\left(i_{t+s}\\right) ds. \\] In general, changes in \\(\\mathbb{E}_t(i_{t+s})\\), for \\(s&gt;0\\), can be affected by all types of shocks that may trigger a reaction by the central bank. However, if a MP announcement takes place between \\(t\\) and \\(t+\\epsilon\\), then most of \\(\\mathbb{E}_{t+\\epsilon}(i_{t+s})-\\mathbb{E}_t(i_{t+s})\\) is to be attributed to the MP shock (see Figure 8.2, from Gürkaynak, Sack, and Swanson (2005)). Hence, a monthly time series of MP shocks can be obtained by summing, over each month, the changes \\(i_{t+ \\epsilon,h} - i_{t,h}\\) associated with a given interest rate (T-bills, futures, swaps) and a given maturity \\(h\\). See among others: Kuttner (2001), Cochrane and Piazzesi (2002), Gürkaynak, Sack, and Swanson (2005), Piazzesi and Swanson (2008), Gertler and Karadi (2015). The time series named FF4_TC, ED2_TC, ED3_TC, ED4_TC, GS1, ff1_vr, ff4_vr, ed2_vr, ff1_gkgreen, ff4_gkgreen, ed2_gkgreen in the data frame Ramey of package IdSS are time series of shocks based on this approach (see Ramey’s website for details). Figure 8.2: Source: Gurkaynak, Sack and Swanson (2005). Transaction rates of Federal funds futures on June 25, 2003, day on which a regularly scheduled FOMC meeting was scheduled. At 2:15 p.m., the FOMC announced that it was lowering its target for the federal funds rate from 1.25% to 1%, while many market participants were expecting a 50 bp cut. This shows that (i) financial markets seem to fully adjust to the policy action within just a few minutes and (ii) the federal funds rate surprise is not necessarily in the same direction as the federal funds rate action itself. Example 8.2 (Identification of Monetary-Policy Shocks Based on the Narrative Approach) Romer and Romer (2004) propose a two-step approach: derive a series for Federal Reserve intentions for the federal funds rate (the explicit target of the Fed) around FOMC meetings, control for Federal Reserve forecasts. This gives a measure of intended monetary policy actions not driven by information about future economic developments. “intentions” are measured as a combination of narrative and quantitative evidence. Sources: (among others) Minutes of FOMC and “Blue Books”. Controls = variables spanning the information the Federal Reserve has about future developments. Data: Federal Reserve’s internal forecasts (inflation, real output and unemployment), “Greenbook’s forecasts” – usually issued 6 days before the FOMC meeting. The shock measure is the residual series in the linear regression of (a) on (b). The time series Ramey$rrshock83 and Ramey$rrshock83b (where Ramey is a data frame included in package IdSS) contain such shocks for the period 1983-2007. (Ramey$rrshock83b uses long-horizon Greenbook forecasts.) To create a measure of news about future government spending, Ramey (2011) uses newspaper articles to construct a time series of (unexpected) fiscal shocks:11 Example 8.3 (Identification of news about future government spending) Ramey (2011)’s measure aims to measure the expected discounted value of government spending changes due to foreign political events. She argues this variable should matter for the wealth effect in a neoclassical framework. The series is constructed by reading periodicals in order to gauge the public’s expectations (Business Week before 2001, other newspapers afterwards). According to Ramey (2011), government sources could not be used because (a) they were either not released in a timely manner or (b) were known to underestimate the costs of certain actions. Figure 8.3 shows the resulting time series of shocks. Figure 8.4 shows the IRF of macro variables to the shock on expected government spending. Figure 8.3: Source: Ramey (2011). Defense News: PDV of Change in Spending as a Percent of GDP. Figure 8.4: Source: Ramey (2011) [Figure X of the paper]. Responses of macro variables to a shock on expected government spending. There are two main IV approaches to estimate IRFs see Stock and Watson (2018): The SVAR-IV approach (Subsection 8.3.2), The LP-IV approach, where \\(y_t\\)’s DGP is left unspecified (Subsection 8.3.3). The LP-IV approach is based on a set of IV regressions (for each variable of interest, one for each forecast horizon). The SVAR-IV approach is based on IV regressions of VAR innovations only (one for each series of VAR innovations). If the VAR adequately captures the DGP, then the IV-SVAR is optimal for all horizons. However, if the VAR is misspecified, then specification errors are compounded at each horizon and a local projection method would lead to better results. 8.3.2 Situation B.1: SVAR-IV approach Assume you have consistent estimates of \\(\\varepsilon_t = B\\eta_t\\), these estimates (\\(\\hat\\varepsilon_{t}\\)) coming from the estimation of a VAR model. We have, for \\(i \\in \\{1,\\dots,n\\}\\): \\[\\begin{eqnarray} \\varepsilon_{i,t} &amp;=&amp; b_{i,1} \\eta_{1,t} + u_{i,t} \\tag{8.5}\\\\ &amp;=&amp; b_{i,1} \\rho z_t + \\underbrace{b_{i,1}\\xi_t + u_{i,t}}_{\\perp z_t}. \\nonumber \\end{eqnarray}\\] (\\(u_{i,t}\\) is a linear combination of the \\(\\eta_{j,t}\\)’s, \\(j\\ge2\\)). Hence, up to a multiplicative factor (\\(\\rho\\)), the (OLS) regressions of the \\(\\hat\\varepsilon_{i,t}\\)’s on \\(z_t\\) (that are consistent of the true \\(\\varepsilon_{i,t}\\)’s) provide consistent estimates of the \\(b_{i,1}\\)’s. Combined with the estimated VAR (the \\(\\Phi_k\\) matrices), this provides consistent estimates of the IRFs of \\(\\eta_{1,t}\\) on \\(y_t\\), though up to a multiplicative factor. This scale ambiguity can be solved by rescaling the structural shock (“unit-effect normalisation”, see Stock and Watson (2018)). Let us consider \\(\\tilde\\eta_{1,t}=b_{1,1}\\eta_{1,t}\\); by construction, \\(\\tilde\\eta_{1,t}\\) has a unit contemporaneous effect on \\(y_{1,t}\\). Denoting by \\(\\tilde{B}_{i,1}\\) the contemporaneous impact of \\(\\tilde\\eta_{1,t}\\) on the \\(i^{th}\\) endogenous variable, we get: \\[ \\tilde{B}_{1} = \\frac{1}{b_{1,1}} {B}_{1}, \\] where \\(B_{1}\\) denotes the \\(1^{st}\\) column of \\(B\\) and \\(\\tilde{B}_{1}=[1,\\tilde{B}_{2,1},\\dots,\\tilde{B}_{n,1}]&#39;\\). Eq. (8.5) gives: \\[\\begin{eqnarray*} \\varepsilon_{1,t} &amp;=&amp; \\tilde\\eta_{1,t} + u_{1,t}\\\\ \\varepsilon_{i,t} &amp;=&amp; \\tilde{B}_{i,1} \\tilde\\eta_{1,t} + u_{i,t}. \\end{eqnarray*}\\] This suggests that \\(\\tilde{B}_{i,1}\\) can be estimated by regressing \\(\\varepsilon_{i,t}\\) on \\(\\varepsilon_{1,t}\\) (or \\(\\hat\\varepsilon_{i,t}\\) on \\(\\hat\\varepsilon_{1,t}\\) in practice), using \\(z_t\\) as an instrument. What about inference? Once cannot use the usual TSLS standard deviations because the \\(\\varepsilon_{i,t}\\)’s are not directly observed. Bootstrap procedures can be resorted to. Stock and Watson (2018) propose, in particular, a Gaussian parametric bootstrap: Assume you have estimated \\(\\{\\widehat{\\Phi}_1,\\dots,\\widehat{\\Phi}_p,\\widehat{B}_1\\}\\) using the SVAR-IV approach based on a size-\\(T\\) sample. Generate \\(N\\) (where \\(N\\) is large) size-\\(T\\) samples from the following VAR: \\[ \\left[ \\begin{array}{cc} \\widehat{\\Phi}(L) &amp; 0 \\\\ 0 &amp; \\widehat{\\rho}(L) \\end{array} \\right] \\left[ \\begin{array}{c} y_t \\\\ z_t \\end{array} \\right] = \\left[ \\begin{array}{c} \\varepsilon_t \\\\ e_t \\end{array} \\right], \\] \\[ \\mbox{where} \\quad \\left[ \\begin{array}{c} \\varepsilon_t \\\\ e_t \\end{array} \\right]\\sim \\, i.i.d.\\,\\mathcal{N}\\left(\\left[\\begin{array}{c}0\\\\0\\end{array}\\right], \\left[\\begin{array}{cc} \\Omega &amp; S&#39;_{\\varepsilon,e}\\\\ S_{\\varepsilon,e}&amp; \\sigma^2_{e} \\end{array}\\right] \\right), \\] where \\(\\widehat{\\rho}(L)\\) and \\(\\sigma^2_{e}\\) result from the estimation of an AR process for \\(z_t\\), and where \\(\\Omega\\) and \\(S_{\\varepsilon,e}\\) are sample covariances for the VAR/AR residuals. For each simulated sample (of \\(\\tilde{y}_t\\) and \\(\\tilde{z}_t\\), say), estimate \\(\\{\\widetilde{\\widehat{\\Phi}}_1,\\dots,\\widetilde{\\widehat{\\Phi}}_p,\\widetilde{\\widehat{B}}_1\\}\\) and associated \\(\\widetilde{\\Psi}_{i,1,h}\\). This provides e.g. a sequence of \\(N\\) estimates of \\(\\Psi_{i,1,h}\\), from which quantiles and conf. intervals can be deduced. In the following lines of code, we use this approach to estimate the response of macroeconomic variables to a monetary policy shock. The instrument is FF4_TC from the Ramsey’s database; they are base on the Gertler and Karadi (2015) approach, that use 3-month fed funds futures. library(vars);library(IdSS) data(&quot;USmonthly&quot;) First.date &lt;- &quot;1990-05-01&quot;;Last.date &lt;- &quot;2012-6-01&quot; indic.first &lt;- which(USmonthly$DATES==First.date) indic.last &lt;- which(USmonthly$DATES==Last.date) USmonthly &lt;- USmonthly[indic.first:indic.last,] shock.name &lt;- c(&quot;FF4_TC&quot;,&quot;ED2_TC&quot;) # &quot;ff1_vr&quot;, &quot;rrshock83b&quot; indic.shock.name &lt;- which(names(USmonthly)%in%shock.name) Z &lt;- as.matrix(USmonthly[,indic.shock.name]) par(plt=c(.1,.95,.1,.95)) plot(USmonthly$DATES,Z[,1],type=&quot;l&quot;,xlab=&quot;&quot;,ylab=&quot;&quot;,lwd=2) lines(USmonthly$DATES,Z[,2],col=&quot;red&quot;,lwd=2,pch=3,lty=2) Figure 8.5: Gertler-Karadi monthly shocks, fed funds futures 3 months (resp. 6 months) in black (resp. in red). considered.variables &lt;- c(&quot;GS1&quot;,&quot;LIP&quot;,&quot;LCPI&quot;,&quot;EBP&quot;) Y &lt;- as.matrix(USmonthly[,considered.variables]) n &lt;- length(considered.variables) colnames(Y) &lt;- considered.variables par(plt=c(.15,.95,.15,.8)) res.svar.iv &lt;- svar.iv(Y,Z,p = 4,names.of.variables=considered.variables, nb.periods.IRF = 20, z.AR.order=1, nb.bootstrap.replications = 100, confidence.interval = 0.90, indic.plot=1) Figure 8.6: Reponses to a monetary-policy shock, SVAR-IV approach. 8.3.3 Situation B.2: LP-IV If you do not want to posit a VAR-type dynamics for \\(y_t\\) –e.g., because you suspect that the true generating model may be a non-invertible VARMA model– you can directly proceed by IV-projection methods to obtain the \\(\\tilde\\Psi_{i,1,h}\\equiv \\Psi_{i,1,h}/b_{1,1}\\) (that are the IRFs of \\(\\tilde\\eta_{1,t}\\) on \\(y_{i,t}\\)). However, Assumptions (IV.i) and (IV.ii) (Eq. (8.4)) have to be complemented with (IV.iii): \\[\\begin{equation*} \\begin{array}{llll} (IV.iii) &amp; \\mathbb{E}(z_t \\eta_{j,t+h}) &amp;= 0 \\, \\mbox{ for } h \\ne 0 &amp; \\mbox{(lead-lag exogeneity)} \\end{array} \\end{equation*}\\] When (IV.i), (IV.ii) and (IV.iii) are satisfied, \\(\\tilde\\Psi_{i,1,h}\\) can be estimated by regressing \\(y_{i,t+h}\\) on \\(y_{1,t}\\), using \\(z_t\\) as an instrument, i.e. by considering the TSLS estimation of: \\[\\begin{equation} y_{i,t+h} = \\alpha_i + \\tilde\\Psi_{i,1,h}y_{1,t} + \\nu_{i,t+h},\\tag{8.6} \\end{equation}\\] where \\(\\nu_{i,t+h}\\) is correlated to \\(y_{1,t}\\), but not to \\(z_t\\). We have indeed: \\[\\begin{eqnarray*} y_{1,t} &amp;=&amp; \\alpha_1 + \\tilde\\eta_{1,t} + v_{1,t}\\\\ y_{i,t+h} &amp;=&amp; \\alpha_i + \\tilde\\Psi_{i,1,h}\\tilde\\eta_{1,t} + v_{i,t+h}, \\end{eqnarray*}\\] where the \\(v_{i,t+h}\\)’s are uncorrelated to \\(z_t\\) under (IV.i), (IV.ii) and (IV.iii). Note again that, for \\(h&gt;0\\), the \\(v_{i,t+h}\\) (and \\(\\nu_{i,t+h}\\)) are auto-correlated. Newey-West corrections therefore have to be used to compute std errors of the \\(\\tilde\\Psi_{i,1,h}\\)’s estimates. Consider the linear regression: \\[ \\mathbf{Y} = \\mathbf{X}\\boldsymbol\\beta + \\boldsymbol\\varepsilon, \\] where \\(\\mathbb{E}(\\boldsymbol\\varepsilon)=0\\), but where the explicative variables \\(\\mathbf{X}\\) can be correlated to the residuals \\(\\boldsymbol\\varepsilon\\). Moreover, the \\(\\boldsymbol\\varepsilon\\)’s may feature heteroskedasticity and be auto-correlated. We denote by \\(\\mathbf{Z}\\) the matrix of instruments, with \\(\\mathbb{E}(\\mathbf{X}&#39;\\mathbf{Z}) \\ne 0\\) but \\(\\mathbb{E}(\\boldsymbol\\varepsilon&#39;\\mathbf{Z}) = 0\\). The IV estimator of \\(\\boldsymbol\\beta\\) is obtained by regressing \\(\\hat{\\mathbf{Y}}\\) on \\(\\hat{\\mathbf{X}}\\), where \\(\\hat{\\mathbf{Y}}\\) and \\(\\hat{\\mathbf{X}}\\) are the respective residuals of the regressions of \\(\\mathbf{Y}\\) and \\(\\mathbf{X}\\) on \\(\\mathbf{Z}\\). \\[\\begin{eqnarray*} \\mathbf{b}_{iv} &amp;=&amp; [\\mathbf{X}&#39;\\mathbf{Z}(\\mathbf{Z}&#39;\\mathbf{Z})^{-1}\\mathbf{Z}&#39;\\mathbf{X}]^{-1}\\mathbf{X}&#39;\\mathbf{Z}(\\mathbf{Z}&#39;\\mathbf{Z})^{-1}\\mathbf{Z}&#39;\\mathbf{Y}\\\\ \\mathbf{b}_{iv} &amp;=&amp; \\boldsymbol\\beta + \\frac{1}{\\sqrt{T}}\\underbrace{T[\\mathbf{X}&#39;\\mathbf{Z}(\\mathbf{Z}&#39;\\mathbf{Z})^{-1}\\mathbf{Z}&#39;\\mathbf{X}]^{-1}\\mathbf{X}&#39;\\mathbf{Z}(\\mathbf{Z}&#39;\\mathbf{Z})^{-1}}_{=Q(\\mathbf{X},\\mathbf{Z}) \\overset{p}{\\rightarrow} \\mathbf{Q}_{xz}}\\underbrace{\\sqrt{T}\\left(\\frac{1}{T}\\mathbf{Z}&#39;\\boldsymbol\\varepsilon\\right)}_{\\overset{d}{\\rightarrow} \\mathcal{N}(0,S)}, \\end{eqnarray*}\\] where \\(\\mathbf{S}\\) is the long-run variance of \\(\\mathbf{z}_t\\varepsilon_t\\).12 The asymptotic covariance matrix of \\(\\sqrt{T}\\mathbf{b}_{iv}\\) is \\(\\mathbf{Q}_{xz} \\mathbf{S} \\mathbf{Q}_{xz}&#39;\\). Therefore, the covariance matrix of \\(\\mathbf{b}_{iv}\\) can be approximated by \\(\\frac{1}{T}Q(\\mathbf{X},\\mathbf{Z})\\hat{\\mathbf{S}}Q(\\mathbf{X},\\mathbf{Z})&#39;\\) where \\(\\hat{\\mathbf{S}}\\) is the Newey-West estimator of \\(\\mathbf{S}\\).13 Assumption (IV.iii) is usually not restrictive for \\(h&gt;0\\) (\\(z_t\\) is usually not affected by future shocks). By contrast, it may be restrictive for \\(h&lt;0\\). This can be solved by adding controls in Regression (8.6). These controls should span the space of \\(\\{\\eta_{t-1},\\eta_{t-2},\\dots\\}\\). If \\(z_t\\) is suspected to be correlated to past values of \\(\\eta_{1,t}\\) but not to the \\(\\eta_{j,t}\\)’s, \\(j&gt;1\\), then one can add lags of \\(z_t\\) as controls (method e.g. advocated by Ramey, 2016, p.108, considering the instrument by Gertler and Karadi (2015)). In the general case, one can use lags of \\(y_t\\) as controls. Note that, even if (IV.iii) holds, adding controls may reduce the variance of the regression error. res.LP.IV &lt;- make.LPIV.irf(Y,Z, nb.periods.IRF = 20, nb.lags.Y.4.control=4, nb.lags.Z.4.control=4, indic.plot = 1, # Plots are displayed if = 1. confidence.interval = 0.90) References "],["PanelVARs.html", "Chapter 9 Panel VARs 9.1 Without Dynamic interdependence 9.2 With Dynamic Interdependencies", " Chapter 9 Panel VARs Panel VARs have the same structure as VAR models, in the sense that all variables are assumed to be endogenous and interdependent, but a cross sectional dimension is added to the representation. There are \\(N\\) units indexed by \\(i\\in\\{1,...,N\\}\\). The index \\(i\\) is generic and could indicate countries, sectors, markets… Then a panel VAR is \\[ y_{it}=c_i+\\Phi_i(L) y_{t-1}+\\varepsilon_{it}.\\] where \\(y_t\\) is the stacked version of \\(y_{it}\\) and \\(\\varepsilon_t\\) is i.i.d., with variance-covariance matrix \\(\\Omega\\). Vector \\(c_i\\) and the lag polynomial \\(\\Phi_i(L)\\) may depend on the unit. Canova and Ciccarelli (2013) provide a survey of panel estimation methods. Contrary to standard VARs, panel VARs may help study * similarities/differences in the transmission of shocks; * Spillovers, contagion. But panel VARs are subject tothe curse of dimensionality. Indeed, they can be characterized by * Dynamic interdependence: potentially, the lags of all endogenous variables of all units can enter the model for unit \\(i\\). * Static interdependence: \\(\\varepsilon_{it}\\) are generally correlated across \\(i\\). * Cross sectional heterogeneity: the intercept, the slope and the variance of the shocks may be unit-specific. 9.1 Without Dynamic interdependence A panel VAR, assuming no dynamic interdependence, is of the form: \\[ y_{it}=c_i+\\Phi_i(L) \\color{red}{y_{it-1}}+\\varepsilon_{it}.\\] As a comparison, consider micro panel data, in the univariate cae (AR(1) case): \\[y_{it}=c_i+{\\color{red}\\phi} y_{it-1}+\\varepsilon_{it}.\\] In that kind of context, we usually have no cross-sectional heterogeneity as \\(\\phi_i=\\phi\\) for all \\(i\\). Typically, we have a large cross-sectional dimension \\(N\\), and a small time dimension \\(T\\). If one uses a ``Fixed-effect’’ regression: \\[y_{it}-\\frac{1}{T}\\sum_{s=1}^Ty_{is}=\\phi(y_{it-1}-\\frac{1}{T}\\sum_{s=1}^Ty_{is})+\\varepsilon_{it}-\\frac{1}{T}\\sum_{s=1}^T\\color{red}{\\varepsilon_{is}},\\] then one faces the Nickell bias: with a lagged dependent variable, the estimator is biased, with a bias of size \\(\\sim 1/T\\). One can then use GMM regressions (Arellano and Bond (1991)) so as to get unbiased estimates. Macro panel data have a different structure, with typically a moderate cross-sectional dimension \\(N\\) and a large time dimension \\(T\\), so that the Nickell bias is negligible (\\(\\rightarrow 0\\) as \\(T\\rightarrow\\infty\\)). 9.1.1 Mean Group Estimator When we etimate \\[ y_{it}=c_i+\\Phi_i(L) y_{it-1}+\\varepsilon_{it},\\] we need to take into account the cross-sectional heterogeneity in the coefficients, i.e., different \\(\\Phi_i\\)’s across \\(i\\)’s. Pooled estimators (assuming cross-sectional homogeneity, i.e. identical \\(\\Phi_i\\)’s across \\(i\\)s) are not consistent (biased) if the underlying dynamics are actually heterogeneous. By contrast, the Mean Group (MG) estimator, which consists in estimating \\(N\\) separate regressions and calculating the coefficient means, is consistent. 9.1.2 Shock identification Shock identification can be performed with standard methods (zeros, signs, FEVM, etc.). We make the assumption that \\(\\Omega\\) is block-diagonal (no interdependence on impact), which is consistent with the assumption of no cross-sectional dependence. 9.2 With Dynamic Interdependencies A panel VAR that accommodates dynamic interdependence is of the form: \\[ y_{it}=c_i+\\Phi_i(L) y_{t-1}+\\varepsilon_{it}.\\] We face a serious curse of dimensionality here: there are \\(NGp+1\\) coefficients to estimate in each equation. A solution is to select some eligible dynamic links (See for instance Negro (2011)). Another alternative is to use a factor model. This consists in capturing the dynamic interdependencies by a set of unobservable factors (See Canova and Ciccarelli (2004) and Canova and Ciccarelli (2009)). See Section 10 for more details on FAVAR models. References "],["FAVAR.html", "Chapter 10 Factor-Augmented VAR 10.1 Principal component analysis (PCA) 10.2 FAVAR models", " Chapter 10 Factor-Augmented VAR VAR models are subject to the curse of dimensionality: If \\(n\\), is large, then the number of parameters (in \\(n^2\\)) explodes. In the case where one suspects that the \\(y_{i,t}\\)’s are mainly driven by a small number of random sources, a factor structure may be imposed, and principal component analysis (PCA, see Appendix 10.1) can be employed to estimate the relevant factors (Bernanke, Boivin, and Eliasz (2005)). 10.1 Principal component analysis (PCA) Principal component analysis (PCA) is a classical and easy-to-use statistical method to reduce the dimension of large datasets containing variables that are linearly driven by a relatively small number of factors. This approach is widely used in data analysis and image compression. Suppose that we have \\(T\\) observations of a \\(n\\)-dimensional random vector \\(x\\), denoted by \\(x_{1},x_{2},\\ldots,x_{T}\\). We suppose that each component of \\(x\\) is of mean zero. Let denote with \\(X\\) the matrix given by \\(\\left[\\begin{array}{cccc} x_{1} &amp; x_{2} &amp; \\ldots &amp; x_{T}\\end{array}\\right]&#39;\\). Denote the \\(j^{th}\\) column of \\(X\\) by \\(X_{j}\\). We want to find the linear combination of the \\(x_{i}\\)’s (\\(x.u\\)), with \\(\\left\\Vert u\\right\\Vert =1\\), with “maximum variance.” That is, we want to solve: \\[\\begin{equation} \\begin{array}{clll} \\underset{u}{\\arg\\max} &amp; u&#39;X&#39;Xu. \\\\ \\mbox{s.t. } &amp; \\left\\Vert u \\right\\Vert =1 \\end{array}\\tag{10.1} \\end{equation}\\] Since \\(X&#39;X\\) is a positive definite matrix, it admits the following decomposition: \\[\\begin{eqnarray*} X&#39;X &amp; = &amp; PDP&#39;\\\\ &amp; = &amp; P\\left[\\begin{array}{ccc} \\lambda_{1}\\\\ &amp; \\ddots\\\\ &amp; &amp; \\lambda_{n} \\end{array}\\right]P&#39;, \\end{eqnarray*}\\] where \\(P\\) is an orthogonal matrix whose columns are the eigenvectors of \\(X&#39;X\\). We can order the eigenvalues such that \\(\\lambda_{1}\\geq\\ldots\\geq\\lambda_{n}\\). (Since \\(X&#39;X\\) is positive definite, all these eigenvalues are positive.) Since \\(P\\) is orthogonal, we have \\(u&#39;X&#39;Xu=u&#39;PDP&#39;u=y&#39;Dy\\) where \\(\\left\\Vert y\\right\\Vert =1\\). Therefore, we have \\(y_{i}^{2}\\leq 1\\) for any \\(i\\leq n\\). As a consequence: \\[ y&#39;Dy=\\sum_{i=1}^{n}y_{i}^{2}\\lambda_{i}\\leq\\lambda_{1}\\sum_{i=1}^{n}y_{i}^{2}=\\lambda_{1}. \\] It is easily seen that the maximum is reached for \\(y=\\left[1,0,\\cdots,0\\right]&#39;\\). Therefore, the maximum of the optimization program (Eq. (10.1)) is obtained for \\(u=P\\left[1,0,\\cdots,0\\right]&#39;\\). That is, \\(u\\) is the eigenvector of \\(X&#39;X\\) that is associated with its larger eigenvalue (first column of \\(P\\)). Let us denote with \\(F\\) the vector that is given by the matrix product \\(XP\\). The columns of \\(F\\), denoted by \\(F_{j}\\), are called factors. We have: \\[ F&#39;F=P&#39;X&#39;XP=D. \\] Therefore, in particular, the \\(F_{j}\\)’s are orthogonal. Since \\(X=FP&#39;\\), the \\(X_{j}\\)’s are linear combinations of the factors. Let us then denote with \\(\\hat{X}_{i,j}\\) the part of \\(X_{i}\\) that is explained by factor \\(F_{j}\\), we have: \\[\\begin{eqnarray*} \\hat{X}_{i,j} &amp; = &amp; p_{ij}F_{j}\\\\ X_{i} &amp; = &amp; \\sum_{j}\\hat{X}_{i,j}=\\sum_{j}p_{ij}F_{j}. \\end{eqnarray*}\\] Consider the share of variance that is explained—through the \\(n\\) variables (\\(X_{1},\\ldots,X_{n}\\))—by the first factor \\(F_{1}\\): \\[\\begin{eqnarray*} \\frac{\\sum_{i}\\hat{X}&#39;_{i,1}\\hat{X}_{i,1}}{\\sum_{i}X_{i}&#39;X_{i}} &amp; = &amp; \\frac{\\sum_{i}p_{i1}F&#39;_{1}F_{1}p_{i1}}{tr(X&#39;X)} = \\frac{\\sum_{i}p_{i1}^{2}\\lambda_{1}}{tr(X&#39;X)} = \\frac{\\lambda_{1}}{\\sum_{i}\\lambda_{i}}. \\end{eqnarray*}\\] Intuitively, if the first eigenvalue is large, it means that the first factor captures a large share of the fluctutaions of the \\(n\\) \\(X_{i}\\)’s. By the same token, it is easily seen that the fraction of the variance of the \\(n\\) variables that is explained by factor \\(j\\) is given by: \\[\\begin{eqnarray*} \\frac{\\sum_{i}\\hat{X}&#39;_{i,j}\\hat{X}_{i,j}}{\\sum_{i}X_{i}&#39;X_{i}} &amp; = &amp; \\frac{\\lambda_{j}}{\\sum_{i}\\lambda_{i}}. \\end{eqnarray*}\\] Let us illustrate PCA on the term structure of yields. The term strucutre of yields (or yield curve) is know to be driven by only a small number of factors (e.g., Litterman and Scheinkman (1991)). One can typically employ PCA to recover such factors. The data used in the example below are taken from the Fred database (tickers: “DGS6MO”,“DGS1”, …). The second plot shows the factor loardings, that indicate that the first factor is a level factor (loadings = black line), the second factor is a slope factor (loadings = blue line), the third factor is a curvature factor (loadings = red line). To run a PCA, one simply has to apply function prcomp to a matrix of data: library(IdSS) USyields &lt;- USyields[complete.cases(USyields),] yds &lt;- USyields[c(&quot;Y1&quot;,&quot;Y2&quot;,&quot;Y3&quot;,&quot;Y5&quot;,&quot;Y7&quot;,&quot;Y10&quot;,&quot;Y20&quot;,&quot;Y30&quot;)] PCA.yds &lt;- prcomp(yds,center=TRUE,scale. = TRUE) Let us know visualize some results. The first plot of Figure 10.1 shows the share of total variance explained by the different principal components (PCs). The second plot shows the facotr loadings. The two bottom plots show how yields (in black) are fitted by linear combinations of the first two PCs only. par(mfrow=c(2,2)) par(plt=c(.1,.95,.2,.8)) barplot(PCA.yds$sdev^2/sum(PCA.yds$sdev^2), main=&quot;Share of variance expl. by PC&#39;s&quot;) axis(1, at=1:dim(yds)[2], labels=colnames(PCA.yds$x)) nb.PC &lt;- 2 plot(-PCA.yds$rotation[,1],type=&quot;l&quot;,lwd=2,ylim=c(-1,1), main=&quot;Factor loadings (1st 3 PCs)&quot;,xaxt=&quot;n&quot;,xlab=&quot;&quot;) axis(1, at=1:dim(yds)[2], labels=colnames(yds)) lines(PCA.yds$rotation[,2],type=&quot;l&quot;,lwd=2,col=&quot;blue&quot;) lines(PCA.yds$rotation[,3],type=&quot;l&quot;,lwd=2,col=&quot;red&quot;) Y1.hat &lt;- PCA.yds$x[,1:nb.PC] %*% PCA.yds$rotation[&quot;Y1&quot;,1:2] Y1.hat &lt;- mean(USyields$Y1) + sd(USyields$Y1) * Y1.hat plot(USyields$date,USyields$Y1,type=&quot;l&quot;,lwd=2, main=&quot;Fit of 1-year yields (2 PCs)&quot;, ylab=&quot;Obs (black) / Fitted by 2PCs (dashed blue)&quot;) lines(USyields$date,Y1.hat,col=&quot;blue&quot;,lty=2,lwd=2) Y10.hat &lt;- PCA.yds$x[,1:nb.PC] %*% PCA.yds$rotation[&quot;Y10&quot;,1:2] Y10.hat &lt;- mean(USyields$Y10) + sd(USyields$Y10) * Y10.hat plot(USyields$date,USyields$Y10,type=&quot;l&quot;,lwd=2, main=&quot;Fit of 10-year yields (2 PCs)&quot;, ylab=&quot;Obs (black) / Fitted by 2PCs (dashed blue)&quot;) lines(USyields$date,Y10.hat,col=&quot;blue&quot;,lty=2,lwd=2) Figure 10.1: Some PCA results. The dataset contains 8 time series of U.S. interest rates of different maturities. 10.2 FAVAR models Let us denote by \\(F_t\\) a \\(k\\)-dimensional vector of latent factors accounting for important shares of the variances of the \\(y_{i,t}\\)’s (with \\(K \\ll n\\)) and by \\(x_t\\) is a small \\(M\\)-dimensional subset of \\(y_t\\) (with \\(M \\ll n\\)). The following factor structure is posited: \\[ y_t = \\Lambda^f F_t + \\Lambda^x x_t + e_t, \\] where the \\(e_t\\) are “small” serially and mutually i.i.d. error terms. That is \\(F_t\\) and \\(x_t\\) are supposed to drive most of the fluctuations of \\(y_t\\)’s components. The model is complemented by positing a VAR dynamics for \\([F_t&#39;,x_t&#39;]&#39;\\): \\[\\begin{equation} \\left[\\begin{array}{c}F_t\\\\x_t\\end{array}\\right] = \\Phi(L)\\left[\\begin{array}{c}F_{t-1}\\\\ x_{t-1}\\end{array}\\right] + v_t.\\tag{10.2} \\end{equation}\\] Standard identification techniques of structural shocks can be employed in Eq. (10.2): Cholesky approach can be used for instance if the last component of \\(x_t\\) is the short-term interest rate and if it is assumed that a MP shock has no contemporaneous impact on other variables. In their identification procedure, Bernanke, Boivin, and Eliasz (2005) exploit the fact that macro-finance variables can be decomposed in two sets—fast-moving and slow-moving variables—and that only the former reacts contemporaneously to monetary-policy shocks. Now, how to estimate the (unobserved) factors \\(F_t\\)? Bernanke, Boivin, and Eliasz (2005) note that the first \\(K+M\\) PCA of the whole dataset (\\(y_t\\)), that they denote by \\(\\hat{C}(F_t,x_t)\\) should span the same space as \\(F_t\\) and \\(x_t\\). To get an estimate of \\(F_t\\), the dependence of \\(\\hat{C}(F_t,x_t)\\) in \\(x_t\\) has to be removed. This is done by regressing, by OLS, \\(\\hat{C}(F_t,x_t)\\) on \\(x_t\\) and on \\(\\hat{C}^*(F_t)\\), where the latter is an estimate of the common components other than \\(x_t\\). To proxy for \\(\\hat{C}^*(F_t)\\), Bernanke, Boivin, and Eliasz (2005) take principal components from the set of slow-moving variables, that are not comtemporaneously correlated to \\(x_t\\). Vector \\(\\hat{F}_t\\) is then computed as \\(\\hat{C}(F_t,x_t) - b_x x_t\\), where \\(b_x\\) are the coefficients coming from the previous OLS regressions. Note that this approach implies that the vectorial space spanned by \\((\\hat{F}_t,x_t)\\) is the same as that spanned by \\(\\hat{C}(F_t,x_t)\\). Below, we employ this method on the dataset built by McCracken and Ng (2016) —the FRED:MD database— that includes 119 time series. library(BVAR)# contains the fred_md dataset library(IdSS) library(vars) data &lt;- fred_transform(fred_md,na.rm = FALSE, type = &quot;fred_md&quot;) data &lt;- data[complete.cases(data),] data.values &lt;- scale(data, center = TRUE, scale = TRUE) data_scaled &lt;- data data_scaled[1:dim(data)[1],1:dim(data)[2]] &lt;- data.values K &lt;- 3 M &lt;- 1 PCA &lt;- prcomp(data_scaled) # implies that PCA$x %*% t(PCA$rotation) = data C.hat &lt;- PCA$x[,1:(K+M)] fast_moving &lt;- c(&quot;HOUST&quot;,&quot;HOUSTNE&quot;,&quot;HOUSTMW&quot;,&quot;HOUSTS&quot;,&quot;HOUSTW&quot;,&quot;HOUSTS&quot;,&quot;AMDMNOx&quot;, &quot;FEDFUNDS&quot;,&quot;CP3Mx&quot;,&quot;TB3MS&quot;,&quot;TB6MS&quot;,&quot;GS1&quot;,&quot;GS5&quot;,&quot;GS10&quot;, &quot;COMPAPFFx&quot;,&quot;TB3SMFFM&quot;,&quot;TB6SMFFM&quot;,&quot;T1YFFM&quot;,&quot;T5YFFM&quot;,&quot;T10YFFM&quot;, &quot;AAAFFM&quot;,&quot;EXSZUSx&quot;,&quot;EXJPUSx&quot;,&quot;EXUSUKx&quot;,&quot;EXCAUSx&quot;) data.slow &lt;- data_scaled[,-which(fast_moving %in% names(data))] PCA.star &lt;- prcomp(data.slow) # implies that PCA$x %*% t(PCA$rotation) = data C.hat.star &lt;- PCA.star$x[,1:K] D &lt;- cbind(data$FEDFUNDS,C.hat.star) b.x &lt;- solve(t(D)%*%D) %*% t(D) %*% C.hat F.hat &lt;- C.hat - data$FEDFUNDS %*% matrix(b.x[1,],nrow=1) data_var &lt;- data.frame(F.hat, FEDFUNDS = data$FEDFUNDS) p &lt;- 10 var &lt;- VAR(data_var, p) Omega &lt;- var(residuals(var)) B &lt;- t(chol(Omega)) D &lt;- cbind(F.hat,data$FEDFUNDS) loadings &lt;- solve(t(D)%*%D) %*% t(D) %*% as.matrix(data_scaled) irf &lt;- simul.VAR(c=rep(0,(K+M)*p),Phi=Acoef(var),B,nb.sim=120, y0.star=rep(0,(K+M)*p),indic.IRF = 1, u.shock = c(rep(0,K+1),1)) irf.all &lt;- irf %*% loadings par(mfrow=c(2,2)) variables.2.plot &lt;- c(&quot;FEDFUNDS&quot;,&quot;INDPRO&quot;,&quot;UNRATE&quot;,&quot;CPIAUCSL&quot;) par(plt=c(.2,.95,.3,.95)) for(i in 1:length(variables.2.plot)){ plot(cumsum(irf.all[,which(variables.2.plot[i]==names(data))]),lwd=2, type=&quot;l&quot;,xlab=&quot;months after shock&quot;,ylab=variables.2.plot[i]) } Figure 10.2: Responses of a monetary-policy shock. FAVAR approach of Bernanke, Boivin, and Eliasz (2005). FRED-MD dataset. References "],["append.html", "Chapter 11 Appendix 11.1 Definitions and statistical results 11.2 Proofs 11.3 Statistical Tables", " Chapter 11 Appendix 11.1 Definitions and statistical results Definition 11.1 (Covariance stationarity) The process \\(y_t\\) is covariance stationary —or weakly stationary— if, for all \\(t\\) and \\(j\\), \\[ \\mathbb{E}(y_t) = \\mu \\quad \\mbox{and} \\quad \\mathbb{E}\\{(y_t - \\mu)(y_{t-j} - \\mu)\\} = \\gamma_j. \\] Definition 11.2 (Likelihood Ratio test statistics) The likelihood ratio associated to a restriction of the form \\(H_0: h({\\boldsymbol\\theta})=0\\) (where \\(h({\\boldsymbol\\theta})\\) is a \\(r\\)-dimensional vector) is given by: \\[ LR = \\frac{\\mathcal{L}_R(\\boldsymbol\\theta;\\mathbf{y})}{\\mathcal{L}_U(\\boldsymbol\\theta;\\mathbf{y})} \\quad (\\in [0,1]), \\] where \\(\\mathcal{L}_R\\) (respectively \\(\\mathcal{L}_U\\)) is the likelihood function that imposes (resp. that does not impose) the restriction. The likelihood ratio test statistic is given by \\(-2\\log(LR)\\), that is: \\[ \\boxed{\\xi^{LR}= 2 (\\log\\mathcal{L}_U(\\boldsymbol\\theta;\\mathbf{y})-\\log\\mathcal{L}_R(\\boldsymbol\\theta;\\mathbf{y})).} \\] Under regularity assumptions and under the null hypothesis, the test statistic follows a chi-square distribution with \\(r\\) degrees of freedom (see Table 11.3). Proposition 11.1 (p.d.f. of a multivariate Gaussian variable) If \\(Y \\sim \\mathcal{N}(\\mu,\\Omega)\\) and if \\(Y\\) is a \\(n\\)-dimensional vector, then the density function of \\(Y\\) is: \\[ \\frac{1}{(2 \\pi)^{n/2}|\\Omega|^{1/2}}\\exp\\left[-\\frac{1}{2}\\left(Y-\\mu\\right)&#39;\\Omega^{-1}\\left(Y-\\mu\\right)\\right]. \\] 11.2 Proofs Proof of Proposition 1.2 Proof. Using Proposition 11.1, we obtain that, conditionally on \\(x_1\\), the log-likelihood is given by \\[\\begin{eqnarray*} \\log\\mathcal{L}(Y_{T};\\theta) &amp; = &amp; -(Tn/2)\\log(2\\pi)+(T/2)\\log\\left|\\Omega^{-1}\\right|\\\\ &amp; &amp; -\\frac{1}{2}\\sum_{t=1}^{T}\\left[\\left(y_{t}-\\Pi&#39;x_{t}\\right)&#39;\\Omega^{-1}\\left(y_{t}-\\Pi&#39;x_{t}\\right)\\right]. \\end{eqnarray*}\\] Let’s rewrite the last term of the log-likelihood: \\[\\begin{eqnarray*} \\sum_{t=1}^{T}\\left[\\left(y_{t}-\\Pi&#39;x_{t}\\right)&#39;\\Omega^{-1}\\left(y_{t}-\\Pi&#39;x_{t}\\right)\\right] &amp; =\\\\ \\sum_{t=1}^{T}\\left[\\left(y_{t}-\\hat{\\Pi}&#39;x_{t}+\\hat{\\Pi}&#39;x_{t}-\\Pi&#39;x_{t}\\right)&#39;\\Omega^{-1}\\left(y_{t}-\\hat{\\Pi}&#39;x_{t}+\\hat{\\Pi}&#39;x_{t}-\\Pi&#39;x_{t}\\right)\\right] &amp; =\\\\ \\sum_{t=1}^{T}\\left[\\left(\\hat{\\varepsilon}_{t}+(\\hat{\\Pi}-\\Pi)&#39;x_{t}\\right)&#39;\\Omega^{-1}\\left(\\hat{\\varepsilon}_{t}+(\\hat{\\Pi}-\\Pi)&#39;x_{t}\\right)\\right], \\end{eqnarray*}\\] where the \\(j^{th}\\) element of the \\((n\\times1)\\) vector \\(\\hat{\\varepsilon}_{t}\\) is the sample residual, for observation \\(t\\), from an OLS regression of \\(y_{j,t}\\) on \\(x_{t}\\). Expanding the previous equation, we get: \\[\\begin{eqnarray*} &amp;&amp;\\sum_{t=1}^{T}\\left[\\left(y_{t}-\\Pi&#39;x_{t}\\right)&#39;\\Omega^{-1}\\left(y_{t}-\\Pi&#39;x_{t}\\right)\\right] = \\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}&#39;\\Omega^{-1}\\hat{\\varepsilon}_{t}\\\\ &amp;&amp;+2\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}&#39;\\Omega^{-1}(\\hat{\\Pi}-\\Pi)&#39;x_{t}+\\sum_{t=1}^{T}x&#39;_{t}(\\hat{\\Pi}-\\Pi)\\Omega^{-1}(\\hat{\\Pi}-\\Pi)&#39;x_{t}. \\end{eqnarray*}\\] Let’s apply the trace operator on the second term (that is a scalar): \\[\\begin{eqnarray*} \\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}&#39;\\Omega^{-1}(\\hat{\\Pi}-\\Pi)&#39;x_{t} &amp; = &amp; Tr\\left(\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}&#39;\\Omega^{-1}(\\hat{\\Pi}-\\Pi)&#39;x_{t}\\right)\\\\ = Tr\\left(\\sum_{t=1}^{T}\\Omega^{-1}(\\hat{\\Pi}-\\Pi)&#39;x_{t}\\hat{\\varepsilon}_{t}&#39;\\right) &amp; = &amp; Tr\\left(\\Omega^{-1}(\\hat{\\Pi}-\\Pi)&#39;\\sum_{t=1}^{T}x_{t}\\hat{\\varepsilon}_{t}&#39;\\right). \\end{eqnarray*}\\] Given that, by construction (property of OLS estimates), the sample residuals are orthogonal to the explanatory variables, this term is zero. Introducing \\(\\tilde{x}_{t}=(\\hat{\\Pi}-\\Pi)&#39;x_{t}\\), we have \\[\\begin{eqnarray*} \\sum_{t=1}^{T}\\left[\\left(y_{t}-\\Pi&#39;x_{t}\\right)&#39;\\Omega^{-1}\\left(y_{t}-\\Pi&#39;x_{t}\\right)\\right] =\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}&#39;\\Omega^{-1}\\hat{\\varepsilon}_{t}+\\sum_{t=1}^{T}\\tilde{x}&#39;_{t}\\Omega^{-1}\\tilde{x}_{t}. \\end{eqnarray*}\\] Since \\(\\Omega\\) is a positive definite matrix, \\(\\Omega^{-1}\\) is as well. Consequently, the smallest value that the last term can take is obtained for \\(\\tilde{x}_{t}=0\\), i.e. when \\(\\Pi=\\hat{\\Pi}.\\) The MLE of \\(\\Omega\\) is the matrix \\(\\hat{\\Omega}\\) that maximizes \\(\\Omega\\overset{\\ell}{\\rightarrow}L(Y_{T};\\hat{\\Pi},\\Omega)\\). We have: \\[\\begin{eqnarray*} \\log\\mathcal{L}(Y_{T};\\hat{\\Pi},\\Omega) &amp; = &amp; -(Tn/2)\\log(2\\pi)+(T/2)\\log\\left|\\Omega^{-1}\\right| -\\frac{1}{2}\\sum_{t=1}^{T}\\left[\\hat{\\varepsilon}_{t}&#39;\\Omega^{-1}\\hat{\\varepsilon}_{t}\\right]. \\end{eqnarray*}\\] Matrix \\(\\hat{\\Omega}\\) is a symmetric positive definite. It is easily checked that the (unrestricted) matrix that maximizes the latter expression is symmetric positive definite matrix. Indeed: \\[ \\frac{\\partial \\log\\mathcal{L}(Y_{T};\\hat{\\Pi},\\Omega)}{\\partial\\Omega}=\\frac{T}{2}\\Omega&#39;-\\frac{1}{2}\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}\\hat{\\varepsilon}&#39;_{t}\\Rightarrow\\hat{\\Omega}&#39;=\\frac{1}{T}\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}\\hat{\\varepsilon}&#39;_{t}, \\] which leads to the result. Proof of Proposition 1.3 Proof. Let us drop the \\(i\\) subscript. Rearranging Eq. (1.12), we have: \\[ \\sqrt{T}(\\mathbf{b}-\\boldsymbol{\\beta}) = (X&#39;X/T)^{-1}\\sqrt{T}(X&#39;\\boldsymbol\\varepsilon/T). \\] Let us consider the autocovariances of \\(\\mathbf{v}_t = x_t \\varepsilon_t\\), denoted by \\(\\gamma^v_j\\). Using the fact that \\(x_t\\) is a linear combination of past \\(\\varepsilon_t\\)s and that \\(\\varepsilon_t\\) is a white noise, we get that \\(\\mathbb{E}(\\varepsilon_t x_t)=0\\). Therefore \\[ \\gamma^v_j = \\mathbb{E}(\\varepsilon_t\\varepsilon_{t-j}x_tx_{t-j}&#39;). \\] If \\(j&gt;0\\), we have \\(\\mathbb{E}(\\varepsilon_t\\varepsilon_{t-j}x_tx_{t-j}&#39;)=\\mathbb{E}(\\mathbb{E}[\\varepsilon_t\\varepsilon_{t-j}x_tx_{t-j}&#39;|\\varepsilon_{t-j},x_t,x_{t-j}])=\\) \\(\\mathbb{E}(\\varepsilon_{t-j}x_tx_{t-j}&#39;\\mathbb{E}[\\varepsilon_t|\\varepsilon_{t-j},x_t,x_{t-j}])=0\\). Note that we have \\(\\mathbb{E}[\\varepsilon_t|\\varepsilon_{t-j},x_t,x_{t-j}]=0\\) because \\(\\{\\varepsilon_t\\}\\) is an i.i.d. white noise sequence. If \\(j=0\\), we have: \\[ \\gamma^v_0 = \\mathbb{E}(\\varepsilon_t^2x_tx_{t}&#39;)= \\mathbb{E}(\\varepsilon_t^2) \\mathbb{E}(x_tx_{t}&#39;)=\\sigma^2\\mathbf{Q}. \\] The convergence in distribution of \\(\\sqrt{T}(X&#39;\\boldsymbol\\varepsilon/T)=\\sqrt{T}\\frac{1}{T}\\sum_{t=1}^Tv_t\\) results from the Central Limit Theorem for covariance-stationary processes, using the \\(\\gamma_j^v\\) computed above. 11.3 Statistical Tables Table 11.1: Quantiles of the \\(\\mathcal{N}(0,1)\\) distribution. If \\(a\\) and \\(b\\) are respectively the row and column number; then the corresponding cell gives \\(\\mathbb{P}(0&lt;X\\le a+b)\\), where \\(X \\sim \\mathcal{N}(0,1)\\). 0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0 0.5000 0.6179 0.7257 0.8159 0.8849 0.9332 0.9641 0.9821 0.9918 0.9965 0.1 0.5040 0.6217 0.7291 0.8186 0.8869 0.9345 0.9649 0.9826 0.9920 0.9966 0.2 0.5080 0.6255 0.7324 0.8212 0.8888 0.9357 0.9656 0.9830 0.9922 0.9967 0.3 0.5120 0.6293 0.7357 0.8238 0.8907 0.9370 0.9664 0.9834 0.9925 0.9968 0.4 0.5160 0.6331 0.7389 0.8264 0.8925 0.9382 0.9671 0.9838 0.9927 0.9969 0.5 0.5199 0.6368 0.7422 0.8289 0.8944 0.9394 0.9678 0.9842 0.9929 0.9970 0.6 0.5239 0.6406 0.7454 0.8315 0.8962 0.9406 0.9686 0.9846 0.9931 0.9971 0.7 0.5279 0.6443 0.7486 0.8340 0.8980 0.9418 0.9693 0.9850 0.9932 0.9972 0.8 0.5319 0.6480 0.7517 0.8365 0.8997 0.9429 0.9699 0.9854 0.9934 0.9973 0.9 0.5359 0.6517 0.7549 0.8389 0.9015 0.9441 0.9706 0.9857 0.9936 0.9974 1 0.5398 0.6554 0.7580 0.8413 0.9032 0.9452 0.9713 0.9861 0.9938 0.9974 1.1 0.5438 0.6591 0.7611 0.8438 0.9049 0.9463 0.9719 0.9864 0.9940 0.9975 1.2 0.5478 0.6628 0.7642 0.8461 0.9066 0.9474 0.9726 0.9868 0.9941 0.9976 1.3 0.5517 0.6664 0.7673 0.8485 0.9082 0.9484 0.9732 0.9871 0.9943 0.9977 1.4 0.5557 0.6700 0.7704 0.8508 0.9099 0.9495 0.9738 0.9875 0.9945 0.9977 1.5 0.5596 0.6736 0.7734 0.8531 0.9115 0.9505 0.9744 0.9878 0.9946 0.9978 1.6 0.5636 0.6772 0.7764 0.8554 0.9131 0.9515 0.9750 0.9881 0.9948 0.9979 1.7 0.5675 0.6808 0.7794 0.8577 0.9147 0.9525 0.9756 0.9884 0.9949 0.9979 1.8 0.5714 0.6844 0.7823 0.8599 0.9162 0.9535 0.9761 0.9887 0.9951 0.9980 1.9 0.5753 0.6879 0.7852 0.8621 0.9177 0.9545 0.9767 0.9890 0.9952 0.9981 2 0.5793 0.6915 0.7881 0.8643 0.9192 0.9554 0.9772 0.9893 0.9953 0.9981 2.1 0.5832 0.6950 0.7910 0.8665 0.9207 0.9564 0.9778 0.9896 0.9955 0.9982 2.2 0.5871 0.6985 0.7939 0.8686 0.9222 0.9573 0.9783 0.9898 0.9956 0.9982 2.3 0.5910 0.7019 0.7967 0.8708 0.9236 0.9582 0.9788 0.9901 0.9957 0.9983 2.4 0.5948 0.7054 0.7995 0.8729 0.9251 0.9591 0.9793 0.9904 0.9959 0.9984 2.5 0.5987 0.7088 0.8023 0.8749 0.9265 0.9599 0.9798 0.9906 0.9960 0.9984 2.6 0.6026 0.7123 0.8051 0.8770 0.9279 0.9608 0.9803 0.9909 0.9961 0.9985 2.7 0.6064 0.7157 0.8078 0.8790 0.9292 0.9616 0.9808 0.9911 0.9962 0.9985 2.8 0.6103 0.7190 0.8106 0.8810 0.9306 0.9625 0.9812 0.9913 0.9963 0.9986 2.9 0.6141 0.7224 0.8133 0.8830 0.9319 0.9633 0.9817 0.9916 0.9964 0.9986 Table 11.2: Quantiles of the Student-\\(t\\) distribution. The rows correspond to different degrees of freedom (\\(\\nu\\), say); the columns correspond to different probabilities (\\(z\\), say). The cell gives \\(q\\) that is s.t. \\(\\mathbb{P}(-q&lt;X&lt;q)=z\\), with \\(X \\sim t(\\nu)\\). 0.05 0.1 0.75 0.9 0.95 0.975 0.99 0.999 1 0.079 0.158 2.414 6.314 12.706 25.452 63.657 636.619 2 0.071 0.142 1.604 2.920 4.303 6.205 9.925 31.599 3 0.068 0.137 1.423 2.353 3.182 4.177 5.841 12.924 4 0.067 0.134 1.344 2.132 2.776 3.495 4.604 8.610 5 0.066 0.132 1.301 2.015 2.571 3.163 4.032 6.869 6 0.065 0.131 1.273 1.943 2.447 2.969 3.707 5.959 7 0.065 0.130 1.254 1.895 2.365 2.841 3.499 5.408 8 0.065 0.130 1.240 1.860 2.306 2.752 3.355 5.041 9 0.064 0.129 1.230 1.833 2.262 2.685 3.250 4.781 10 0.064 0.129 1.221 1.812 2.228 2.634 3.169 4.587 20 0.063 0.127 1.185 1.725 2.086 2.423 2.845 3.850 30 0.063 0.127 1.173 1.697 2.042 2.360 2.750 3.646 40 0.063 0.126 1.167 1.684 2.021 2.329 2.704 3.551 50 0.063 0.126 1.164 1.676 2.009 2.311 2.678 3.496 60 0.063 0.126 1.162 1.671 2.000 2.299 2.660 3.460 70 0.063 0.126 1.160 1.667 1.994 2.291 2.648 3.435 80 0.063 0.126 1.159 1.664 1.990 2.284 2.639 3.416 90 0.063 0.126 1.158 1.662 1.987 2.280 2.632 3.402 100 0.063 0.126 1.157 1.660 1.984 2.276 2.626 3.390 200 0.063 0.126 1.154 1.653 1.972 2.258 2.601 3.340 500 0.063 0.126 1.152 1.648 1.965 2.248 2.586 3.310 Table 11.3: Quantiles of the \\(\\chi^2\\) distribution. The rows correspond to different degrees of freedom; the columns correspond to different probabilities. 0.05 0.1 0.75 0.9 0.95 0.975 0.99 0.999 1 0.004 0.016 1.323 2.706 3.841 5.024 6.635 10.828 2 0.103 0.211 2.773 4.605 5.991 7.378 9.210 13.816 3 0.352 0.584 4.108 6.251 7.815 9.348 11.345 16.266 4 0.711 1.064 5.385 7.779 9.488 11.143 13.277 18.467 5 1.145 1.610 6.626 9.236 11.070 12.833 15.086 20.515 6 1.635 2.204 7.841 10.645 12.592 14.449 16.812 22.458 7 2.167 2.833 9.037 12.017 14.067 16.013 18.475 24.322 8 2.733 3.490 10.219 13.362 15.507 17.535 20.090 26.124 9 3.325 4.168 11.389 14.684 16.919 19.023 21.666 27.877 10 3.940 4.865 12.549 15.987 18.307 20.483 23.209 29.588 20 10.851 12.443 23.828 28.412 31.410 34.170 37.566 45.315 30 18.493 20.599 34.800 40.256 43.773 46.979 50.892 59.703 40 26.509 29.051 45.616 51.805 55.758 59.342 63.691 73.402 50 34.764 37.689 56.334 63.167 67.505 71.420 76.154 86.661 60 43.188 46.459 66.981 74.397 79.082 83.298 88.379 99.607 70 51.739 55.329 77.577 85.527 90.531 95.023 100.425 112.317 80 60.391 64.278 88.130 96.578 101.879 106.629 112.329 124.839 90 69.126 73.291 98.650 107.565 113.145 118.136 124.116 137.208 100 77.929 82.358 109.141 118.498 124.342 129.561 135.807 149.449 200 168.279 174.835 213.102 226.021 233.994 241.058 249.445 267.541 500 449.147 459.926 520.950 540.930 553.127 563.852 576.493 603.446 Table 11.4: Quantiles of the \\(\\mathcal{F}\\) distribution. The columns and rows correspond to different degrees of freedom (resp. \\(n_1\\) and \\(n_2\\)). The different panels correspond to different probabilities (\\(\\alpha\\)) The corresponding cell gives \\(z\\) that is s.t. \\(\\mathbb{P}(X \\le z)=\\alpha\\), with \\(X \\sim \\mathcal{F}(n_1,n_2)\\). 1 2 3 4 5 6 7 8 9 10 alpha = 0.9 5 4.060 3.780 3.619 3.520 3.453 3.405 3.368 3.339 3.316 3.297 10 3.285 2.924 2.728 2.605 2.522 2.461 2.414 2.377 2.347 2.323 15 3.073 2.695 2.490 2.361 2.273 2.208 2.158 2.119 2.086 2.059 20 2.975 2.589 2.380 2.249 2.158 2.091 2.040 1.999 1.965 1.937 50 2.809 2.412 2.197 2.061 1.966 1.895 1.840 1.796 1.760 1.729 100 2.756 2.356 2.139 2.002 1.906 1.834 1.778 1.732 1.695 1.663 500 2.716 2.313 2.095 1.956 1.859 1.786 1.729 1.683 1.644 1.612 alpha = 0.95 5 6.608 5.786 5.409 5.192 5.050 4.950 4.876 4.818 4.772 4.735 10 4.965 4.103 3.708 3.478 3.326 3.217 3.135 3.072 3.020 2.978 15 4.543 3.682 3.287 3.056 2.901 2.790 2.707 2.641 2.588 2.544 20 4.351 3.493 3.098 2.866 2.711 2.599 2.514 2.447 2.393 2.348 50 4.034 3.183 2.790 2.557 2.400 2.286 2.199 2.130 2.073 2.026 100 3.936 3.087 2.696 2.463 2.305 2.191 2.103 2.032 1.975 1.927 500 3.860 3.014 2.623 2.390 2.232 2.117 2.028 1.957 1.899 1.850 alpha = 0.99 5 16.258 13.274 12.060 11.392 10.967 10.672 10.456 10.289 10.158 10.051 10 10.044 7.559 6.552 5.994 5.636 5.386 5.200 5.057 4.942 4.849 15 8.683 6.359 5.417 4.893 4.556 4.318 4.142 4.004 3.895 3.805 20 8.096 5.849 4.938 4.431 4.103 3.871 3.699 3.564 3.457 3.368 50 7.171 5.057 4.199 3.720 3.408 3.186 3.020 2.890 2.785 2.698 100 6.895 4.824 3.984 3.513 3.206 2.988 2.823 2.694 2.590 2.503 500 6.686 4.648 3.821 3.357 3.054 2.838 2.675 2.547 2.443 2.356 "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
